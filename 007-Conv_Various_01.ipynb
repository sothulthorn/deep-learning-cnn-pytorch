{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 초기화\n",
    "* pytorch의 nn.Linear와 nn.Conv2d는 He Uniform 기반 weight 초기화\n",
    "* 가중치 초기화는 nn.init.kaiming_uniform_(), nn.init.kaiming_normal_()등을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "torch.manual_seed(2025)\n",
    "\n",
    "linear_01 = nn.Linear(in_features=12, out_features=6)\n",
    "print(f'weight boundary: {linear_01.weight.min().item()} ~ {linear_01.weight.max().item()}')\n",
    "\n",
    "# pytorch weight는 1/sqrt(fan_in)\n",
    "fan_in = linear_01.in_features  \n",
    "bound = 1 / math.sqrt(fan_in)\n",
    "print('bound:', bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(2025)\n",
    "\n",
    "conv_01 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3)\n",
    "print(f'weight boundary: {conv_01.weight.min().item()} ~ {conv_01.weight.max().item()}')\n",
    "\n",
    "# pytorch weight는 1/sqrt(fan_in). Conv2d의 fan_in은 in_channels * kernel_height * kernel_width\n",
    "fan_in = conv_01.in_channels * 3 * 3\n",
    "bound = 1 / math.sqrt(fan_in)\n",
    "print('bound:', bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleCNN_01(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Linear(in_features=12544, out_features=num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                print('kaiming normal initialization applied')\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "simple_cnn_01 = SimpleCNN_01(num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization 적용\n",
    "* Linear Layer 이후 적용 시는 BatchNorm1d(num_features) 를 적용. num_features는 Linear Layer의 out_features와 동일\n",
    "* Conv2d Layer 이후 적용 시는 BatchNorm2d(num_features) 를 적용. num_features는 Conv2d의 out_channels와 동일\n",
    "* 기존 Network 모델의 Conv -> Activation을 Conv -> BN -> Activation 으로 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR 10 Dataset 및 DataLoader 생성, Trainer 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "#전체 6만개 데이터 중, 5만개는 학습 데이터용. 이를 다시 학습과 검증용으로 split , 1만개는 테스트 데이터용\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "tr_size = int(0.85 * len(train_dataset))\n",
    "val_size = len(train_dataset) - tr_size\n",
    "tr_dataset, val_dataset = random_split(train_dataset, [tr_size, val_size])\n",
    "print('tr:', len(tr_dataset), 'valid:', len(val_dataset))\n",
    "\n",
    "tr_loader = DataLoader(tr_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, optimizer, train_loader, val_loader, device=None):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        \n",
    "        # running 평균 loss 계산. \n",
    "        accu_loss = 0.0\n",
    "        running_avg_loss = 0.0\n",
    "        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        accuracy = 0.0\n",
    "        \n",
    "        # tqdm으로 실시간 training loop 진행 상황 시각화\n",
    "        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1} [Training..]\", leave=True) as progress_bar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "                # 반드시 to(self.device). to(device) 아님. \n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.  \n",
    "                accu_loss += loss.item()\n",
    "                running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                # accuracy metric 계산\n",
    "                # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n",
    "                num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산\n",
    "                num_total += inputs.shape[0]\n",
    "                accu_num_correct += num_correct\n",
    "                accuracy = accu_num_correct / num_total\n",
    "\n",
    "                #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n",
    "                progress_bar.update(1)\n",
    "                if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch횟수마다 또는 맨 마지막 batch에서 update \n",
    "                    progress_bar.set_postfix({\"Loss\": running_avg_loss, \n",
    "                                              \"Accuracy\": accuracy})\n",
    "        \n",
    "        return running_avg_loss, accuracy\n",
    "                \n",
    "    def validate_epoch(self, epoch):\n",
    "        if not self.val_loader:\n",
    "            return None\n",
    "            \n",
    "        self.model.eval()\n",
    "\n",
    "        # running 평균 loss 계산. \n",
    "        accu_loss = 0\n",
    "        running_avg_loss = 0\n",
    "        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        accuracy = 0.0\n",
    "        with tqdm(total=len(self.val_loader), desc=f\"Epoch {epoch+1} [Validating]\", leave=True) as progress_bar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(self.val_loader):\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(inputs)\n",
    "                    \n",
    "                    loss = self.loss_fn(outputs, targets)\n",
    "                    # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.  \n",
    "                    accu_loss += loss.item()\n",
    "                    running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                    # accuracy metric 계산\n",
    "                    # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n",
    "                    num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                    # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산  \n",
    "                    num_total += inputs.shape[0]\n",
    "                    accu_num_correct += num_correct\n",
    "                    accuracy = accu_num_correct / num_total\n",
    "\n",
    "                    #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n",
    "                    progress_bar.update(1)\n",
    "                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch횟수마다 또는 맨 마지막 batch에서 update \n",
    "                        progress_bar.set_postfix({\"Loss\": running_avg_loss, \n",
    "                                                  \"Accuracy\":accuracy})\n",
    "        return running_avg_loss, accuracy\n",
    "    \n",
    "    def fit(self, epochs):\n",
    "        # epoch 시마다 학습/검증 결과를 기록하는 history dict 생성.\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            val_loss, val_acc = self.validate_epoch(epoch)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}\",\n",
    "                  f\", Val Loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}\" if val_loss is not None else \"\")\n",
    "            # epoch 시마다 학습/검증 결과를 기록.\n",
    "            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "            \n",
    "        return history \n",
    "    \n",
    "    # 학습이 완료된 모델을 return \n",
    "    def get_trained_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization을 모델에 적용 후 성능 검증\n",
    "* 기존 Network 모델의 Conv -> Activation을 Conv -> BN -> Activation 으로 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "\n",
    "class SimpleCNNWithBN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        #kernel 크기 3, filter 개수 32 연속 적용.\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)     \n",
    "        )\n",
    "        \n",
    "        #out_channels이 64인 2개의 Conv2d. stride=1이 기본값, padding='same'은 version 1.8에서 소개됨.  \n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        # Sequential Module을 이용하여 Conv Layer들을 생성. 이 경우 relu activation위해 ReLU Layer 연결 생성 필요.\n",
    "        # filter갯수 128개인 Conv Layer 2개 적용 후 Max Pooling 적용. \n",
    "        self.conv_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # GAP 및 최종 Classifier Layer\n",
    "        self.classifier_block = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=128, out_features=num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.conv_block_3(x)\n",
    "        x = self.classifier_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "simple_cnn = SimpleCNNWithBN(num_classes=10)\n",
    "\n",
    "summary(model=simple_cnn, input_size=(1, 3, 32, 32), \n",
    "        col_names=['input_size', 'output_size', 'num_params'], \n",
    "        row_settings=['var_names'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 반복되는 Sequential Container 부분을 함수화 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "\n",
    "class SimpleCNNWithBN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        #padding 1로 conv 적용 후 출력 면적 사이즈를 입력 면적 사이즈와 동일하게 유지. \n",
    "        self.conv_block_1 = self.create_convbn_block(first_channels=3, middle_channels=32, last_channels=32)        \n",
    "        \n",
    "        #out_channels이 64인 2개의 Conv2d. stride=1이 기본값, padding='same'은 version 1.8에서 소개됨.  \n",
    "        self.conv_block_2 = self.create_convbn_block(first_channels=32, middle_channels=64, last_channels=64)\n",
    "        \n",
    "        # filter갯수 128개인 Conv Layer 2개 적용 후 Max Pooling 적용. \n",
    "        self.conv_block_3 = self.create_convbn_block(first_channels=64, middle_channels=128, last_channels=128)\n",
    "        \n",
    "        # GAP 및 최종 Classifier Layer\n",
    "        self.classifier_block = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=128, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def create_convbn_block(self, first_channels, middle_channels, last_channels):\n",
    "        conv_bn_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=first_channels, out_channels=middle_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(middle_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=middle_channels, out_channels=last_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(last_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        return conv_bn_block\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.conv_block_3(x)\n",
    "        x = self.classifier_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "simple_cnn = SimpleCNNWithBN(num_classes=10)\n",
    "\n",
    "summary(model=simple_cnn, input_size=(1, 3, 32, 32), \n",
    "        col_names=['input_size', 'output_size', 'num_params'], \n",
    "        row_settings=['var_names'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "model = SimpleCNNWithBN(num_classes=NUM_CLASSES)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n",
    "       train_loader=tr_loader, val_loader=val_loader, device=device)\n",
    "# 학습 및 평가 \n",
    "history = trainer.fit(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor 클래스로 예측 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        # 현재 입력으로 들어온 데이터의 batch 통계(mean, variance)를 사용하지 않고, 학습 시 계산된 running 통계값을 사용\n",
    "        self.model.eval()\n",
    "        eval_metric = 0.0\n",
    "        # 정확도 계산을 위한 전체 건수 및 누적 정확건수\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "\n",
    "        with tqdm(total=len(loader), desc=f\"[Evaluating]\", leave=True) as progress_bar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "                    pred = self.model(inputs)\n",
    "\n",
    "                    # 정확도 계산을 위해 누적 전체 건수와 누적 전체 num_correct 건수 계산  \n",
    "                    num_correct = (pred.argmax(-1) == targets).sum().item()\n",
    "                    num_total += inputs.shape[0]\n",
    "                    accu_num_correct += num_correct\n",
    "                    eval_metric = accu_num_correct / num_total\n",
    "\n",
    "                    progress_bar.update(1)\n",
    "                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:\n",
    "                        progress_bar.set_postfix({\"Accuracy\": eval_metric})\n",
    "        \n",
    "        return eval_metric\n",
    "\n",
    "    def predict_proba(self, inputs):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            #예측값을 반환하므로 targets은 필요 없음.\n",
    "            #targets = targets.to(self.device)\n",
    "            pred_proba = F.softmax(outputs, dim=-1) #또는 dim=1\n",
    "\n",
    "        return pred_proba\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        pred_proba = self.predict_proba(inputs)\n",
    "        pred_class = torch.argmax(pred_proba, dim=-1)\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_model = trainer.get_trained_model()\n",
    "\n",
    "# 학습데이터와 동일하게 정규화된 데이터를 입력해야 함. \n",
    "# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "predictor = Predictor(model=trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "* Pytorch는 Dropout을 위해 nn.Dropout(p) Layer 제공\n",
    "* nn.Dropout(p)는 지정된 p 확률로 입력 tensor의 element값을 0로, 0으로 변경되지 않은 element들은 scale factor로 scaling 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.randn(4, 10)\n",
    "print(f\"input Tensor:\\n{input_tensor}\")\n",
    "\n",
    "num_zeros = torch.sum(input_tensor == 0).item()\n",
    "print(f\"Number of zeros in input_tensor: {num_zeros}\")\n",
    "\n",
    "# p=0.5로 Dropout Layer정의\n",
    "dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "# Dropout Layer 적용. \n",
    "output_tensor = dropout(input_tensor)\n",
    "\n",
    "# Dropout 적용 후 tensor의 element value가 0인 건수 조사\n",
    "num_zeros = torch.sum(output_tensor == 0).item()\n",
    "\n",
    "# output tensor의 element 전체 건수\n",
    "total_elements = output_tensor.numel()\n",
    "\n",
    "# output tensor의 전체 element 중 0인 건수 비율 조사. \n",
    "percentage_zeros = (num_zeros / total_elements) * 100\n",
    "\n",
    "print(f\"Output Tensor:\\n{output_tensor}\")\n",
    "print(f\"Number of zeros in output tensor: {num_zeros}\")\n",
    "print(f\"Percentage of zeros: {percentage_zeros:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier 부분을 Dropout과 Linear로 연결\n",
    "* Pytorch는 기존 모델(Module)의 서브 모듈만 따로 동적으로 연결하여 모델을 변경할 수 있게 함.\n",
    "* 기존 모델에서 classification block 부분만 Dropout을 적용 할 수 있도록 모델 구조 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "simple_cnnbn_base = SimpleCNNWithBN(num_classes=NUM_CLASSES)\n",
    "simple_cnnbn_base.classifier_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "do_classifier_block = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(in_features=128*4*4, out_features=300),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=300, out_features=10),\n",
    "        )\n",
    "simple_cnnbn_base.classifier_block = do_classifier_block\n",
    "print(simple_cnnbn_base.classifier_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "summary(model=simple_cnnbn_base, input_size=(1, 3, 32, 32), \n",
    "        col_names=['input_size', 'output_size', 'num_params'], \n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_do_classifier_block(first_features, second_features, first_dos, second_dos, num_classes=10):\n",
    "    return nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=first_dos),\n",
    "            nn.Linear(in_features=first_features, out_features=second_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=second_dos),\n",
    "            nn.Linear(in_features=second_features, out_features=num_classes),\n",
    "        )\n",
    "\n",
    "simple_cnnbn_base = SimpleCNNWithBN(num_classes=NUM_CLASSES)\n",
    "do_classifier_block = create_do_classifier_block(first_features=128*4*4, second_features=300,\n",
    "                                                 first_dos=0.5, second_dos=0.3, num_classes=10)\n",
    "simple_cnnbn_base.classifier_block = do_classifier_block\n",
    "\n",
    "summary(model=simple_cnnbn_base, input_size=(1, 3, 32, 32), \n",
    "        col_names=['input_size', 'output_size', 'num_params'], \n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "model = SimpleCNNWithBN(num_classes=NUM_CLASSES)\n",
    "do_classifier_block = create_do_classifier_block(first_features=128*4*4, second_features=300,\n",
    "                                                 first_dos=0.5, second_dos=0.3, num_classes=10)\n",
    "model.classifier_block = do_classifier_block\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n",
    "       train_loader=tr_loader, val_loader=val_loader, device=device)\n",
    "# 학습 및 평가 \n",
    "history = trainer.fit(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_model = trainer.get_trained_model()\n",
    "\n",
    "# 학습데이터와 동일하게 정규화된 데이터를 입력해야 함. \n",
    "# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "predictor = Predictor(model=trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
