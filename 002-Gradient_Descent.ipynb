{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 보스턴 주택 가격 데이터 세트를 Peceptron 기반에서 학습 및 테스트하기 위한 데이터 로드\n",
    "* 보스턴 주택 가격 데이터 csv파일을 다운로드하고 이를 DataFrame으로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  \n",
       "0     15.3  396.90   4.98   24.0  \n",
       "1     17.8  396.90   9.14   21.6  \n",
       "2     17.8  392.83   4.03   34.7  \n",
       "3     18.7  394.63   2.94   33.4  \n",
       "4     18.7  396.90   5.33   36.2  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "boston_df = pd.read_csv('data/boston_house_price.csv')\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 14)\n"
     ]
    }
   ],
   "source": [
    "print(boston_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight와 Bias의 Update 값을 계산하는 함수 생성.\n",
    "* w1은 RM(방의 계수) 피처의 Weight 값\n",
    "* w2는 LSTAT(하위계층 비율) 피처의 Weight 값\n",
    "* bias는 Bias\n",
    "* N은 입력 데이터 건수\n",
    "![](https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/main/image/gradient_descent.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# gradient_descent()함수에서 반복적으로 호출되면서 update될 weight/bias 값을 계산하는 함수. \n",
    "# rm은 RM(방 개수), lstat(하위계층 비율), target은 PRICE임. 전체 해당 tensor가 다 입력됨. \n",
    "# 반환 값은 weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 loss로 반환.\n",
    "def get_update_weights_value(bias, w1, w2, rm, lstat, target, learning_rate=0.01):\n",
    "    # 데이터 건수\n",
    "    N = target.shape[0]\n",
    "    # 예측 값. \n",
    "    predicted = w1 * rm + w2 * lstat + bias\n",
    "    # 실제값과 예측값의 차이\n",
    "    diff = target - predicted \n",
    "    \n",
    "    # weight와 bias를 얼마나 update할 것인지를 계산.  \n",
    "    w1_update = -(2/N) * learning_rate * (torch.matmul(rm, diff))\n",
    "    w2_update = -(2/N) * learning_rate * (torch.matmul(lstat, diff))\n",
    "    bias_update = -(2/N) * learning_rate * torch.sum(diff)\n",
    "    \n",
    "    # Mean Squared Error값을 계산. \n",
    "    mse_loss = torch.mean(diff ** 2)\n",
    "    \n",
    "    # weight와 bias가 update되어야 할 값과 Mean Squared Error 값을 반환. \n",
    "    return bias_update, w1_update, w2_update, mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent 를 적용하는 함수 생성\n",
    "* iter_epochs 수만큼 반복적으로 get_update_weights_value()를 호출하여 update될 weight/bias값을 구한 뒤 Weight/Bias를 Update적용. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# RM, LSTAT feature tensor와 PRICE target tensor를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \n",
    "def gradient_descent(features, target, iter_epochs=1000, learning_rate=0.01, verbose=True):\n",
    "    # w1, w2는 1차원 tensor로 변환하되 초기 값은 0으로 설정\n",
    "    # bias도 1차원 tensor로 변환하되 초기 값은 1로 설정. \n",
    "    w1 = torch.zeros(1, dtype=torch.float32)\n",
    "    w2 = torch.zeros(1, dtype=torch.float32)\n",
    "    bias = torch.ones(1, dtype=torch.float32)\n",
    "    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n",
    "    \n",
    "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 tensor형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
    "    rm = features[:, 0]\n",
    "    lstat = features[:, 1]\n",
    "    \n",
    "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
    "    for i in range(1, iter_epochs+1):\n",
    "        # weight/bias update 값 계산\n",
    "        bias_update, w1_update, w2_update, loss = get_update_weights_value(bias, w1, w2, \n",
    "                                                                           rm, lstat, target, learning_rate=0.01)\n",
    "        \n",
    "        # weight/bias의 update 적용.\n",
    "        w1 = w1 - w1_update\n",
    "        w2 = w2 - w2_update\n",
    "        bias = bias - bias_update\n",
    "        if verbose: # 10회 epochs 시마다 출력\n",
    "            if i % 10 == 0:\n",
    "                print(f'Epoch: {i}/{iter_epochs}')\n",
    "                print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n",
    "        \n",
    "    return w1, w2, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent 적용\n",
    "* 신경망은 데이터를 정규화/표준화 작업을 미리 선행해 주어야 함. \n",
    "* 이를 위해 사이킷런의 MinMaxScaler를 이용하여 개별 feature값은 0~1사이 값으로 변환후 학습 적용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최초 w1, w2, bias: 0.0 0.0 1.0\n",
      "Epoch: 10/5000\n",
      "w1: 2.1600239276885986, w2: 0.8827961683273315, bias: 4.814126491546631, loss: 364.92802077239907\n",
      "Epoch: 20/5000\n",
      "w1: 3.83414626121521, w2: 1.488028645515442, bias: 7.705874919891357, loss: 244.00112523756874\n",
      "Epoch: 30/5000\n",
      "w1: 5.139560699462891, w2: 1.8834487199783325, bias: 9.898355484008789, loss: 174.09917235653666\n",
      "Epoch: 40/5000\n",
      "w1: 6.165116786956787, w2: 2.1204135417938232, bias: 11.560710906982422, loss: 133.53020121296407\n",
      "Epoch: 50/5000\n",
      "w1: 6.978174686431885, w2: 2.23785138130188, bias: 12.8211669921875, loss: 109.82715995177246\n",
      "Epoch: 60/5000\n",
      "w1: 7.629802703857422, w2: 2.265268325805664, bias: 13.776935577392578, loss: 95.82475012805263\n",
      "Epoch: 70/5000\n",
      "w1: 8.158720016479492, w2: 2.225027322769165, bias: 14.501710891723633, loss: 87.40472070696126\n",
      "Epoch: 80/5000\n",
      "w1: 8.594283103942871, w2: 2.134077310562134, bias: 15.05136489868164, loss: 82.2000806840636\n",
      "Epoch: 90/5000\n",
      "w1: 8.958751678466797, w2: 2.0052602291107178, bias: 15.468253135681152, loss: 78.85029674857002\n",
      "Epoch: 100/5000\n",
      "w1: 9.269001960754395, w2: 1.848307490348816, bias: 15.784483909606934, loss: 76.57334085917176\n",
      "Epoch: 110/5000\n",
      "w1: 9.537833213806152, w2: 1.6705907583236694, bias: 16.024398803710938, loss: 74.9196070997964\n",
      "Epoch: 120/5000\n",
      "w1: 9.77495002746582, w2: 1.47769296169281, bias: 16.206453323364258, loss: 73.63057815319392\n",
      "Epoch: 130/5000\n",
      "w1: 9.987712860107422, w2: 1.2738415002822876, bias: 16.34463882446289, loss: 72.55755325832801\n",
      "Epoch: 140/5000\n",
      "w1: 10.1817045211792, w2: 1.0622360706329346, bias: 16.449560165405273, loss: 71.61494702548367\n",
      "Epoch: 150/5000\n",
      "w1: 10.361161231994629, w2: 0.8452968001365662, bias: 16.529264450073242, loss: 70.7534498955806\n",
      "Epoch: 160/5000\n",
      "w1: 10.529294967651367, w2: 0.6248531341552734, bias: 16.58984375, loss: 69.94461147570567\n",
      "Epoch: 170/5000\n",
      "w1: 10.688545227050781, w2: 0.4022870659828186, bias: 16.635913848876953, loss: 69.17197101059817\n",
      "Epoch: 180/5000\n",
      "w1: 10.840765953063965, w2: 0.17864146828651428, bias: 16.670982360839844, loss: 68.42594438714845\n",
      "Epoch: 190/5000\n",
      "w1: 10.987363815307617, w2: -0.045298442244529724, bias: 16.697708129882812, loss: 67.70091838405226\n",
      "Epoch: 200/5000\n",
      "w1: 11.12940502166748, w2: -0.26894235610961914, bias: 16.718103408813477, loss: 66.99356210609766\n",
      "Epoch: 210/5000\n",
      "w1: 11.267704963684082, w2: -0.49184736609458923, bias: 16.733692169189453, loss: 66.30185452309605\n",
      "Epoch: 220/5000\n",
      "w1: 11.402885437011719, w2: -0.7136829495429993, bias: 16.745635986328125, loss: 65.62452333366802\n",
      "Epoch: 230/5000\n",
      "w1: 11.535415649414062, w2: -0.9342033267021179, bias: 16.75480842590332, loss: 64.96074485286708\n",
      "Epoch: 240/5000\n",
      "w1: 11.665658950805664, w2: -1.153226613998413, bias: 16.76187515258789, loss: 64.30993954455666\n",
      "Epoch: 250/5000\n",
      "w1: 11.793889045715332, w2: -1.3706201314926147, bias: 16.767343521118164, loss: 63.67167959873891\n",
      "Epoch: 260/5000\n",
      "w1: 11.920317649841309, w2: -1.5862876176834106, bias: 16.77159309387207, loss: 63.045622256090574\n",
      "Epoch: 270/5000\n",
      "w1: 12.045108795166016, w2: -1.8001607656478882, bias: 16.77491569519043, loss: 62.431477171926346\n",
      "Epoch: 280/5000\n",
      "w1: 12.168391227722168, w2: -2.012192964553833, bias: 16.777530670166016, loss: 61.82898035187066\n",
      "Epoch: 290/5000\n",
      "w1: 12.290260314941406, w2: -2.2223525047302246, bias: 16.77960205078125, loss: 61.23789490257106\n",
      "Epoch: 300/5000\n",
      "w1: 12.410794258117676, w2: -2.4306209087371826, bias: 16.78126335144043, loss: 60.657992415148215\n",
      "Epoch: 310/5000\n",
      "w1: 12.530052185058594, w2: -2.6369879245758057, bias: 16.782602310180664, loss: 60.089056976004706\n",
      "Epoch: 320/5000\n",
      "w1: 12.648086547851562, w2: -2.8414502143859863, bias: 16.783695220947266, loss: 59.53087323658953\n",
      "Epoch: 330/5000\n",
      "w1: 12.764933586120605, w2: -3.0440104007720947, bias: 16.78459930419922, loss: 58.98323935225674\n",
      "Epoch: 340/5000\n",
      "w1: 12.88062572479248, w2: -3.24467396736145, bias: 16.785354614257812, loss: 58.44595586900678\n",
      "Epoch: 350/5000\n",
      "w1: 12.99519157409668, w2: -3.4434497356414795, bias: 16.785993576049805, loss: 57.91882324789889\n",
      "Epoch: 360/5000\n",
      "w1: 13.108652114868164, w2: -3.6403491497039795, bias: 16.78653907775879, loss: 57.401649901601964\n",
      "Epoch: 370/5000\n",
      "w1: 13.221028327941895, w2: -3.835383653640747, bias: 16.787006378173828, loss: 56.89424849295415\n",
      "Epoch: 380/5000\n",
      "w1: 13.332338333129883, w2: -4.028567790985107, bias: 16.78741455078125, loss: 56.396432539828204\n",
      "Epoch: 390/5000\n",
      "w1: 13.442597389221191, w2: -4.219915390014648, bias: 16.78777313232422, loss: 55.9080204136309\n",
      "Epoch: 400/5000\n",
      "w1: 13.5518159866333, w2: -4.4094438552856445, bias: 16.7880916595459, loss: 55.428836289780556\n",
      "Epoch: 410/5000\n",
      "w1: 13.660008430480957, w2: -4.597165584564209, bias: 16.788373947143555, loss: 54.95870822827663\n",
      "Epoch: 420/5000\n",
      "w1: 13.76718807220459, w2: -4.783097743988037, bias: 16.78862762451172, loss: 54.49746117857592\n",
      "Epoch: 430/5000\n",
      "w1: 13.873366355895996, w2: -4.96725606918335, bias: 16.788850784301758, loss: 54.04492961253782\n",
      "Epoch: 440/5000\n",
      "w1: 13.978551864624023, w2: -5.149657726287842, bias: 16.789051055908203, loss: 53.600945499746004\n",
      "Epoch: 450/5000\n",
      "w1: 14.082757949829102, w2: -5.330317974090576, bias: 16.789228439331055, loss: 53.1653504039712\n",
      "Epoch: 460/5000\n",
      "w1: 14.185993194580078, w2: -5.509252548217773, bias: 16.789384841918945, loss: 52.73798433248157\n",
      "Epoch: 470/5000\n",
      "w1: 14.288265228271484, w2: -5.686478614807129, bias: 16.789522171020508, loss: 52.31869399199101\n",
      "Epoch: 480/5000\n",
      "w1: 14.38958740234375, w2: -5.862010955810547, bias: 16.789640426635742, loss: 51.90732409704085\n",
      "Epoch: 490/5000\n",
      "w1: 14.48996639251709, w2: -6.035866737365723, bias: 16.78974151611328, loss: 51.50372535866629\n",
      "Epoch: 500/5000\n",
      "w1: 14.5894136428833, w2: -6.2080607414245605, bias: 16.789827346801758, loss: 51.10775152132248\n",
      "Epoch: 510/5000\n",
      "w1: 14.687936782836914, w2: -6.378608703613281, bias: 16.78989601135254, loss: 50.71925887033235\n",
      "Epoch: 520/5000\n",
      "w1: 14.785545349121094, w2: -6.547528266906738, bias: 16.789949417114258, loss: 50.338101803148405\n",
      "Epoch: 530/5000\n",
      "w1: 14.882247924804688, w2: -6.714831352233887, bias: 16.789987564086914, loss: 49.96414792155913\n",
      "Epoch: 540/5000\n",
      "w1: 14.978052139282227, w2: -6.88053560256958, bias: 16.790008544921875, loss: 49.59725914344493\n",
      "Epoch: 550/5000\n",
      "w1: 15.072966575622559, w2: -7.044655799865723, bias: 16.790016174316406, loss: 49.23730079108317\n",
      "Epoch: 560/5000\n",
      "w1: 15.167000770568848, w2: -7.207205772399902, bias: 16.790010452270508, loss: 48.88414377321954\n",
      "Epoch: 570/5000\n",
      "w1: 15.260162353515625, w2: -7.368200778961182, bias: 16.78999137878418, loss: 48.53765968933464\n",
      "Epoch: 580/5000\n",
      "w1: 15.352463722229004, w2: -7.527656555175781, bias: 16.789953231811523, loss: 48.19771788189601\n",
      "Epoch: 590/5000\n",
      "w1: 15.443906784057617, w2: -7.685587406158447, bias: 16.789901733398438, loss: 47.86419864002177\n",
      "Epoch: 600/5000\n",
      "w1: 15.534501075744629, w2: -7.842007637023926, bias: 16.789838790893555, loss: 47.53698282844233\n",
      "Epoch: 610/5000\n",
      "w1: 15.624256134033203, w2: -7.996932029724121, bias: 16.789762496948242, loss: 47.21594688379376\n",
      "Epoch: 620/5000\n",
      "w1: 15.713179588317871, w2: -8.150373458862305, bias: 16.789669036865234, loss: 46.900976521767696\n",
      "Epoch: 630/5000\n",
      "w1: 15.801278114318848, w2: -8.302348136901855, bias: 16.789566040039062, loss: 46.59195642040891\n",
      "Epoch: 640/5000\n",
      "w1: 15.88856029510498, w2: -8.452868461608887, bias: 16.789451599121094, loss: 46.28877437837304\n",
      "Epoch: 650/5000\n",
      "w1: 15.975034713745117, w2: -8.601951599121094, bias: 16.789318084716797, loss: 45.99131579962141\n",
      "Epoch: 660/5000\n",
      "w1: 16.06070899963379, w2: -8.749606132507324, bias: 16.789175033569336, loss: 45.69947790902645\n",
      "Epoch: 670/5000\n",
      "w1: 16.145591735839844, w2: -8.89584732055664, bias: 16.78902244567871, loss: 45.4131527875823\n",
      "Epoch: 680/5000\n",
      "w1: 16.229686737060547, w2: -9.040689468383789, bias: 16.788850784301758, loss: 45.132237884990616\n",
      "Epoch: 690/5000\n",
      "w1: 16.31300163269043, w2: -9.184144973754883, bias: 16.788671493530273, loss: 44.85663076046775\n",
      "Epoch: 700/5000\n",
      "w1: 16.395551681518555, w2: -9.326227188110352, bias: 16.788480758666992, loss: 44.58622729642974\n",
      "Epoch: 710/5000\n",
      "w1: 16.477338790893555, w2: -9.466949462890625, bias: 16.788272857666016, loss: 44.32093027710049\n",
      "Epoch: 720/5000\n",
      "w1: 16.55836296081543, w2: -9.606324195861816, bias: 16.788057327270508, loss: 44.060649823221674\n",
      "Epoch: 730/5000\n",
      "w1: 16.638641357421875, w2: -9.744364738464355, bias: 16.78782844543457, loss: 43.805285180293986\n",
      "Epoch: 740/5000\n",
      "w1: 16.718175888061523, w2: -9.881084442138672, bias: 16.787586212158203, loss: 43.55474538664371\n",
      "Epoch: 750/5000\n",
      "w1: 16.79697608947754, w2: -10.016494750976562, bias: 16.787338256835938, loss: 43.308935831447016\n",
      "Epoch: 760/5000\n",
      "w1: 16.875049591064453, w2: -10.150609970092773, bias: 16.787071228027344, loss: 43.06776837421799\n",
      "Epoch: 770/5000\n",
      "w1: 16.95240020751953, w2: -10.283439636230469, bias: 16.78679847717285, loss: 42.83115868036707\n",
      "Epoch: 780/5000\n",
      "w1: 17.029037475585938, w2: -10.414997100830078, bias: 16.78651237487793, loss: 42.59901848345892\n",
      "Epoch: 790/5000\n",
      "w1: 17.104965209960938, w2: -10.545295715332031, bias: 16.78621482849121, loss: 42.371263746054986\n",
      "Epoch: 800/5000\n",
      "w1: 17.180194854736328, w2: -10.67434310913086, bias: 16.78590965270996, loss: 42.147812619612885\n",
      "Epoch: 810/5000\n",
      "w1: 17.254728317260742, w2: -10.802156448364258, bias: 16.785587310791016, loss: 41.928580118317484\n",
      "Epoch: 820/5000\n",
      "w1: 17.328575134277344, w2: -10.92874526977539, bias: 16.785261154174805, loss: 41.713490212703086\n",
      "Epoch: 830/5000\n",
      "w1: 17.40174102783203, w2: -11.054121971130371, bias: 16.7849178314209, loss: 41.502461221739274\n",
      "Epoch: 840/5000\n",
      "w1: 17.47422981262207, w2: -11.17829704284668, bias: 16.784568786621094, loss: 41.29542006945274\n",
      "Epoch: 850/5000\n",
      "w1: 17.546051025390625, w2: -11.30128288269043, bias: 16.78420639038086, loss: 41.092288879993184\n",
      "Epoch: 860/5000\n",
      "w1: 17.617210388183594, w2: -11.423089981079102, bias: 16.78383445739746, loss: 40.89299408160445\n",
      "Epoch: 870/5000\n",
      "w1: 17.687713623046875, w2: -11.543729782104492, bias: 16.7834529876709, loss: 40.69746461537404\n",
      "Epoch: 880/5000\n",
      "w1: 17.757566452026367, w2: -11.663215637207031, bias: 16.783061981201172, loss: 40.50562681180181\n",
      "Epoch: 890/5000\n",
      "w1: 17.826778411865234, w2: -11.781557083129883, bias: 16.78266143798828, loss: 40.31740937081047\n",
      "Epoch: 900/5000\n",
      "w1: 17.895353317260742, w2: -11.898761749267578, bias: 16.782249450683594, loss: 40.13274890578939\n",
      "Epoch: 910/5000\n",
      "w1: 17.963294982910156, w2: -12.014842987060547, bias: 16.781829833984375, loss: 39.951577103002656\n",
      "Epoch: 920/5000\n",
      "w1: 18.030611038208008, w2: -12.12981128692627, bias: 16.78139877319336, loss: 39.773829338425486\n",
      "Epoch: 930/5000\n",
      "w1: 18.097307205200195, w2: -12.24367618560791, bias: 16.780960083007812, loss: 39.599438600117935\n",
      "Epoch: 940/5000\n",
      "w1: 18.163389205932617, w2: -12.356451034545898, bias: 16.78050994873047, loss: 39.42834084169731\n",
      "Epoch: 950/5000\n",
      "w1: 18.228866577148438, w2: -12.468143463134766, bias: 16.780052185058594, loss: 39.26047448482956\n",
      "Epoch: 960/5000\n",
      "w1: 18.293737411499023, w2: -12.578764915466309, bias: 16.779584884643555, loss: 39.0957796905641\n",
      "Epoch: 970/5000\n",
      "w1: 18.358013153076172, w2: -12.68832778930664, bias: 16.77910804748535, loss: 38.934193900242974\n",
      "Epoch: 980/5000\n",
      "w1: 18.42169761657715, w2: -12.79684066772461, bias: 16.778623580932617, loss: 38.77565793212175\n",
      "Epoch: 990/5000\n",
      "w1: 18.48480224609375, w2: -12.904309272766113, bias: 16.778127670288086, loss: 38.62011725754139\n",
      "Epoch: 1000/5000\n",
      "w1: 18.547317504882812, w2: -13.010747909545898, bias: 16.77762794494629, loss: 38.467518677320385\n",
      "Epoch: 1010/5000\n",
      "w1: 18.6092586517334, w2: -13.116164207458496, bias: 16.77711296081543, loss: 38.3178030942905\n",
      "Epoch: 1020/5000\n",
      "w1: 18.670637130737305, w2: -13.220568656921387, bias: 16.77659797668457, loss: 38.1709126221554\n",
      "Epoch: 1030/5000\n",
      "w1: 18.73145294189453, w2: -13.323972702026367, bias: 16.776065826416016, loss: 38.02679387806149\n",
      "Epoch: 1040/5000\n",
      "w1: 18.791709899902344, w2: -13.426383018493652, bias: 16.775531768798828, loss: 37.885398735943454\n",
      "Epoch: 1050/5000\n",
      "w1: 18.85141372680664, w2: -13.527811050415039, bias: 16.774986267089844, loss: 37.74667323036341\n",
      "Epoch: 1060/5000\n",
      "w1: 18.910572052001953, w2: -13.628266334533691, bias: 16.774433135986328, loss: 37.610567402634544\n",
      "Epoch: 1070/5000\n",
      "w1: 18.96918487548828, w2: -13.72775650024414, bias: 16.77387237548828, loss: 37.477032301477855\n",
      "Epoch: 1080/5000\n",
      "w1: 19.02726173400879, w2: -13.826290130615234, bias: 16.773300170898438, loss: 37.346020481253625\n",
      "Epoch: 1090/5000\n",
      "w1: 19.084806442260742, w2: -13.923881530761719, bias: 16.772727966308594, loss: 37.21747981533902\n",
      "Epoch: 1100/5000\n",
      "w1: 19.141822814941406, w2: -14.02053451538086, bias: 16.772138595581055, loss: 37.091368484504315\n",
      "Epoch: 1110/5000\n",
      "w1: 19.198318481445312, w2: -14.116259574890137, bias: 16.771547317504883, loss: 36.967637482773526\n",
      "Epoch: 1120/5000\n",
      "w1: 19.254295349121094, w2: -14.211065292358398, bias: 16.77094841003418, loss: 36.846243758351335\n",
      "Epoch: 1130/5000\n",
      "w1: 19.30976104736328, w2: -14.304959297180176, bias: 16.77033805847168, loss: 36.72714262935961\n",
      "Epoch: 1140/5000\n",
      "w1: 19.364717483520508, w2: -14.397953033447266, bias: 16.76972770690918, loss: 36.610289888417626\n",
      "Epoch: 1150/5000\n",
      "w1: 19.419172286987305, w2: -14.490052223205566, bias: 16.769102096557617, loss: 36.495644227366974\n",
      "Epoch: 1160/5000\n",
      "w1: 19.473129272460938, w2: -14.581267356872559, bias: 16.76847267150879, loss: 36.38316338977692\n",
      "Epoch: 1170/5000\n",
      "w1: 19.526592254638672, w2: -14.67160701751709, bias: 16.767839431762695, loss: 36.27280596813302\n",
      "Epoch: 1180/5000\n",
      "w1: 19.57956886291504, w2: -14.761076927185059, bias: 16.76719093322754, loss: 36.16453267441383\n",
      "Epoch: 1190/5000\n",
      "w1: 19.63205909729004, w2: -14.849687576293945, bias: 16.766542434692383, loss: 36.05830452652216\n",
      "Epoch: 1200/5000\n",
      "w1: 19.684072494506836, w2: -14.937446594238281, bias: 16.765884399414062, loss: 35.95408074264275\n",
      "Epoch: 1210/5000\n",
      "w1: 19.73560905456543, w2: -15.024362564086914, bias: 16.765216827392578, loss: 35.851825349652465\n",
      "Epoch: 1220/5000\n",
      "w1: 19.78667449951172, w2: -15.110442161560059, bias: 16.764549255371094, loss: 35.75150175942404\n",
      "Epoch: 1230/5000\n",
      "w1: 19.8372745513916, w2: -15.195694923400879, bias: 16.763870239257812, loss: 35.65307237184198\n",
      "Epoch: 1240/5000\n",
      "w1: 19.887414932250977, w2: -15.280128479003906, bias: 16.76318359375, loss: 35.55649919214092\n",
      "Epoch: 1250/5000\n",
      "w1: 19.937095642089844, w2: -15.363749504089355, bias: 16.762496948242188, loss: 35.46175112336975\n",
      "Epoch: 1260/5000\n",
      "w1: 19.9863224029541, w2: -15.446566581726074, bias: 16.761796951293945, loss: 35.36879219554381\n",
      "Epoch: 1270/5000\n",
      "w1: 20.03510284423828, w2: -15.528587341308594, bias: 16.761091232299805, loss: 35.277587946468046\n",
      "Epoch: 1280/5000\n",
      "w1: 20.083438873291016, w2: -15.609819412231445, bias: 16.760385513305664, loss: 35.18810544538165\n",
      "Epoch: 1290/5000\n",
      "w1: 20.13133430480957, w2: -15.690269470214844, bias: 16.759668350219727, loss: 35.10031218381605\n",
      "Epoch: 1300/5000\n",
      "w1: 20.17879295349121, w2: -15.76994514465332, bias: 16.758943557739258, loss: 35.0141770127787\n",
      "Epoch: 1310/5000\n",
      "w1: 20.225818634033203, w2: -15.848852157592773, bias: 16.75821876525879, loss: 34.92967010203727\n",
      "Epoch: 1320/5000\n",
      "w1: 20.272417068481445, w2: -15.927002906799316, bias: 16.75748634338379, loss: 34.84675678153044\n",
      "Epoch: 1330/5000\n",
      "w1: 20.318592071533203, w2: -16.0044002532959, bias: 16.756742477416992, loss: 34.76540876154539\n",
      "Epoch: 1340/5000\n",
      "w1: 20.364347457885742, w2: -16.081052780151367, bias: 16.755998611450195, loss: 34.68559629074666\n",
      "Epoch: 1350/5000\n",
      "w1: 20.409687042236328, w2: -16.15696907043457, bias: 16.755250930786133, loss: 34.60729000698664\n",
      "Epoch: 1360/5000\n",
      "w1: 20.454614639282227, w2: -16.232152938842773, bias: 16.754487991333008, loss: 34.53046322157857\n",
      "Epoch: 1370/5000\n",
      "w1: 20.499135971069336, w2: -16.30661392211914, bias: 16.753725051879883, loss: 34.45508452654029\n",
      "Epoch: 1380/5000\n",
      "w1: 20.543251037597656, w2: -16.380355834960938, bias: 16.752962112426758, loss: 34.381129698297165\n",
      "Epoch: 1390/5000\n",
      "w1: 20.58696746826172, w2: -16.453388214111328, bias: 16.752185821533203, loss: 34.30857198567661\n",
      "Epoch: 1400/5000\n",
      "w1: 20.63028335571289, w2: -16.525718688964844, bias: 16.75140380859375, loss: 34.23738436981979\n",
      "Epoch: 1410/5000\n",
      "w1: 20.673208236694336, w2: -16.59735107421875, bias: 16.750621795654297, loss: 34.16753869434283\n",
      "Epoch: 1420/5000\n",
      "w1: 20.715742111206055, w2: -16.668292999267578, bias: 16.74983787536621, loss: 34.099013993686185\n",
      "Epoch: 1430/5000\n",
      "w1: 20.75789451599121, w2: -16.738554000854492, bias: 16.74903678894043, loss: 34.031779799766035\n",
      "Epoch: 1440/5000\n",
      "w1: 20.799665451049805, w2: -16.808134078979492, bias: 16.74823570251465, loss: 33.96581528352218\n",
      "Epoch: 1450/5000\n",
      "w1: 20.841054916381836, w2: -16.877046585083008, bias: 16.747434616088867, loss: 33.901095816796484\n",
      "Epoch: 1460/5000\n",
      "w1: 20.88206672668457, w2: -16.945293426513672, bias: 16.746627807617188, loss: 33.83760157135098\n",
      "Epoch: 1470/5000\n",
      "w1: 20.922712326049805, w2: -17.012882232666016, bias: 16.745807647705078, loss: 33.775301081630914\n",
      "Epoch: 1480/5000\n",
      "w1: 20.962989807128906, w2: -17.07982063293457, bias: 16.74498748779297, loss: 33.71417728882301\n",
      "Epoch: 1490/5000\n",
      "w1: 21.00290298461914, w2: -17.1461124420166, bias: 16.74416732788086, loss: 33.65420737927294\n",
      "Epoch: 1500/5000\n",
      "w1: 21.04245376586914, w2: -17.211763381958008, bias: 16.74333953857422, loss: 33.59537163002969\n",
      "Epoch: 1510/5000\n",
      "w1: 21.081647872924805, w2: -17.276782989501953, bias: 16.74250030517578, loss: 33.53764489841211\n",
      "Epoch: 1520/5000\n",
      "w1: 21.1204891204834, w2: -17.341175079345703, bias: 16.741661071777344, loss: 33.48100612767444\n",
      "Epoch: 1530/5000\n",
      "w1: 21.158979415893555, w2: -17.40494728088379, bias: 16.740821838378906, loss: 33.42543694714444\n",
      "Epoch: 1540/5000\n",
      "w1: 21.197120666503906, w2: -17.468103408813477, bias: 16.739978790283203, loss: 33.37091705808649\n",
      "Epoch: 1550/5000\n",
      "w1: 21.23491859436035, w2: -17.530649185180664, bias: 16.739120483398438, loss: 33.317425666762475\n",
      "Epoch: 1560/5000\n",
      "w1: 21.272377014160156, w2: -17.592592239379883, bias: 16.738262176513672, loss: 33.26494214874606\n",
      "Epoch: 1570/5000\n",
      "w1: 21.309497833251953, w2: -17.6539363861084, bias: 16.737403869628906, loss: 33.21345030779954\n",
      "Epoch: 1580/5000\n",
      "w1: 21.346281051635742, w2: -17.714689254760742, bias: 16.73654556274414, loss: 33.16293093776056\n",
      "Epoch: 1590/5000\n",
      "w1: 21.382734298706055, w2: -17.774856567382812, bias: 16.735671997070312, loss: 33.113363630056945\n",
      "Epoch: 1600/5000\n",
      "w1: 21.418859481811523, w2: -17.83443832397461, bias: 16.73479461669922, loss: 33.06473333185564\n",
      "Epoch: 1610/5000\n",
      "w1: 21.454660415649414, w2: -17.893442153930664, bias: 16.733917236328125, loss: 33.01702244668631\n",
      "Epoch: 1620/5000\n",
      "w1: 21.490140914916992, w2: -17.95187759399414, bias: 16.73303985595703, loss: 32.97021163573098\n",
      "Epoch: 1630/5000\n",
      "w1: 21.52530288696289, w2: -18.009756088256836, bias: 16.732158660888672, loss: 32.92427727520316\n",
      "Epoch: 1640/5000\n",
      "w1: 21.56014633178711, w2: -18.06707191467285, bias: 16.73126220703125, loss: 32.879212953832806\n",
      "Epoch: 1650/5000\n",
      "w1: 21.594676971435547, w2: -18.12383270263672, bias: 16.730365753173828, loss: 32.834999683701575\n",
      "Epoch: 1660/5000\n",
      "w1: 21.62889862060547, w2: -18.180044174194336, bias: 16.729469299316406, loss: 32.79162018579407\n",
      "Epoch: 1670/5000\n",
      "w1: 21.662811279296875, w2: -18.235713958740234, bias: 16.728572845458984, loss: 32.74905942412351\n",
      "Epoch: 1680/5000\n",
      "w1: 21.69641876220703, w2: -18.290843963623047, bias: 16.72767448425293, loss: 32.707302369075975\n",
      "Epoch: 1690/5000\n",
      "w1: 21.729724884033203, w2: -18.345441818237305, bias: 16.72675895690918, loss: 32.66633198248878\n",
      "Epoch: 1700/5000\n",
      "w1: 21.76273536682129, w2: -18.39950942993164, bias: 16.72584342956543, loss: 32.626134967817116\n",
      "Epoch: 1710/5000\n",
      "w1: 21.795448303222656, w2: -18.45305633544922, bias: 16.72492790222168, loss: 32.58669561883812\n",
      "Epoch: 1720/5000\n",
      "w1: 21.827869415283203, w2: -18.506084442138672, bias: 16.72401237487793, loss: 32.54800052885923\n",
      "Epoch: 1730/5000\n",
      "w1: 21.85999870300293, w2: -18.5585994720459, bias: 16.72309684753418, loss: 32.51003549706391\n",
      "Epoch: 1740/5000\n",
      "w1: 21.891841888427734, w2: -18.610607147216797, bias: 16.722169876098633, loss: 32.472786251412714\n",
      "Epoch: 1750/5000\n",
      "w1: 21.923398971557617, w2: -18.662111282348633, bias: 16.721235275268555, loss: 32.43623869053991\n",
      "Epoch: 1760/5000\n",
      "w1: 21.954675674438477, w2: -18.713115692138672, bias: 16.720300674438477, loss: 32.4003811431224\n",
      "Epoch: 1770/5000\n",
      "w1: 21.985671997070312, w2: -18.763628005981445, bias: 16.7193660736084, loss: 32.36520052728748\n",
      "Epoch: 1780/5000\n",
      "w1: 22.01639175415039, w2: -18.813650131225586, bias: 16.71843147277832, loss: 32.330682015310124\n",
      "Epoch: 1790/5000\n",
      "w1: 22.046836853027344, w2: -18.863187789916992, bias: 16.717496871948242, loss: 32.296815839127405\n",
      "Epoch: 1800/5000\n",
      "w1: 22.077009201049805, w2: -18.91224479675293, bias: 16.7165470123291, loss: 32.26358862707242\n",
      "Epoch: 1810/5000\n",
      "w1: 22.106914520263672, w2: -18.960824966430664, bias: 16.715593338012695, loss: 32.2309877981275\n",
      "Epoch: 1820/5000\n",
      "w1: 22.136552810668945, w2: -19.00893783569336, bias: 16.71463966369629, loss: 32.19899992170756\n",
      "Epoch: 1830/5000\n",
      "w1: 22.16592788696289, w2: -19.056583404541016, bias: 16.713685989379883, loss: 32.16761608856399\n",
      "Epoch: 1840/5000\n",
      "w1: 22.19504165649414, w2: -19.10376739501953, bias: 16.712732315063477, loss: 32.13682354995558\n",
      "Epoch: 1850/5000\n",
      "w1: 22.223896026611328, w2: -19.150493621826172, bias: 16.71177864074707, loss: 32.10661212142816\n",
      "Epoch: 1860/5000\n",
      "w1: 22.252492904663086, w2: -19.196767807006836, bias: 16.710817337036133, loss: 32.076969204190576\n",
      "Epoch: 1870/5000\n",
      "w1: 22.28083610534668, w2: -19.242591857910156, bias: 16.7098445892334, loss: 32.047886645894316\n",
      "Epoch: 1880/5000\n",
      "w1: 22.308927536010742, w2: -19.28797149658203, bias: 16.708871841430664, loss: 32.019352059424314\n",
      "Epoch: 1890/5000\n",
      "w1: 22.336769104003906, w2: -19.332910537719727, bias: 16.70789909362793, loss: 31.991355413853736\n",
      "Epoch: 1900/5000\n",
      "w1: 22.364364624023438, w2: -19.37741470336914, bias: 16.706926345825195, loss: 31.963885468595922\n",
      "Epoch: 1910/5000\n",
      "w1: 22.391714096069336, w2: -19.42148780822754, bias: 16.70595359802246, loss: 31.936933905074316\n",
      "Epoch: 1920/5000\n",
      "w1: 22.418821334838867, w2: -19.465131759643555, bias: 16.704980850219727, loss: 31.91048982113918\n",
      "Epoch: 1930/5000\n",
      "w1: 22.44568634033203, w2: -19.508352279663086, bias: 16.704004287719727, loss: 31.884545672560602\n",
      "Epoch: 1940/5000\n",
      "w1: 22.472314834594727, w2: -19.5511531829834, bias: 16.703012466430664, loss: 31.85908971131339\n",
      "Epoch: 1950/5000\n",
      "w1: 22.498706817626953, w2: -19.593538284301758, bias: 16.7020206451416, loss: 31.834113052117484\n",
      "Epoch: 1960/5000\n",
      "w1: 22.52486801147461, w2: -19.635509490966797, bias: 16.70102882385254, loss: 31.809608382171213\n",
      "Epoch: 1970/5000\n",
      "w1: 22.550796508789062, w2: -19.67707633972168, bias: 16.700037002563477, loss: 31.78556422823953\n",
      "Epoch: 1980/5000\n",
      "w1: 22.576496124267578, w2: -19.718238830566406, bias: 16.699045181274414, loss: 31.761973079502326\n",
      "Epoch: 1990/5000\n",
      "w1: 22.60196876525879, w2: -19.75899887084961, bias: 16.69805335998535, loss: 31.73882723178646\n",
      "Epoch: 2000/5000\n",
      "w1: 22.627216339111328, w2: -19.799362182617188, bias: 16.69706153869629, loss: 31.716117556865054\n",
      "Epoch: 2010/5000\n",
      "w1: 22.652240753173828, w2: -19.839340209960938, bias: 16.696067810058594, loss: 31.69383405115757\n",
      "Epoch: 2020/5000\n",
      "w1: 22.677043914794922, w2: -19.878917694091797, bias: 16.695056915283203, loss: 31.671973581737472\n",
      "Epoch: 2030/5000\n",
      "w1: 22.701629638671875, w2: -19.918113708496094, bias: 16.694046020507812, loss: 31.65052430921581\n",
      "Epoch: 2040/5000\n",
      "w1: 22.72599983215332, w2: -19.956928253173828, bias: 16.693035125732422, loss: 31.62947872856374\n",
      "Epoch: 2050/5000\n",
      "w1: 22.750152587890625, w2: -19.995361328125, bias: 16.69202423095703, loss: 31.608831099532864\n",
      "Epoch: 2060/5000\n",
      "w1: 22.774093627929688, w2: -20.033430099487305, bias: 16.69101333618164, loss: 31.588569499272797\n",
      "Epoch: 2070/5000\n",
      "w1: 22.79782485961914, w2: -20.071123123168945, bias: 16.69000244140625, loss: 31.56869115555591\n",
      "Epoch: 2080/5000\n",
      "w1: 22.821348190307617, w2: -20.108449935913086, bias: 16.68899154663086, loss: 31.549185889637954\n",
      "Epoch: 2090/5000\n",
      "w1: 22.84466552734375, w2: -20.145414352416992, bias: 16.68798065185547, loss: 31.530048411631466\n",
      "Epoch: 2100/5000\n",
      "w1: 22.867773056030273, w2: -20.18202018737793, bias: 16.686969757080078, loss: 31.511272091773925\n",
      "Epoch: 2110/5000\n",
      "w1: 22.89068031311035, w2: -20.2182674407959, bias: 16.685941696166992, loss: 31.492848488564675\n",
      "Epoch: 2120/5000\n",
      "w1: 22.913387298583984, w2: -20.254159927368164, bias: 16.684911727905273, loss: 31.47477319503275\n",
      "Epoch: 2130/5000\n",
      "w1: 22.935901641845703, w2: -20.289703369140625, bias: 16.683881759643555, loss: 31.457035274422264\n",
      "Epoch: 2140/5000\n",
      "w1: 22.958208084106445, w2: -20.324899673461914, bias: 16.682851791381836, loss: 31.43963489102621\n",
      "Epoch: 2150/5000\n",
      "w1: 22.980323791503906, w2: -20.359752655029297, bias: 16.681821823120117, loss: 31.422561009020278\n",
      "Epoch: 2160/5000\n",
      "w1: 23.002248764038086, w2: -20.39426612854004, bias: 16.6807918548584, loss: 31.405808340237474\n",
      "Epoch: 2170/5000\n",
      "w1: 23.023983001708984, w2: -20.428442001342773, bias: 16.67976188659668, loss: 31.389370368301687\n",
      "Epoch: 2180/5000\n",
      "w1: 23.0455265045166, w2: -20.462284088134766, bias: 16.67873191833496, loss: 31.373242624574207\n",
      "Epoch: 2190/5000\n",
      "w1: 23.066879272460938, w2: -20.49579620361328, bias: 16.677701950073242, loss: 31.357418137492715\n",
      "Epoch: 2200/5000\n",
      "w1: 23.088041305541992, w2: -20.528982162475586, bias: 16.676671981811523, loss: 31.34189259820294\n",
      "Epoch: 2210/5000\n",
      "w1: 23.109027862548828, w2: -20.561843872070312, bias: 16.675642013549805, loss: 31.32665770051176\n",
      "Epoch: 2220/5000\n",
      "w1: 23.12982749938965, w2: -20.59438705444336, bias: 16.674598693847656, loss: 31.311709553165063\n",
      "Epoch: 2230/5000\n",
      "w1: 23.150447845458984, w2: -20.626611709594727, bias: 16.673551559448242, loss: 31.29704212500964\n",
      "Epoch: 2240/5000\n",
      "w1: 23.17088508605957, w2: -20.658517837524414, bias: 16.672502517700195, loss: 31.28265207401354\n",
      "Epoch: 2250/5000\n",
      "w1: 23.191150665283203, w2: -20.690113067626953, bias: 16.67145347595215, loss: 31.26853211286755\n",
      "Epoch: 2260/5000\n",
      "w1: 23.211238861083984, w2: -20.721399307250977, bias: 16.6704044342041, loss: 31.25467798063498\n",
      "Epoch: 2270/5000\n",
      "w1: 23.23115348815918, w2: -20.752378463745117, bias: 16.669355392456055, loss: 31.24108454801803\n",
      "Epoch: 2280/5000\n",
      "w1: 23.25089454650879, w2: -20.78305435180664, bias: 16.668306350708008, loss: 31.227746938858335\n",
      "Epoch: 2290/5000\n",
      "w1: 23.270463943481445, w2: -20.813432693481445, bias: 16.66725730895996, loss: 31.214659986397457\n",
      "Epoch: 2300/5000\n",
      "w1: 23.28986358642578, w2: -20.8435115814209, bias: 16.666208267211914, loss: 31.201819188030523\n",
      "Epoch: 2310/5000\n",
      "w1: 23.309093475341797, w2: -20.8732967376709, bias: 16.665159225463867, loss: 31.189220475745074\n",
      "Epoch: 2320/5000\n",
      "w1: 23.32815933227539, w2: -20.90279197692871, bias: 16.66411018371582, loss: 31.17685743262193\n",
      "Epoch: 2330/5000\n",
      "w1: 23.34705924987793, w2: -20.93199348449707, bias: 16.663061141967773, loss: 31.164728583498036\n",
      "Epoch: 2340/5000\n",
      "w1: 23.365793228149414, w2: -20.96091079711914, bias: 16.662012100219727, loss: 31.15282817996963\n",
      "Epoch: 2350/5000\n",
      "w1: 23.384368896484375, w2: -20.98954963684082, bias: 16.660961151123047, loss: 31.141149189972737\n",
      "Epoch: 2360/5000\n",
      "w1: 23.402780532836914, w2: -21.01790428161621, bias: 16.659900665283203, loss: 31.129691209550007\n",
      "Epoch: 2370/5000\n",
      "w1: 23.421035766601562, w2: -21.045982360839844, bias: 16.658836364746094, loss: 31.118447857889063\n",
      "Epoch: 2380/5000\n",
      "w1: 23.43913459777832, w2: -21.07378387451172, bias: 16.65777015686035, loss: 31.107415741457714\n",
      "Epoch: 2390/5000\n",
      "w1: 23.45707893371582, w2: -21.1013126373291, bias: 16.65670394897461, loss: 31.096591155047527\n",
      "Epoch: 2400/5000\n",
      "w1: 23.47486686706543, w2: -21.128570556640625, bias: 16.655635833740234, loss: 31.085970444544195\n",
      "Epoch: 2410/5000\n",
      "w1: 23.492502212524414, w2: -21.155561447143555, bias: 16.654569625854492, loss: 31.075549048444596\n",
      "Epoch: 2420/5000\n",
      "w1: 23.50998878479004, w2: -21.18228530883789, bias: 16.653501510620117, loss: 31.065323430613542\n",
      "Epoch: 2430/5000\n",
      "w1: 23.527324676513672, w2: -21.20874786376953, bias: 16.652433395385742, loss: 31.055289646029447\n",
      "Epoch: 2440/5000\n",
      "w1: 23.544513702392578, w2: -21.234949111938477, bias: 16.651365280151367, loss: 31.045445141251637\n",
      "Epoch: 2450/5000\n",
      "w1: 23.561553955078125, w2: -21.260894775390625, bias: 16.650297164916992, loss: 31.035784424561744\n",
      "Epoch: 2460/5000\n",
      "w1: 23.578447341918945, w2: -21.28658676147461, bias: 16.649229049682617, loss: 31.02630538619002\n",
      "Epoch: 2470/5000\n",
      "w1: 23.595199584960938, w2: -21.31202507019043, bias: 16.648160934448242, loss: 31.017004319222085\n",
      "Epoch: 2480/5000\n",
      "w1: 23.611806869506836, w2: -21.33721160888672, bias: 16.647092819213867, loss: 31.00787821342795\n",
      "Epoch: 2490/5000\n",
      "w1: 23.628273010253906, w2: -21.362150192260742, bias: 16.646024703979492, loss: 30.99892362809859\n",
      "Epoch: 2500/5000\n",
      "w1: 23.644596099853516, w2: -21.386842727661133, bias: 16.644956588745117, loss: 30.990137652668736\n",
      "Epoch: 2510/5000\n",
      "w1: 23.66077995300293, w2: -21.411293029785156, bias: 16.643888473510742, loss: 30.981516031670036\n",
      "Epoch: 2520/5000\n",
      "w1: 23.67682647705078, w2: -21.435503005981445, bias: 16.642820358276367, loss: 30.973056739782603\n",
      "Epoch: 2530/5000\n",
      "w1: 23.69273567199707, w2: -21.45947265625, bias: 16.641752243041992, loss: 30.96475624852387\n",
      "Epoch: 2540/5000\n",
      "w1: 23.708507537841797, w2: -21.48320960998535, bias: 16.640684127807617, loss: 30.956611397575937\n",
      "Epoch: 2550/5000\n",
      "w1: 23.724145889282227, w2: -21.506711959838867, bias: 16.639616012573242, loss: 30.9486193820693\n",
      "Epoch: 2560/5000\n",
      "w1: 23.739652633666992, w2: -21.529983520507812, bias: 16.63853645324707, loss: 30.94077674647386\n",
      "Epoch: 2570/5000\n",
      "w1: 23.75502586364746, w2: -21.553024291992188, bias: 16.63745880126953, loss: 30.93308176573821\n",
      "Epoch: 2580/5000\n",
      "w1: 23.77027130126953, w2: -21.575836181640625, bias: 16.636377334594727, loss: 30.92553098952484\n",
      "Epoch: 2590/5000\n",
      "w1: 23.785385131835938, w2: -21.59842300415039, bias: 16.635297775268555, loss: 30.918122144191923\n",
      "Epoch: 2600/5000\n",
      "w1: 23.800371170043945, w2: -21.620786666870117, bias: 16.634220123291016, loss: 30.910852478375\n",
      "Epoch: 2610/5000\n",
      "w1: 23.81523323059082, w2: -21.64293098449707, bias: 16.633136749267578, loss: 30.903718439379897\n",
      "Epoch: 2620/5000\n",
      "w1: 23.829971313476562, w2: -21.664854049682617, bias: 16.632055282592773, loss: 30.89671824883421\n",
      "Epoch: 2630/5000\n",
      "w1: 23.844581604003906, w2: -21.686561584472656, bias: 16.630971908569336, loss: 30.88984963006606\n",
      "Epoch: 2640/5000\n",
      "w1: 23.859071731567383, w2: -21.70805549621582, bias: 16.6298885345459, loss: 30.88310930003944\n",
      "Epoch: 2650/5000\n",
      "w1: 23.87343978881836, w2: -21.72933578491211, bias: 16.628803253173828, loss: 30.87649516685232\n",
      "Epoch: 2660/5000\n",
      "w1: 23.88768768310547, w2: -21.75040626525879, bias: 16.62771987915039, loss: 30.870004947466978\n",
      "Epoch: 2670/5000\n",
      "w1: 23.90181541442871, w2: -21.77126693725586, bias: 16.62663459777832, loss: 30.863636571767397\n",
      "Epoch: 2680/5000\n",
      "w1: 23.91582489013672, w2: -21.791919708251953, bias: 16.62554931640625, loss: 30.857387664964573\n",
      "Epoch: 2690/5000\n",
      "w1: 23.929716110229492, w2: -21.81237030029297, bias: 16.62446403503418, loss: 30.851255360257568\n",
      "Epoch: 2700/5000\n",
      "w1: 23.943492889404297, w2: -21.832616806030273, bias: 16.623376846313477, loss: 30.84523823157837\n",
      "Epoch: 2710/5000\n",
      "w1: 23.957151412963867, w2: -21.852663040161133, bias: 16.622291564941406, loss: 30.83933341210997\n",
      "Epoch: 2720/5000\n",
      "w1: 23.9706974029541, w2: -21.872509002685547, bias: 16.621204376220703, loss: 30.833540024177005\n",
      "Epoch: 2730/5000\n",
      "w1: 23.984130859375, w2: -21.89216423034668, bias: 16.6201171875, loss: 30.827853486298938\n",
      "Epoch: 2740/5000\n",
      "w1: 23.997451782226562, w2: -21.911617279052734, bias: 16.61903190612793, loss: 30.822274423009052\n",
      "Epoch: 2750/5000\n",
      "w1: 24.010662078857422, w2: -21.930871963500977, bias: 16.617944717407227, loss: 30.816800998510775\n",
      "Epoch: 2760/5000\n",
      "w1: 24.023761749267578, w2: -21.949935913085938, bias: 16.616857528686523, loss: 30.811429900500965\n",
      "Epoch: 2770/5000\n",
      "w1: 24.03675079345703, w2: -21.968809127807617, bias: 16.61577033996582, loss: 30.806159675588166\n",
      "Epoch: 2780/5000\n",
      "w1: 24.049633026123047, w2: -21.987510681152344, bias: 16.614683151245117, loss: 30.800985444869774\n",
      "Epoch: 2790/5000\n",
      "w1: 24.062408447265625, w2: -22.00602149963379, bias: 16.613595962524414, loss: 30.7959082100604\n",
      "Epoch: 2800/5000\n",
      "w1: 24.0750789642334, w2: -22.024341583251953, bias: 16.61250877380371, loss: 30.79092717574809\n",
      "Epoch: 2810/5000\n",
      "w1: 24.087642669677734, w2: -22.0424861907959, bias: 16.611421585083008, loss: 30.786038304820043\n",
      "Epoch: 2820/5000\n",
      "w1: 24.1001033782959, w2: -22.060443878173828, bias: 16.610334396362305, loss: 30.781241665917356\n",
      "Epoch: 2830/5000\n",
      "w1: 24.112459182739258, w2: -22.078229904174805, bias: 16.6092472076416, loss: 30.776533674255568\n",
      "Epoch: 2840/5000\n",
      "w1: 24.124713897705078, w2: -22.095834732055664, bias: 16.6081600189209, loss: 30.771914422116755\n",
      "Epoch: 2850/5000\n",
      "w1: 24.136865615844727, w2: -22.113264083862305, bias: 16.607072830200195, loss: 30.767381643494222\n",
      "Epoch: 2860/5000\n",
      "w1: 24.1489200592041, w2: -22.13051986694336, bias: 16.605985641479492, loss: 30.762933092579633\n",
      "Epoch: 2870/5000\n",
      "w1: 24.160871505737305, w2: -22.14760398864746, bias: 16.60489845275879, loss: 30.758567706857978\n",
      "Epoch: 2880/5000\n",
      "w1: 24.172725677490234, w2: -22.164518356323242, bias: 16.603811264038086, loss: 30.75428375729171\n",
      "Epoch: 2890/5000\n",
      "w1: 24.184484481811523, w2: -22.181262969970703, bias: 16.602724075317383, loss: 30.75007964094035\n",
      "Epoch: 2900/5000\n",
      "w1: 24.196147918701172, w2: -22.197839736938477, bias: 16.60163688659668, loss: 30.745953909016716\n",
      "Epoch: 2910/5000\n",
      "w1: 24.20771598815918, w2: -22.214250564575195, bias: 16.600549697875977, loss: 30.741905447775967\n",
      "Epoch: 2920/5000\n",
      "w1: 24.219188690185547, w2: -22.230499267578125, bias: 16.599462509155273, loss: 30.73793192009241\n",
      "Epoch: 2930/5000\n",
      "w1: 24.230566024780273, w2: -22.246583938598633, bias: 16.59837532043457, loss: 30.734033186805195\n",
      "Epoch: 2940/5000\n",
      "w1: 24.24184799194336, w2: -22.262508392333984, bias: 16.597288131713867, loss: 30.730206965443795\n",
      "Epoch: 2950/5000\n",
      "w1: 24.253036499023438, w2: -22.27827262878418, bias: 16.596200942993164, loss: 30.72645282848925\n",
      "Epoch: 2960/5000\n",
      "w1: 24.264137268066406, w2: -22.293880462646484, bias: 16.59511375427246, loss: 30.722767797952063\n",
      "Epoch: 2970/5000\n",
      "w1: 24.275142669677734, w2: -22.3093318939209, bias: 16.594026565551758, loss: 30.71915165456548\n",
      "Epoch: 2980/5000\n",
      "w1: 24.286062240600586, w2: -22.324628829956055, bias: 16.592939376831055, loss: 30.715602671918298\n",
      "Epoch: 2990/5000\n",
      "w1: 24.296892166137695, w2: -22.339773178100586, bias: 16.59185218811035, loss: 30.712119938887962\n",
      "Epoch: 3000/5000\n",
      "w1: 24.307632446289062, w2: -22.354764938354492, bias: 16.59076499938965, loss: 30.708702105138684\n",
      "Epoch: 3010/5000\n",
      "w1: 24.318286895751953, w2: -22.369606018066406, bias: 16.589677810668945, loss: 30.705348247562135\n",
      "Epoch: 3020/5000\n",
      "w1: 24.328853607177734, w2: -22.38429832458496, bias: 16.588590621948242, loss: 30.702056414231656\n",
      "Epoch: 3030/5000\n",
      "w1: 24.339336395263672, w2: -22.398841857910156, bias: 16.58750343322754, loss: 30.6988260060019\n",
      "Epoch: 3040/5000\n",
      "w1: 24.349733352661133, w2: -22.413238525390625, bias: 16.586416244506836, loss: 30.69565567656361\n",
      "Epoch: 3050/5000\n",
      "w1: 24.36004638671875, w2: -22.427492141723633, bias: 16.585329055786133, loss: 30.692544497412413\n",
      "Epoch: 3060/5000\n",
      "w1: 24.370275497436523, w2: -22.441600799560547, bias: 16.58424186706543, loss: 30.689490969551105\n",
      "Epoch: 3070/5000\n",
      "w1: 24.38042449951172, w2: -22.455568313598633, bias: 16.583154678344727, loss: 30.686494276306135\n",
      "Epoch: 3080/5000\n",
      "w1: 24.390491485595703, w2: -22.469396591186523, bias: 16.582067489624023, loss: 30.68355308372502\n",
      "Epoch: 3090/5000\n",
      "w1: 24.400476455688477, w2: -22.483083724975586, bias: 16.58098030090332, loss: 30.680666606562767\n",
      "Epoch: 3100/5000\n",
      "w1: 24.410381317138672, w2: -22.496633529663086, bias: 16.579893112182617, loss: 30.677833690551672\n",
      "Epoch: 3110/5000\n",
      "w1: 24.420207977294922, w2: -22.510046005249023, bias: 16.578805923461914, loss: 30.675053506495217\n",
      "Epoch: 3120/5000\n",
      "w1: 24.429954528808594, w2: -22.52332305908203, bias: 16.57771873474121, loss: 30.672325051558783\n",
      "Epoch: 3130/5000\n",
      "w1: 24.439624786376953, w2: -22.536466598510742, bias: 16.576631546020508, loss: 30.669646972793043\n",
      "Epoch: 3140/5000\n",
      "w1: 24.44921875, w2: -22.549476623535156, bias: 16.575544357299805, loss: 30.667018557755416\n",
      "Epoch: 3150/5000\n",
      "w1: 24.458736419677734, w2: -22.56235694885254, bias: 16.5744571685791, loss: 30.66443861172514\n",
      "Epoch: 3160/5000\n",
      "w1: 24.468177795410156, w2: -22.575103759765625, bias: 16.5733699798584, loss: 30.66190693472388\n",
      "Epoch: 3170/5000\n",
      "w1: 24.477542877197266, w2: -22.587724685668945, bias: 16.572282791137695, loss: 30.659422115666864\n",
      "Epoch: 3180/5000\n",
      "w1: 24.486833572387695, w2: -22.600217819213867, bias: 16.571195602416992, loss: 30.656983077123353\n",
      "Epoch: 3190/5000\n",
      "w1: 24.496051788330078, w2: -22.61258316040039, bias: 16.57010841369629, loss: 30.654589407901394\n",
      "Epoch: 3200/5000\n",
      "w1: 24.50519561767578, w2: -22.62482261657715, bias: 16.569021224975586, loss: 30.652240287215005\n",
      "Epoch: 3210/5000\n",
      "w1: 24.514266967773438, w2: -22.63693618774414, bias: 16.567935943603516, loss: 30.649934638965135\n",
      "Epoch: 3220/5000\n",
      "w1: 24.52326774597168, w2: -22.648927688598633, bias: 16.566852569580078, loss: 30.647671675627098\n",
      "Epoch: 3230/5000\n",
      "w1: 24.532196044921875, w2: -22.660797119140625, bias: 16.565771102905273, loss: 30.645450509669246\n",
      "Epoch: 3240/5000\n",
      "w1: 24.54105567932129, w2: -22.67254638671875, bias: 16.56468963623047, loss: 30.64327056373198\n",
      "Epoch: 3250/5000\n",
      "w1: 24.54984474182129, w2: -22.68417739868164, bias: 16.563608169555664, loss: 30.641130682107175\n",
      "Epoch: 3260/5000\n",
      "w1: 24.558565139770508, w2: -22.695690155029297, bias: 16.56252670288086, loss: 30.639030465464533\n",
      "Epoch: 3270/5000\n",
      "w1: 24.567214965820312, w2: -22.70708656311035, bias: 16.561447143554688, loss: 30.636968948863903\n",
      "Epoch: 3280/5000\n",
      "w1: 24.57579803466797, w2: -22.718364715576172, bias: 16.560367584228516, loss: 30.634945615055127\n",
      "Epoch: 3290/5000\n",
      "w1: 24.584314346313477, w2: -22.729528427124023, bias: 16.559288024902344, loss: 30.632959697860166\n",
      "Epoch: 3300/5000\n",
      "w1: 24.592761993408203, w2: -22.740577697753906, bias: 16.558208465576172, loss: 30.631010505923605\n",
      "Epoch: 3310/5000\n",
      "w1: 24.601144790649414, w2: -22.751516342163086, bias: 16.557132720947266, loss: 30.62909708143595\n",
      "Epoch: 3320/5000\n",
      "w1: 24.609464645385742, w2: -22.762344360351562, bias: 16.556053161621094, loss: 30.62721860406225\n",
      "Epoch: 3330/5000\n",
      "w1: 24.617713928222656, w2: -22.773059844970703, bias: 16.554977416992188, loss: 30.62537527630419\n",
      "Epoch: 3340/5000\n",
      "w1: 24.625900268554688, w2: -22.783668518066406, bias: 16.553903579711914, loss: 30.623565859006206\n",
      "Epoch: 3350/5000\n",
      "w1: 24.634021759033203, w2: -22.794166564941406, bias: 16.55282974243164, loss: 30.621789738270163\n",
      "Epoch: 3360/5000\n",
      "w1: 24.642080307006836, w2: -22.80455780029297, bias: 16.551755905151367, loss: 30.62004637759559\n",
      "Epoch: 3370/5000\n",
      "w1: 24.650075912475586, w2: -22.814842224121094, bias: 16.550682067871094, loss: 30.618335375916317\n",
      "Epoch: 3380/5000\n",
      "w1: 24.65801429748535, w2: -22.825021743774414, bias: 16.54960823059082, loss: 30.61665517642389\n",
      "Epoch: 3390/5000\n",
      "w1: 24.6658878326416, w2: -22.835098266601562, bias: 16.54853630065918, loss: 30.615006071567084\n",
      "Epoch: 3400/5000\n",
      "w1: 24.6737003326416, w2: -22.845069885253906, bias: 16.54746437072754, loss: 30.61338740910805\n",
      "Epoch: 3410/5000\n",
      "w1: 24.681453704833984, w2: -22.85494041442871, bias: 16.5463924407959, loss: 30.61179840041109\n",
      "Epoch: 3420/5000\n",
      "w1: 24.689146041870117, w2: -22.864707946777344, bias: 16.54532241821289, loss: 30.61023885262104\n",
      "Epoch: 3430/5000\n",
      "w1: 24.696779251098633, w2: -22.874378204345703, bias: 16.544254302978516, loss: 30.60870762880289\n",
      "Epoch: 3440/5000\n",
      "w1: 24.704355239868164, w2: -22.88394546508789, bias: 16.543184280395508, loss: 30.607204511002895\n",
      "Epoch: 3450/5000\n",
      "w1: 24.711872100830078, w2: -22.893415451049805, bias: 16.542116165161133, loss: 30.605729102990765\n",
      "Epoch: 3460/5000\n",
      "w1: 24.719331741333008, w2: -22.902790069580078, bias: 16.541048049926758, loss: 30.604280492261623\n",
      "Epoch: 3470/5000\n",
      "w1: 24.726734161376953, w2: -22.91206932067871, bias: 16.539979934692383, loss: 30.602858302598435\n",
      "Epoch: 3480/5000\n",
      "w1: 24.734079360961914, w2: -22.921253204345703, bias: 16.538911819458008, loss: 30.60146216132586\n",
      "Epoch: 3490/5000\n",
      "w1: 24.74136734008789, w2: -22.930341720581055, bias: 16.537843704223633, loss: 30.600091699310212\n",
      "Epoch: 3500/5000\n",
      "w1: 24.74860191345215, w2: -22.939334869384766, bias: 16.536775588989258, loss: 30.598746275945178\n",
      "Epoch: 3510/5000\n",
      "w1: 24.755781173706055, w2: -22.948232650756836, bias: 16.535707473754883, loss: 30.59742567187931\n",
      "Epoch: 3520/5000\n",
      "w1: 24.762907028198242, w2: -22.957040786743164, bias: 16.534639358520508, loss: 30.59612906179201\n",
      "Epoch: 3530/5000\n",
      "w1: 24.769977569580078, w2: -22.965757369995117, bias: 16.533571243286133, loss: 30.594856076070744\n",
      "Epoch: 3540/5000\n",
      "w1: 24.776996612548828, w2: -22.974382400512695, bias: 16.532503128051758, loss: 30.593606411833168\n",
      "Epoch: 3550/5000\n",
      "w1: 24.78396224975586, w2: -22.98291778564453, bias: 16.531435012817383, loss: 30.592379442475323\n",
      "Epoch: 3560/5000\n",
      "w1: 24.790876388549805, w2: -22.991365432739258, bias: 16.530366897583008, loss: 30.591174847078925\n",
      "Epoch: 3570/5000\n",
      "w1: 24.797739028930664, w2: -22.999723434448242, bias: 16.529298782348633, loss: 30.589992279527433\n",
      "Epoch: 3580/5000\n",
      "w1: 24.804548263549805, w2: -23.00799560546875, bias: 16.528230667114258, loss: 30.588831297600077\n",
      "Epoch: 3590/5000\n",
      "w1: 24.811307907104492, w2: -23.01618003845215, bias: 16.527162551879883, loss: 30.587691310540478\n",
      "Epoch: 3600/5000\n",
      "w1: 24.818017959594727, w2: -23.024280548095703, bias: 16.526094436645508, loss: 30.586572149727903\n",
      "Epoch: 3610/5000\n",
      "w1: 24.824678421020508, w2: -23.03229522705078, bias: 16.525026321411133, loss: 30.585473113583177\n",
      "Epoch: 3620/5000\n",
      "w1: 24.831289291381836, w2: -23.04022789001465, bias: 16.523958206176758, loss: 30.584394044834898\n",
      "Epoch: 3630/5000\n",
      "w1: 24.83785057067871, w2: -23.048076629638672, bias: 16.522891998291016, loss: 30.583334805371475\n",
      "Epoch: 3640/5000\n",
      "w1: 24.844364166259766, w2: -23.055843353271484, bias: 16.521835327148438, loss: 30.58229450617169\n",
      "Epoch: 3650/5000\n",
      "w1: 24.850830078125, w2: -23.063528060913086, bias: 16.520776748657227, loss: 30.581273341144552\n",
      "Epoch: 3660/5000\n",
      "w1: 24.857248306274414, w2: -23.071134567260742, bias: 16.51972007751465, loss: 30.58027037832032\n",
      "Epoch: 3670/5000\n",
      "w1: 24.863618850708008, w2: -23.07866096496582, bias: 16.51866340637207, loss: 30.57928572920848\n",
      "Epoch: 3680/5000\n",
      "w1: 24.86994171142578, w2: -23.08610725402832, bias: 16.517606735229492, loss: 30.578318912774684\n",
      "Epoch: 3690/5000\n",
      "w1: 24.876216888427734, w2: -23.093477249145508, bias: 16.51655387878418, loss: 30.57736961465082\n",
      "Epoch: 3700/5000\n",
      "w1: 24.882444381713867, w2: -23.100770950317383, bias: 16.5155029296875, loss: 30.57643739134406\n",
      "Epoch: 3710/5000\n",
      "w1: 24.888628005981445, w2: -23.107986450195312, bias: 16.514450073242188, loss: 30.575522079816906\n",
      "Epoch: 3720/5000\n",
      "w1: 24.894765853881836, w2: -23.115127563476562, bias: 16.513399124145508, loss: 30.574623207879696\n",
      "Epoch: 3730/5000\n",
      "w1: 24.90085792541504, w2: -23.1221923828125, bias: 16.51235008239746, loss: 30.57374056322475\n",
      "Epoch: 3740/5000\n",
      "w1: 24.906904220581055, w2: -23.129182815551758, bias: 16.51129913330078, loss: 30.572873972587697\n",
      "Epoch: 3750/5000\n",
      "w1: 24.91290855407715, w2: -23.136098861694336, bias: 16.510250091552734, loss: 30.572022905230654\n",
      "Epoch: 3760/5000\n",
      "w1: 24.918867111206055, w2: -23.142942428588867, bias: 16.509201049804688, loss: 30.571187105311505\n",
      "Epoch: 3770/5000\n",
      "w1: 24.924781799316406, w2: -23.14971351623535, bias: 16.50815200805664, loss: 30.570366469992003\n",
      "Epoch: 3780/5000\n",
      "w1: 24.930654525756836, w2: -23.156414031982422, bias: 16.507102966308594, loss: 30.569560560287698\n",
      "Epoch: 3790/5000\n",
      "w1: 24.936485290527344, w2: -23.163042068481445, bias: 16.506053924560547, loss: 30.568769041614004\n",
      "Epoch: 3800/5000\n",
      "w1: 24.942272186279297, w2: -23.169601440429688, bias: 16.5050048828125, loss: 30.567991586730678\n",
      "Epoch: 3810/5000\n",
      "w1: 24.94801902770996, w2: -23.176090240478516, bias: 16.503955841064453, loss: 30.56722824515962\n",
      "Epoch: 3820/5000\n",
      "w1: 24.953723907470703, w2: -23.182510375976562, bias: 16.502906799316406, loss: 30.566478448445956\n",
      "Epoch: 3830/5000\n",
      "w1: 24.959388732910156, w2: -23.188861846923828, bias: 16.50185775756836, loss: 30.565742020294227\n",
      "Epoch: 3840/5000\n",
      "w1: 24.96501350402832, w2: -23.195146560668945, bias: 16.500808715820312, loss: 30.565018771758734\n",
      "Epoch: 3850/5000\n",
      "w1: 24.970600128173828, w2: -23.20136260986328, bias: 16.499759674072266, loss: 30.564308291503888\n",
      "Epoch: 3860/5000\n",
      "w1: 24.976144790649414, w2: -23.2075138092041, bias: 16.49871063232422, loss: 30.56361061112026\n",
      "Epoch: 3870/5000\n",
      "w1: 24.981651306152344, w2: -23.213598251342773, bias: 16.497661590576172, loss: 30.56292522366177\n",
      "Epoch: 3880/5000\n",
      "w1: 24.987119674682617, w2: -23.219615936279297, bias: 16.496612548828125, loss: 30.56225218546786\n",
      "Epoch: 3890/5000\n",
      "w1: 24.992549896240234, w2: -23.225570678710938, bias: 16.495563507080078, loss: 30.56159097957604\n",
      "Epoch: 3900/5000\n",
      "w1: 24.997941970825195, w2: -23.231460571289062, bias: 16.494524002075195, loss: 30.560941539840787\n",
      "Epoch: 3910/5000\n",
      "w1: 25.0032958984375, w2: -23.237287521362305, bias: 16.493486404418945, loss: 30.560303733752765\n",
      "Epoch: 3920/5000\n",
      "w1: 25.00861167907715, w2: -23.243051528930664, bias: 16.492448806762695, loss: 30.55967726634737\n",
      "Epoch: 3930/5000\n",
      "w1: 25.01388931274414, w2: -23.248756408691406, bias: 16.49141502380371, loss: 30.559061780358018\n",
      "Epoch: 3940/5000\n",
      "w1: 25.019128799438477, w2: -23.254398345947266, bias: 16.490381240844727, loss: 30.558457230821272\n",
      "Epoch: 3950/5000\n",
      "w1: 25.024330139160156, w2: -23.259979248046875, bias: 16.489347457885742, loss: 30.55786354976533\n",
      "Epoch: 3960/5000\n",
      "w1: 25.029497146606445, w2: -23.265501022338867, bias: 16.48831558227539, loss: 30.55728019490013\n",
      "Epoch: 3970/5000\n",
      "w1: 25.034626007080078, w2: -23.270963668823242, bias: 16.487285614013672, loss: 30.55670721771461\n",
      "Epoch: 3980/5000\n",
      "w1: 25.03972053527832, w2: -23.2763671875, bias: 16.486255645751953, loss: 30.556144258340233\n",
      "Epoch: 3990/5000\n",
      "w1: 25.04477882385254, w2: -23.281713485717773, bias: 16.485225677490234, loss: 30.555591170608928\n",
      "Epoch: 4000/5000\n",
      "w1: 25.049800872802734, w2: -23.28700065612793, bias: 16.484195709228516, loss: 30.55504791663276\n",
      "Epoch: 4010/5000\n",
      "w1: 25.05478858947754, w2: -23.2922306060791, bias: 16.483165740966797, loss: 30.554514256935047\n",
      "Epoch: 4020/5000\n",
      "w1: 25.05974006652832, w2: -23.297405242919922, bias: 16.482135772705078, loss: 30.553989863552648\n",
      "Epoch: 4030/5000\n",
      "w1: 25.064659118652344, w2: -23.302522659301758, bias: 16.48110580444336, loss: 30.553474610967537\n",
      "Epoch: 4040/5000\n",
      "w1: 25.06954574584961, w2: -23.307584762573242, bias: 16.48007583618164, loss: 30.552968273946778\n",
      "Epoch: 4050/5000\n",
      "w1: 25.074398040771484, w2: -23.312591552734375, bias: 16.479045867919922, loss: 30.55247091223266\n",
      "Epoch: 4060/5000\n",
      "w1: 25.07921600341797, w2: -23.317543029785156, bias: 16.478015899658203, loss: 30.55198221519776\n",
      "Epoch: 4070/5000\n",
      "w1: 25.084003448486328, w2: -23.32244110107422, bias: 16.476985931396484, loss: 30.55150196943543\n",
      "Epoch: 4080/5000\n",
      "w1: 25.088760375976562, w2: -23.327285766601562, bias: 16.475955963134766, loss: 30.551029962110412\n",
      "Epoch: 4090/5000\n",
      "w1: 25.093481063842773, w2: -23.332075119018555, bias: 16.474925994873047, loss: 30.550566350875894\n",
      "Epoch: 4100/5000\n",
      "w1: 25.098173141479492, w2: -23.33681297302246, bias: 16.473896026611328, loss: 30.550110567558562\n",
      "Epoch: 4110/5000\n",
      "w1: 25.102834701538086, w2: -23.34149932861328, bias: 16.47286605834961, loss: 30.549662681146117\n",
      "Epoch: 4120/5000\n",
      "w1: 25.10746192932129, w2: -23.346134185791016, bias: 16.471837997436523, loss: 30.549222668816743\n",
      "Epoch: 4130/5000\n",
      "w1: 25.112058639526367, w2: -23.35071563720703, bias: 16.4708194732666, loss: 30.548790360175477\n",
      "Epoch: 4140/5000\n",
      "w1: 25.11662483215332, w2: -23.355247497558594, bias: 16.469802856445312, loss: 30.548365551678437\n",
      "Epoch: 4150/5000\n",
      "w1: 25.121158599853516, w2: -23.359729766845703, bias: 16.468786239624023, loss: 30.547948137334064\n",
      "Epoch: 4160/5000\n",
      "w1: 25.12566375732422, w2: -23.364164352416992, bias: 16.467771530151367, loss: 30.547537654737184\n",
      "Epoch: 4170/5000\n",
      "w1: 25.130136489868164, w2: -23.368549346923828, bias: 16.466760635375977, loss: 30.547134534411086\n",
      "Epoch: 4180/5000\n",
      "w1: 25.134580612182617, w2: -23.372882843017578, bias: 16.46574592590332, loss: 30.546738210787133\n",
      "Epoch: 4190/5000\n",
      "w1: 25.138994216918945, w2: -23.37717056274414, bias: 16.46473503112793, loss: 30.54634881689298\n",
      "Epoch: 4200/5000\n",
      "w1: 25.14337730407715, w2: -23.38140869140625, bias: 16.46372413635254, loss: 30.545966049881763\n",
      "Epoch: 4210/5000\n",
      "w1: 25.147733688354492, w2: -23.385602951049805, bias: 16.46271324157715, loss: 30.545589737253902\n",
      "Epoch: 4220/5000\n",
      "w1: 25.152057647705078, w2: -23.389751434326172, bias: 16.461702346801758, loss: 30.5452197931886\n",
      "Epoch: 4230/5000\n",
      "w1: 25.156354904174805, w2: -23.39385223388672, bias: 16.460691452026367, loss: 30.544856281937957\n",
      "Epoch: 4240/5000\n",
      "w1: 25.160621643066406, w2: -23.397907257080078, bias: 16.459680557250977, loss: 30.544498956672705\n",
      "Epoch: 4250/5000\n",
      "w1: 25.16486358642578, w2: -23.40191650390625, bias: 16.458669662475586, loss: 30.544147573846455\n",
      "Epoch: 4260/5000\n",
      "w1: 25.169076919555664, w2: -23.405881881713867, bias: 16.457658767700195, loss: 30.543802126863316\n",
      "Epoch: 4270/5000\n",
      "w1: 25.173263549804688, w2: -23.409801483154297, bias: 16.456647872924805, loss: 30.54346253732119\n",
      "Epoch: 4280/5000\n",
      "w1: 25.17742347717285, w2: -23.413677215576172, bias: 16.455636978149414, loss: 30.54312863874514\n",
      "Epoch: 4290/5000\n",
      "w1: 25.181554794311523, w2: -23.417509078979492, bias: 16.454626083374023, loss: 30.542800513683748\n",
      "Epoch: 4300/5000\n",
      "w1: 25.18566131591797, w2: -23.42129898071289, bias: 16.453615188598633, loss: 30.542477774223\n",
      "Epoch: 4310/5000\n",
      "w1: 25.189741134643555, w2: -23.4250431060791, bias: 16.452604293823242, loss: 30.542160417394783\n",
      "Epoch: 4320/5000\n",
      "w1: 25.19379425048828, w2: -23.42874526977539, bias: 16.45159339904785, loss: 30.541848521630484\n",
      "Epoch: 4330/5000\n",
      "w1: 25.197824478149414, w2: -23.432405471801758, bias: 16.45058822631836, loss: 30.54154174781177\n",
      "Epoch: 4340/5000\n",
      "w1: 25.201826095581055, w2: -23.436023712158203, bias: 16.449588775634766, loss: 30.541240215731595\n",
      "Epoch: 4350/5000\n",
      "w1: 25.20580291748047, w2: -23.43960189819336, bias: 16.448593139648438, loss: 30.54094374648349\n",
      "Epoch: 4360/5000\n",
      "w1: 25.209754943847656, w2: -23.443140029907227, bias: 16.44759750366211, loss: 30.540652156989612\n",
      "Epoch: 4370/5000\n",
      "w1: 25.21367835998535, w2: -23.446636199951172, bias: 16.44660186767578, loss: 30.54036545426086\n",
      "Epoch: 4380/5000\n",
      "w1: 25.217578887939453, w2: -23.45009422302246, bias: 16.445608139038086, loss: 30.540083514783863\n",
      "Epoch: 4390/5000\n",
      "w1: 25.221454620361328, w2: -23.45351219177246, bias: 16.444616317749023, loss: 30.539806272202092\n",
      "Epoch: 4400/5000\n",
      "w1: 25.22530174255371, w2: -23.456890106201172, bias: 16.44362449645996, loss: 30.539533714913812\n",
      "Epoch: 4410/5000\n",
      "w1: 25.2291259765625, w2: -23.460229873657227, bias: 16.4426326751709, loss: 30.539265640304077\n",
      "Epoch: 4420/5000\n",
      "w1: 25.232927322387695, w2: -23.463531494140625, bias: 16.441640853881836, loss: 30.539001913845496\n",
      "Epoch: 4430/5000\n",
      "w1: 25.236703872680664, w2: -23.466794967651367, bias: 16.440649032592773, loss: 30.538742475258374\n",
      "Epoch: 4440/5000\n",
      "w1: 25.240455627441406, w2: -23.470020294189453, bias: 16.43965721130371, loss: 30.538487407586295\n",
      "Epoch: 4450/5000\n",
      "w1: 25.244184494018555, w2: -23.473209381103516, bias: 16.43866539001465, loss: 30.538236446640962\n",
      "Epoch: 4460/5000\n",
      "w1: 25.247892379760742, w2: -23.476362228393555, bias: 16.437673568725586, loss: 30.53798946548354\n",
      "Epoch: 4470/5000\n",
      "w1: 25.251577377319336, w2: -23.479476928710938, bias: 16.436681747436523, loss: 30.53774653867703\n",
      "Epoch: 4480/5000\n",
      "w1: 25.255237579345703, w2: -23.48255729675293, bias: 16.43568992614746, loss: 30.537507550337676\n",
      "Epoch: 4490/5000\n",
      "w1: 25.25887680053711, w2: -23.485599517822266, bias: 16.4346981048584, loss: 30.5372724456198\n",
      "Epoch: 4500/5000\n",
      "w1: 25.262493133544922, w2: -23.488605499267578, bias: 16.433706283569336, loss: 30.537041170207914\n",
      "Epoch: 4510/5000\n",
      "w1: 25.266088485717773, w2: -23.491575241088867, bias: 16.432714462280273, loss: 30.536813601919242\n",
      "Epoch: 4520/5000\n",
      "w1: 25.269662857055664, w2: -23.494510650634766, bias: 16.43172264099121, loss: 30.536589633014874\n",
      "Epoch: 4530/5000\n",
      "w1: 25.273216247558594, w2: -23.497411727905273, bias: 16.430742263793945, loss: 30.53636930752898\n",
      "Epoch: 4540/5000\n",
      "w1: 25.276748657226562, w2: -23.500280380249023, bias: 16.42976188659668, loss: 30.536152519955248\n",
      "Epoch: 4550/5000\n",
      "w1: 25.280256271362305, w2: -23.50311279296875, bias: 16.42878532409668, loss: 30.53593925087921\n",
      "Epoch: 4560/5000\n",
      "w1: 25.283742904663086, w2: -23.50591278076172, bias: 16.42780876159668, loss: 30.53572941645578\n",
      "Epoch: 4570/5000\n",
      "w1: 25.287206649780273, w2: -23.508682250976562, bias: 16.426834106445312, loss: 30.535522917088436\n",
      "Epoch: 4580/5000\n",
      "w1: 25.2906494140625, w2: -23.511417388916016, bias: 16.425861358642578, loss: 30.5353197981529\n",
      "Epoch: 4590/5000\n",
      "w1: 25.294071197509766, w2: -23.514118194580078, bias: 16.424888610839844, loss: 30.53511988969206\n",
      "Epoch: 4600/5000\n",
      "w1: 25.297473907470703, w2: -23.516788482666016, bias: 16.42391586303711, loss: 30.53492303179876\n",
      "Epoch: 4610/5000\n",
      "w1: 25.30085563659668, w2: -23.519428253173828, bias: 16.422943115234375, loss: 30.534729297731648\n",
      "Epoch: 4620/5000\n",
      "w1: 25.304216384887695, w2: -23.52203369140625, bias: 16.42197036743164, loss: 30.5345386437903\n",
      "Epoch: 4630/5000\n",
      "w1: 25.307558059692383, w2: -23.524608612060547, bias: 16.420997619628906, loss: 30.53435091428949\n",
      "Epoch: 4640/5000\n",
      "w1: 25.31087875366211, w2: -23.52715492248535, bias: 16.420024871826172, loss: 30.534166132461415\n",
      "Epoch: 4650/5000\n",
      "w1: 25.314180374145508, w2: -23.5296688079834, bias: 16.419052124023438, loss: 30.533984194824267\n",
      "Epoch: 4660/5000\n",
      "w1: 25.317462921142578, w2: -23.532154083251953, bias: 16.418079376220703, loss: 30.53380506219676\n",
      "Epoch: 4670/5000\n",
      "w1: 25.32072639465332, w2: -23.53460693359375, bias: 16.41710662841797, loss: 30.533628695770606\n",
      "Epoch: 4680/5000\n",
      "w1: 25.323970794677734, w2: -23.537031173706055, bias: 16.416133880615234, loss: 30.53345501114607\n",
      "Epoch: 4690/5000\n",
      "w1: 25.32719612121582, w2: -23.539424896240234, bias: 16.4151611328125, loss: 30.533284062755964\n",
      "Epoch: 4700/5000\n",
      "w1: 25.33040428161621, w2: -23.54178810119629, bias: 16.414188385009766, loss: 30.533115660506983\n",
      "Epoch: 4710/5000\n",
      "w1: 25.333593368530273, w2: -23.54412269592285, bias: 16.41321563720703, loss: 30.532949874746716\n",
      "Epoch: 4720/5000\n",
      "w1: 25.33676528930664, w2: -23.546428680419922, bias: 16.412256240844727, loss: 30.53278663078415\n",
      "Epoch: 4730/5000\n",
      "w1: 25.33991813659668, w2: -23.5487060546875, bias: 16.411296844482422, loss: 30.532625950609354\n",
      "Epoch: 4740/5000\n",
      "w1: 25.34305191040039, w2: -23.550954818725586, bias: 16.41033935546875, loss: 30.532467712530682\n",
      "Epoch: 4750/5000\n",
      "w1: 25.346166610717773, w2: -23.553176879882812, bias: 16.409381866455078, loss: 30.53231192587594\n",
      "Epoch: 4760/5000\n",
      "w1: 25.34926414489746, w2: -23.55537223815918, bias: 16.40842628479004, loss: 30.532158377639554\n",
      "Epoch: 4770/5000\n",
      "w1: 25.35234260559082, w2: -23.557538986206055, bias: 16.407472610473633, loss: 30.532007275243224\n",
      "Epoch: 4780/5000\n",
      "w1: 25.355403900146484, w2: -23.559680938720703, bias: 16.406518936157227, loss: 30.531858366907908\n",
      "Epoch: 4790/5000\n",
      "w1: 25.35844612121582, w2: -23.561796188354492, bias: 16.40556526184082, loss: 30.5317116814763\n",
      "Epoch: 4800/5000\n",
      "w1: 25.36147117614746, w2: -23.563884735107422, bias: 16.404611587524414, loss: 30.531567210838343\n",
      "Epoch: 4810/5000\n",
      "w1: 25.36448097229004, w2: -23.565948486328125, bias: 16.403657913208008, loss: 30.531424788049204\n",
      "Epoch: 4820/5000\n",
      "w1: 25.367473602294922, w2: -23.567983627319336, bias: 16.4027042388916, loss: 30.531284481405024\n",
      "Epoch: 4830/5000\n",
      "w1: 25.37044906616211, w2: -23.56999397277832, bias: 16.401750564575195, loss: 30.531146318085728\n",
      "Epoch: 4840/5000\n",
      "w1: 25.37340545654297, w2: -23.571979522705078, bias: 16.40079689025879, loss: 30.531010136459287\n",
      "Epoch: 4850/5000\n",
      "w1: 25.376346588134766, w2: -23.573938369750977, bias: 16.399843215942383, loss: 30.530875966097707\n",
      "Epoch: 4860/5000\n",
      "w1: 25.3792724609375, w2: -23.57587242126465, bias: 16.398889541625977, loss: 30.53074370455849\n",
      "Epoch: 4870/5000\n",
      "w1: 25.382183074951172, w2: -23.577783584594727, bias: 16.39793586730957, loss: 30.530613289694283\n",
      "Epoch: 4880/5000\n",
      "w1: 25.38507843017578, w2: -23.579666137695312, bias: 16.396982192993164, loss: 30.53048476868921\n",
      "Epoch: 4890/5000\n",
      "w1: 25.387958526611328, w2: -23.581525802612305, bias: 16.396028518676758, loss: 30.530358134200792\n",
      "Epoch: 4900/5000\n",
      "w1: 25.39082145690918, w2: -23.583362579345703, bias: 16.395082473754883, loss: 30.530233324828515\n",
      "Epoch: 4910/5000\n",
      "w1: 25.39366912841797, w2: -23.585174560546875, bias: 16.394142150878906, loss: 30.53011033471596\n",
      "Epoch: 4920/5000\n",
      "w1: 25.396501541137695, w2: -23.58696174621582, bias: 16.39320182800293, loss: 30.529989154203175\n",
      "Epoch: 4930/5000\n",
      "w1: 25.39931869506836, w2: -23.588727951049805, bias: 16.392263412475586, loss: 30.52986962096676\n",
      "Epoch: 4940/5000\n",
      "w1: 25.40212059020996, w2: -23.590471267700195, bias: 16.391326904296875, loss: 30.52975180038779\n",
      "Epoch: 4950/5000\n",
      "w1: 25.404905319213867, w2: -23.592193603515625, bias: 16.390392303466797, loss: 30.529635725000922\n",
      "Epoch: 4960/5000\n",
      "w1: 25.407672882080078, w2: -23.59389305114746, bias: 16.38945770263672, loss: 30.529521280575835\n",
      "Epoch: 4970/5000\n",
      "w1: 25.410425186157227, w2: -23.59556770324707, bias: 16.38852310180664, loss: 30.52940844477337\n",
      "Epoch: 4980/5000\n",
      "w1: 25.413164138793945, w2: -23.59722137451172, bias: 16.387588500976562, loss: 30.529297143362633\n",
      "Epoch: 4990/5000\n",
      "w1: 25.415889739990234, w2: -23.598854064941406, bias: 16.386653900146484, loss: 30.52918732434377\n",
      "Epoch: 5000/5000\n",
      "w1: 25.418598175048828, w2: -23.6004638671875, bias: 16.385719299316406, loss: 30.52907915329666\n",
      "##### 최종 w1, w2, bias #######\n",
      "25.418598175048828 -23.6004638671875 16.385719299316406\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features_np = scaler.fit_transform(boston_df[['RM', 'LSTAT']])\n",
    "\n",
    "scaled_features_ts = torch.from_numpy(scaled_features_np)\n",
    "targets_ts = torch.from_numpy(boston_df['PRICE'].values)\n",
    "\n",
    "w1, w2, bias = gradient_descent(scaled_features_ts, targets_ts, iter_epochs=5000, verbose=True)\n",
    "print('##### 최종 w1, w2, bias #######')\n",
    "print(w1.item(), w2.item(), bias.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 계산된 Weight와 Bias를 이용하여 Price 예측\n",
    "* 예측 feature 역시 0~1사이의 scaled값을 이용하고 Weight와 bias를 적용하여 예측값 계산. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>PREDICTED_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.948606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "      <td>25.489461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "      <td>32.538213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "      <td>32.337287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "      <td>31.506543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.02985</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.430</td>\n",
       "      <td>58.7</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.12</td>\n",
       "      <td>5.21</td>\n",
       "      <td>28.7</td>\n",
       "      <td>28.092617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.08829</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.012</td>\n",
       "      <td>66.6</td>\n",
       "      <td>5.5605</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>395.60</td>\n",
       "      <td>12.43</td>\n",
       "      <td>22.9</td>\n",
       "      <td>21.354932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.14455</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.172</td>\n",
       "      <td>96.1</td>\n",
       "      <td>5.9505</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>19.15</td>\n",
       "      <td>27.1</td>\n",
       "      <td>17.757951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.21124</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>5.631</td>\n",
       "      <td>100.0</td>\n",
       "      <td>6.0821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.63</td>\n",
       "      <td>29.93</td>\n",
       "      <td>16.5</td>\n",
       "      <td>8.102840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.17004</td>\n",
       "      <td>12.5</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.524</td>\n",
       "      <td>6.004</td>\n",
       "      <td>85.9</td>\n",
       "      <td>6.5921</td>\n",
       "      <td>5.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>386.71</td>\n",
       "      <td>17.10</td>\n",
       "      <td>18.9</td>\n",
       "      <td>18.274740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
       "5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
       "6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
       "7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
       "8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
       "9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  PRICE  PREDICTED_PRICE  \n",
       "0     15.3  396.90   4.98   24.0        28.948606  \n",
       "1     17.8  396.90   9.14   21.6        25.489461  \n",
       "2     17.8  392.83   4.03   34.7        32.538213  \n",
       "3     18.7  394.63   2.94   33.4        32.337287  \n",
       "4     18.7  396.90   5.33   36.2        31.506543  \n",
       "5     18.7  394.12   5.21   28.7        28.092617  \n",
       "6     15.2  395.60  12.43   22.9        21.354932  \n",
       "7     15.2  396.90  19.15   27.1        17.757951  \n",
       "8     15.2  386.63  29.93   16.5         8.102840  \n",
       "9     15.2  386.71  17.10   18.9        18.274740  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = scaled_features_ts[:, 0]*w1 + scaled_features_ts[:, 1]*w2 + bias\n",
    "boston_df['PREDICTED_PRICE'] = predicted.cpu().numpy()\n",
    "boston_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.529068392718102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "total_mse = mean_squared_error(boston_df['PRICE'], boston_df['PREDICTED_PRICE'])\n",
    "print(total_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 2) (354,) (152, 2) (152,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tr_features, test_features, tr_target, test_target = train_test_split(scaled_features_np, boston_df['PRICE'].values, \n",
    "                                                                      test_size=0.3, random_state=2025)\n",
    "print(tr_features.shape, tr_target.shape, test_features.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최초 w1, w2, bias: 0.0 0.0 1.0\n",
      "Epoch: 10/5000\n",
      "w1: 2.1782474517822266, w2: 0.8989012241363525, bias: 4.82615327835083, loss: 369.1777488693663\n",
      "Epoch: 20/5000\n",
      "w1: 3.865514039993286, w2: 1.5150399208068848, bias: 7.724394798278809, loss: 247.23309861532792\n",
      "Epoch: 30/5000\n",
      "w1: 5.180546760559082, w2: 1.9176135063171387, bias: 9.919800758361816, loss: 176.86106703093324\n",
      "Epoch: 40/5000\n",
      "w1: 6.213293075561523, w2: 2.159029245376587, bias: 11.58285140991211, loss: 136.08428868980482\n",
      "Epoch: 50/5000\n",
      "w1: 7.03188419342041, w2: 2.2789769172668457, bias: 12.842673301696777, loss: 112.29406054215411\n",
      "Epoch: 60/5000\n",
      "w1: 7.687928199768066, w2: 2.3075127601623535, bias: 13.797069549560547, loss: 98.25655334337485\n",
      "Epoch: 70/5000\n",
      "w1: 8.220519065856934, w2: 2.2673964500427246, bias: 14.520127296447754, loss: 89.8214789681167\n",
      "Epoch: 80/5000\n",
      "w1: 8.659270286560059, w2: 2.1758623123168945, bias: 15.067954063415527, loss: 84.60770563979638\n",
      "Epoch: 90/5000\n",
      "w1: 9.02661418914795, w2: 2.0459582805633545, bias: 15.483052253723145, loss: 81.24901586776296\n",
      "Epoch: 100/5000\n",
      "w1: 9.339546203613281, w2: 1.8875619173049927, bias: 15.797615051269531, loss: 78.96146083532709\n",
      "Epoch: 110/5000\n",
      "w1: 9.610944747924805, w2: 1.708149790763855, bias: 16.036022186279297, loss: 77.29516504410438\n",
      "Epoch: 120/5000\n",
      "w1: 9.850563049316406, w2: 1.5133790969848633, bias: 16.216745376586914, loss: 75.99190605129962\n",
      "Epoch: 130/5000\n",
      "w1: 10.065794944763184, w2: 1.3075300455093384, bias: 16.353771209716797, loss: 74.9034052475852\n",
      "Epoch: 140/5000\n",
      "w1: 10.262240409851074, w2: 1.0938389301300049, bias: 16.45769691467285, loss: 73.94448921624975\n",
      "Epoch: 150/5000\n",
      "w1: 10.444144248962402, w2: 0.8747522234916687, bias: 16.536548614501953, loss: 73.06616334067238\n",
      "Epoch: 160/5000\n",
      "w1: 10.614727973937988, w2: 0.6521176695823669, bias: 16.596405029296875, loss: 72.24020727263056\n",
      "Epoch: 170/5000\n",
      "w1: 10.776432037353516, w2: 0.4273296296596527, bias: 16.641870498657227, loss: 71.45031252805038\n",
      "Epoch: 180/5000\n",
      "w1: 10.931106567382812, w2: 0.20143954455852509, bias: 16.676429748535156, loss: 70.68701672215099\n",
      "Epoch: 190/5000\n",
      "w1: 11.080159187316895, w2: -0.02476121298968792, bias: 16.702728271484375, loss: 69.9447735341496\n",
      "Epoch: 200/5000\n",
      "w1: 11.224658966064453, w2: -0.2506784200668335, bias: 16.722768783569336, loss: 69.22029253595441\n",
      "Epoch: 210/5000\n",
      "w1: 11.3654146194458, w2: -0.47586673498153687, bias: 16.73805809020996, loss: 68.51158478503602\n",
      "Epoch: 220/5000\n",
      "w1: 11.503044128417969, w2: -0.6999933123588562, bias: 16.74974822998047, loss: 67.81740401496076\n",
      "Epoch: 230/5000\n",
      "w1: 11.638015747070312, w2: -0.9228113889694214, bias: 16.758712768554688, loss: 67.13693200258119\n",
      "Epoch: 240/5000\n",
      "w1: 11.770689964294434, w2: -1.1441383361816406, bias: 16.765609741210938, loss: 66.4695980220533\n",
      "Epoch: 250/5000\n",
      "w1: 11.901341438293457, w2: -1.3638412952423096, bias: 16.770936965942383, loss: 65.8149761642368\n",
      "Epoch: 260/5000\n",
      "w1: 12.030180931091309, w2: -1.5818233489990234, bias: 16.775068283081055, loss: 65.17272632802919\n",
      "Epoch: 270/5000\n",
      "w1: 12.157370567321777, w2: -1.79801607131958, bias: 16.778295516967773, loss: 64.54255623657217\n",
      "Epoch: 280/5000\n",
      "w1: 12.283034324645996, w2: -2.0123722553253174, bias: 16.78083038330078, loss: 63.92420951346721\n",
      "Epoch: 290/5000\n",
      "w1: 12.407271385192871, w2: -2.224860668182373, bias: 16.782840728759766, loss: 63.31744276420126\n",
      "Epoch: 300/5000\n",
      "w1: 12.530157089233398, w2: -2.4354617595672607, bias: 16.78445053100586, loss: 62.722031823449456\n",
      "Epoch: 310/5000\n",
      "w1: 12.651752471923828, w2: -2.6441657543182373, bias: 16.78575325012207, loss: 62.13775471128608\n",
      "Epoch: 320/5000\n",
      "w1: 12.772106170654297, w2: -2.8509693145751953, bias: 16.786819458007812, loss: 61.56440089465468\n",
      "Epoch: 330/5000\n",
      "w1: 12.891257286071777, w2: -3.0558741092681885, bias: 16.787708282470703, loss: 61.00176321088714\n",
      "Epoch: 340/5000\n",
      "w1: 13.009238243103027, w2: -3.2588858604431152, bias: 16.788454055786133, loss: 60.44964096274147\n",
      "Epoch: 350/5000\n",
      "w1: 13.12607479095459, w2: -3.4600136280059814, bias: 16.789093017578125, loss: 59.907836394958174\n",
      "Epoch: 360/5000\n",
      "w1: 13.241788864135742, w2: -3.6592679023742676, bias: 16.78964614868164, loss: 59.376157213540594\n",
      "Epoch: 370/5000\n",
      "w1: 13.356401443481445, w2: -3.856661081314087, bias: 16.790132522583008, loss: 58.85441317469145\n",
      "Epoch: 380/5000\n",
      "w1: 13.469927787780762, w2: -4.052206516265869, bias: 16.79056739807129, loss: 58.342420498174825\n",
      "Epoch: 390/5000\n",
      "w1: 13.58238410949707, w2: -4.245919227600098, bias: 16.790958404541016, loss: 57.839995541845234\n",
      "Epoch: 400/5000\n",
      "w1: 13.693787574768066, w2: -4.437812805175781, bias: 16.791311264038086, loss: 57.34695699495646\n",
      "Epoch: 410/5000\n",
      "w1: 13.804144859313965, w2: -4.627903938293457, bias: 16.791635513305664, loss: 56.863134637294614\n",
      "Epoch: 420/5000\n",
      "w1: 13.913471221923828, w2: -4.816207408905029, bias: 16.791934967041016, loss: 56.388352515888016\n",
      "Epoch: 430/5000\n",
      "w1: 14.021778106689453, w2: -5.002738952636719, bias: 16.792213439941406, loss: 55.922444547459165\n",
      "Epoch: 440/5000\n",
      "w1: 14.12907600402832, w2: -5.187515735626221, bias: 16.79247283935547, loss: 55.46524192003706\n",
      "Epoch: 450/5000\n",
      "w1: 14.235376358032227, w2: -5.3705525398254395, bias: 16.79271697998047, loss: 55.016581256133094\n",
      "Epoch: 460/5000\n",
      "w1: 14.340688705444336, w2: -5.551865100860596, bias: 16.792945861816406, loss: 54.576304607756626\n",
      "Epoch: 470/5000\n",
      "w1: 14.445021629333496, w2: -5.731470584869385, bias: 16.79315757751465, loss: 54.144256271148976\n",
      "Epoch: 480/5000\n",
      "w1: 14.548385620117188, w2: -5.909383296966553, bias: 16.793357849121094, loss: 53.72028154425121\n",
      "Epoch: 490/5000\n",
      "w1: 14.65079116821289, w2: -6.085619926452637, bias: 16.793548583984375, loss: 53.30422842425009\n",
      "Epoch: 500/5000\n",
      "w1: 14.752245903015137, w2: -6.260195255279541, bias: 16.79372215270996, loss: 52.89595114958269\n",
      "Epoch: 510/5000\n",
      "w1: 14.852760314941406, w2: -6.433125019073486, bias: 16.79388999938965, loss: 52.49530341434485\n",
      "Epoch: 520/5000\n",
      "w1: 14.952342987060547, w2: -6.604424476623535, bias: 16.794042587280273, loss: 52.102140685573815\n",
      "Epoch: 530/5000\n",
      "w1: 15.051002502441406, w2: -6.774109840393066, bias: 16.794187545776367, loss: 51.71632522708685\n",
      "Epoch: 540/5000\n",
      "w1: 15.148746490478516, w2: -6.942195892333984, bias: 16.794321060180664, loss: 51.33771876092863\n",
      "Epoch: 550/5000\n",
      "w1: 15.245584487915039, w2: -7.108696460723877, bias: 16.79444694519043, loss: 50.966189486372755\n",
      "Epoch: 560/5000\n",
      "w1: 15.341525077819824, w2: -7.273627281188965, bias: 16.7945613861084, loss: 50.601601855778824\n",
      "Epoch: 570/5000\n",
      "w1: 15.436576843261719, w2: -7.437004089355469, bias: 16.794666290283203, loss: 50.24382608014042\n",
      "Epoch: 580/5000\n",
      "w1: 15.53074836730957, w2: -7.59883975982666, bias: 16.794761657714844, loss: 49.892735921991296\n",
      "Epoch: 590/5000\n",
      "w1: 15.62404727935791, w2: -7.759149551391602, bias: 16.794849395751953, loss: 49.54820654549583\n",
      "Epoch: 600/5000\n",
      "w1: 15.716482162475586, w2: -7.917947769165039, bias: 16.794925689697266, loss: 49.210115801603116\n",
      "Epoch: 610/5000\n",
      "w1: 15.808060646057129, w2: -8.075248718261719, bias: 16.79499626159668, loss: 48.8783421493941\n",
      "Epoch: 620/5000\n",
      "w1: 15.898792266845703, w2: -8.231064796447754, bias: 16.795053482055664, loss: 48.55276867549458\n",
      "Epoch: 630/5000\n",
      "w1: 15.988682746887207, w2: -8.38541316986084, bias: 16.795106887817383, loss: 48.23327813584084\n",
      "Epoch: 640/5000\n",
      "w1: 16.077741622924805, w2: -8.538305282592773, bias: 16.79514503479004, loss: 47.91975840549087\n",
      "Epoch: 650/5000\n",
      "w1: 16.16597557067871, w2: -8.689753532409668, bias: 16.795183181762695, loss: 47.612099149911515\n",
      "Epoch: 660/5000\n",
      "w1: 16.253393173217773, w2: -8.839775085449219, bias: 16.795202255249023, loss: 47.31018613065763\n",
      "Epoch: 670/5000\n",
      "w1: 16.340002059936523, w2: -8.988381385803223, bias: 16.79522132873535, loss: 47.013915184802535\n",
      "Epoch: 680/5000\n",
      "w1: 16.42580795288086, w2: -9.13558578491211, bias: 16.795225143432617, loss: 46.72318216633684\n",
      "Epoch: 690/5000\n",
      "w1: 16.51082420349121, w2: -9.281402587890625, bias: 16.795225143432617, loss: 46.43787722261538\n",
      "Epoch: 700/5000\n",
      "w1: 16.595050811767578, w2: -9.425844192504883, bias: 16.795217514038086, loss: 46.15790295077578\n",
      "Epoch: 710/5000\n",
      "w1: 16.678497314453125, w2: -9.56892204284668, bias: 16.795198440551758, loss: 45.88316617749736\n",
      "Epoch: 720/5000\n",
      "w1: 16.76117706298828, w2: -9.710650444030762, bias: 16.795177459716797, loss: 45.61355583316284\n",
      "Epoch: 730/5000\n",
      "w1: 16.84309196472168, w2: -9.851040840148926, bias: 16.79513931274414, loss: 45.34898693785181\n",
      "Epoch: 740/5000\n",
      "w1: 16.92424964904785, w2: -9.990107536315918, bias: 16.795101165771484, loss: 45.089359178242496\n",
      "Epoch: 750/5000\n",
      "w1: 17.00465202331543, w2: -10.127861976623535, bias: 16.7950496673584, loss: 44.83458752755134\n",
      "Epoch: 760/5000\n",
      "w1: 17.084312438964844, w2: -10.264315605163574, bias: 16.794992446899414, loss: 44.58457588648038\n",
      "Epoch: 770/5000\n",
      "w1: 17.163236618041992, w2: -10.399482727050781, bias: 16.794933319091797, loss: 44.33923627094079\n",
      "Epoch: 780/5000\n",
      "w1: 17.241432189941406, w2: -10.53337574005127, bias: 16.794857025146484, loss: 44.098477647670634\n",
      "Epoch: 790/5000\n",
      "w1: 17.318904876708984, w2: -10.666004180908203, bias: 16.794780731201172, loss: 43.86221855960307\n",
      "Epoch: 800/5000\n",
      "w1: 17.395660400390625, w2: -10.797380447387695, bias: 16.794694900512695, loss: 43.63037503736381\n",
      "Epoch: 810/5000\n",
      "w1: 17.47170639038086, w2: -10.927515983581543, bias: 16.794599533081055, loss: 43.402866959154686\n",
      "Epoch: 820/5000\n",
      "w1: 17.54705047607422, w2: -11.056424140930176, bias: 16.794504165649414, loss: 43.17960792887924\n",
      "Epoch: 830/5000\n",
      "w1: 17.6216983795166, w2: -11.18411636352539, bias: 16.794391632080078, loss: 42.9605189126383\n",
      "Epoch: 840/5000\n",
      "w1: 17.69565773010254, w2: -11.310604095458984, bias: 16.79427719116211, loss: 42.74552400131076\n",
      "Epoch: 850/5000\n",
      "w1: 17.76893424987793, w2: -11.435897827148438, bias: 16.794160842895508, loss: 42.53454590163696\n",
      "Epoch: 860/5000\n",
      "w1: 17.841533660888672, w2: -11.56000804901123, bias: 16.79402732849121, loss: 42.32750935396776\n",
      "Epoch: 870/5000\n",
      "w1: 17.91345977783203, w2: -11.68294620513916, bias: 16.793893814086914, loss: 42.12434604482872\n",
      "Epoch: 880/5000\n",
      "w1: 17.98472785949707, w2: -11.804723739624023, bias: 16.79375648498535, loss: 41.92497550454372\n",
      "Epoch: 890/5000\n",
      "w1: 18.055334091186523, w2: -11.92535400390625, bias: 16.793603897094727, loss: 41.72932831252457\n",
      "Epoch: 900/5000\n",
      "w1: 18.125289916992188, w2: -12.044843673706055, bias: 16.7934513092041, loss: 41.53733883307718\n",
      "Epoch: 910/5000\n",
      "w1: 18.19460105895996, w2: -12.16320514678955, bias: 16.793292999267578, loss: 41.348936965619004\n",
      "Epoch: 920/5000\n",
      "w1: 18.26327133178711, w2: -12.280447959899902, bias: 16.793121337890625, loss: 41.16405581782643\n",
      "Epoch: 930/5000\n",
      "w1: 18.331310272216797, w2: -12.396584510803223, bias: 16.792949676513672, loss: 40.98262871972978\n",
      "Epoch: 940/5000\n",
      "w1: 18.39872169494629, w2: -12.511624336242676, bias: 16.79277229309082, loss: 40.80459136383921\n",
      "Epoch: 950/5000\n",
      "w1: 18.465511322021484, w2: -12.625578880310059, bias: 16.79258155822754, loss: 40.62987883610556\n",
      "Epoch: 960/5000\n",
      "w1: 18.53168487548828, w2: -12.738456726074219, bias: 16.792390823364258, loss: 40.45843239393635\n",
      "Epoch: 970/5000\n",
      "w1: 18.597248077392578, w2: -12.850268363952637, bias: 16.79219627380371, loss: 40.29018829890406\n",
      "Epoch: 980/5000\n",
      "w1: 18.662208557128906, w2: -12.961023330688477, bias: 16.7919864654541, loss: 40.12508932678074\n",
      "Epoch: 990/5000\n",
      "w1: 18.726566314697266, w2: -13.070732116699219, bias: 16.791776657104492, loss: 39.96307625485282\n",
      "Epoch: 1000/5000\n",
      "w1: 18.790332794189453, w2: -13.179405212402344, bias: 16.791566848754883, loss: 39.80408941937236\n",
      "Epoch: 1010/5000\n",
      "w1: 18.853513717651367, w2: -13.287052154541016, bias: 16.791339874267578, loss: 39.648071342133235\n",
      "Epoch: 1020/5000\n",
      "w1: 18.916107177734375, w2: -13.393682479858398, bias: 16.79111099243164, loss: 39.49497097924586\n",
      "Epoch: 1030/5000\n",
      "w1: 18.978124618530273, w2: -13.499306678771973, bias: 16.790882110595703, loss: 39.34473047065049\n",
      "Epoch: 1040/5000\n",
      "w1: 19.039569854736328, w2: -13.603931427001953, bias: 16.790645599365234, loss: 39.19729908784736\n",
      "Epoch: 1050/5000\n",
      "w1: 19.100454330444336, w2: -13.707568168640137, bias: 16.79039764404297, loss: 39.05261963589955\n",
      "Epoch: 1060/5000\n",
      "w1: 19.160776138305664, w2: -13.810224533081055, bias: 16.790149688720703, loss: 38.910644659044166\n",
      "Epoch: 1070/5000\n",
      "w1: 19.220544815063477, w2: -13.911911010742188, bias: 16.789901733398438, loss: 38.771321691828916\n",
      "Epoch: 1080/5000\n",
      "w1: 19.27976417541504, w2: -14.012638092041016, bias: 16.789636611938477, loss: 38.63459963254951\n",
      "Epoch: 1090/5000\n",
      "w1: 19.338436126708984, w2: -14.112411499023438, bias: 16.789369583129883, loss: 38.50043416947219\n",
      "Epoch: 1100/5000\n",
      "w1: 19.396570205688477, w2: -14.211243629455566, bias: 16.78910255432129, loss: 38.36877454381326\n",
      "Epoch: 1110/5000\n",
      "w1: 19.45416831970215, w2: -14.309141159057617, bias: 16.788827896118164, loss: 38.23957584289222\n",
      "Epoch: 1120/5000\n",
      "w1: 19.5112361907959, w2: -14.406113624572754, bias: 16.788541793823242, loss: 38.11279108047929\n",
      "Epoch: 1130/5000\n",
      "w1: 19.567779541015625, w2: -14.502169609069824, bias: 16.78825569152832, loss: 37.98837363505559\n",
      "Epoch: 1140/5000\n",
      "w1: 19.623802185058594, w2: -14.597317695617676, bias: 16.7879695892334, loss: 37.866282061276245\n",
      "Epoch: 1150/5000\n",
      "w1: 19.679309844970703, w2: -14.691567420959473, bias: 16.787673950195312, loss: 37.74647129659667\n",
      "Epoch: 1160/5000\n",
      "w1: 19.73430633544922, w2: -14.78492546081543, bias: 16.787368774414062, loss: 37.62889930634117\n",
      "Epoch: 1170/5000\n",
      "w1: 19.788799285888672, w2: -14.877399444580078, bias: 16.787063598632812, loss: 37.51352399518777\n",
      "Epoch: 1180/5000\n",
      "w1: 19.842790603637695, w2: -14.968999862670898, bias: 16.786758422851562, loss: 37.4003050173711\n",
      "Epoch: 1190/5000\n",
      "w1: 19.896286010742188, w2: -15.059737205505371, bias: 16.78644561767578, loss: 37.2891984403602\n",
      "Epoch: 1200/5000\n",
      "w1: 19.949289321899414, w2: -15.149616241455078, bias: 16.786121368408203, loss: 37.18016740109642\n",
      "Epoch: 1210/5000\n",
      "w1: 20.001808166503906, w2: -15.238642692565918, bias: 16.785797119140625, loss: 37.07317549952207\n",
      "Epoch: 1220/5000\n",
      "w1: 20.05384063720703, w2: -15.326827049255371, bias: 16.785472869873047, loss: 36.968184781310924\n",
      "Epoch: 1230/5000\n",
      "w1: 20.105396270751953, w2: -15.414177894592285, bias: 16.785144805908203, loss: 36.86515466970015\n",
      "Epoch: 1240/5000\n",
      "w1: 20.156478881835938, w2: -15.500702857971191, bias: 16.784801483154297, loss: 36.76404900500382\n",
      "Epoch: 1250/5000\n",
      "w1: 20.207094192504883, w2: -15.586409568786621, bias: 16.78445816040039, loss: 36.66483231709758\n",
      "Epoch: 1260/5000\n",
      "w1: 20.257244110107422, w2: -15.671305656433105, bias: 16.784114837646484, loss: 36.56746789401095\n",
      "Epoch: 1270/5000\n",
      "w1: 20.306934356689453, w2: -15.755399703979492, bias: 16.783771514892578, loss: 36.47192234155373\n",
      "Epoch: 1280/5000\n",
      "w1: 20.356168746948242, w2: -15.838698387145996, bias: 16.78341293334961, loss: 36.37816259684504\n",
      "Epoch: 1290/5000\n",
      "w1: 20.404951095581055, w2: -15.921208381652832, bias: 16.783050537109375, loss: 36.28615433770789\n",
      "Epoch: 1300/5000\n",
      "w1: 20.45328712463379, w2: -16.00293731689453, bias: 16.78268814086914, loss: 36.19586502039942\n",
      "Epoch: 1310/5000\n",
      "w1: 20.501178741455078, w2: -16.08388900756836, bias: 16.782325744628906, loss: 36.10726631565328\n",
      "Epoch: 1320/5000\n",
      "w1: 20.54863166809082, w2: -16.164073944091797, bias: 16.78196144104004, loss: 36.02032328445998\n",
      "Epoch: 1330/5000\n",
      "w1: 20.59564781188965, w2: -16.24350357055664, bias: 16.781579971313477, loss: 35.93500334545292\n",
      "Epoch: 1340/5000\n",
      "w1: 20.642234802246094, w2: -16.322181701660156, bias: 16.781198501586914, loss: 35.851274213652495\n",
      "Epoch: 1350/5000\n",
      "w1: 20.68839454650879, w2: -16.400117874145508, bias: 16.78081703186035, loss: 35.76911057339234\n",
      "Epoch: 1360/5000\n",
      "w1: 20.734132766723633, w2: -16.47731590270996, bias: 16.78043556213379, loss: 35.688479558679255\n",
      "Epoch: 1370/5000\n",
      "w1: 20.779451370239258, w2: -16.553783416748047, bias: 16.780052185058594, loss: 35.60935585698485\n",
      "Epoch: 1380/5000\n",
      "w1: 20.824356079101562, w2: -16.629526138305664, bias: 16.779651641845703, loss: 35.5317106956197\n",
      "Epoch: 1390/5000\n",
      "w1: 20.868846893310547, w2: -16.704551696777344, bias: 16.779251098632812, loss: 35.455517357604094\n",
      "Epoch: 1400/5000\n",
      "w1: 20.912933349609375, w2: -16.778867721557617, bias: 16.778850555419922, loss: 35.38074581308158\n",
      "Epoch: 1410/5000\n",
      "w1: 20.956615447998047, w2: -16.852481842041016, bias: 16.77845001220703, loss: 35.30736964318076\n",
      "Epoch: 1420/5000\n",
      "w1: 20.999897003173828, w2: -16.925397872924805, bias: 16.77804946899414, loss: 35.235366192531444\n",
      "Epoch: 1430/5000\n",
      "w1: 21.042781829833984, w2: -16.997621536254883, bias: 16.77763557434082, loss: 35.16470757011654\n",
      "Epoch: 1440/5000\n",
      "w1: 21.085277557373047, w2: -17.069162368774414, bias: 16.7772159576416, loss: 35.095369718855935\n",
      "Epoch: 1450/5000\n",
      "w1: 21.12738609313965, w2: -17.140024185180664, bias: 16.776796340942383, loss: 35.02732493140707\n",
      "Epoch: 1460/5000\n",
      "w1: 21.169099807739258, w2: -17.210214614868164, bias: 16.776376724243164, loss: 34.96055712012844\n",
      "Epoch: 1470/5000\n",
      "w1: 21.210432052612305, w2: -17.279741287231445, bias: 16.775957107543945, loss: 34.89503666198282\n",
      "Epoch: 1480/5000\n",
      "w1: 21.25139808654785, w2: -17.348609924316406, bias: 16.775537490844727, loss: 34.83073400646185\n",
      "Epoch: 1490/5000\n",
      "w1: 21.2919864654541, w2: -17.416826248168945, bias: 16.775100708007812, loss: 34.76763381320467\n",
      "Epoch: 1500/5000\n",
      "w1: 21.332202911376953, w2: -17.48439598083496, bias: 16.774662017822266, loss: 34.70571328295524\n",
      "Epoch: 1510/5000\n",
      "w1: 21.372051239013672, w2: -17.55132484436035, bias: 16.77422332763672, loss: 34.64495099627644\n",
      "Epoch: 1520/5000\n",
      "w1: 21.411537170410156, w2: -17.61762046813965, bias: 16.773784637451172, loss: 34.58532207718401\n",
      "Epoch: 1530/5000\n",
      "w1: 21.45066261291504, w2: -17.683286666870117, bias: 16.773345947265625, loss: 34.52680735347383\n",
      "Epoch: 1540/5000\n",
      "w1: 21.489429473876953, w2: -17.748332977294922, bias: 16.772907257080078, loss: 34.469385454538845\n",
      "Epoch: 1550/5000\n",
      "w1: 21.527843475341797, w2: -17.812761306762695, bias: 16.77245330810547, loss: 34.41303580651575\n",
      "Epoch: 1560/5000\n",
      "w1: 21.565906524658203, w2: -17.8765811920166, bias: 16.771995544433594, loss: 34.35773875618616\n",
      "Epoch: 1570/5000\n",
      "w1: 21.60362434387207, w2: -17.93979263305664, bias: 16.77153778076172, loss: 34.30347518867663\n",
      "Epoch: 1580/5000\n",
      "w1: 21.64099884033203, w2: -18.00240707397461, bias: 16.771080017089844, loss: 34.250224367670384\n",
      "Epoch: 1590/5000\n",
      "w1: 21.67803192138672, w2: -18.064428329467773, bias: 16.77062225341797, loss: 34.19796851187763\n",
      "Epoch: 1600/5000\n",
      "w1: 21.714725494384766, w2: -18.1258602142334, bias: 16.770164489746094, loss: 34.14668918449233\n",
      "Epoch: 1610/5000\n",
      "w1: 21.751083374023438, w2: -18.18671417236328, bias: 16.769702911376953, loss: 34.09636615168729\n",
      "Epoch: 1620/5000\n",
      "w1: 21.787113189697266, w2: -18.24699592590332, bias: 16.76922607421875, loss: 34.046979267427865\n",
      "Epoch: 1630/5000\n",
      "w1: 21.822813034057617, w2: -18.306705474853516, bias: 16.768749237060547, loss: 33.99851548253011\n",
      "Epoch: 1640/5000\n",
      "w1: 21.858190536499023, w2: -18.365842819213867, bias: 16.768272399902344, loss: 33.95095910335992\n",
      "Epoch: 1650/5000\n",
      "w1: 21.89324378967285, w2: -18.424415588378906, bias: 16.76779556274414, loss: 33.90429464960193\n",
      "Epoch: 1660/5000\n",
      "w1: 21.927980422973633, w2: -18.482433319091797, bias: 16.767318725585938, loss: 33.85850092055889\n",
      "Epoch: 1670/5000\n",
      "w1: 21.962398529052734, w2: -18.539901733398438, bias: 16.766841888427734, loss: 33.81356174075977\n",
      "Epoch: 1680/5000\n",
      "w1: 21.996503829956055, w2: -18.596824645996094, bias: 16.76636505126953, loss: 33.769463360747274\n",
      "Epoch: 1690/5000\n",
      "w1: 22.030298233032227, w2: -18.653207778930664, bias: 16.76587677001953, loss: 33.72618723759852\n",
      "Epoch: 1700/5000\n",
      "w1: 22.06378746032715, w2: -18.709056854248047, bias: 16.765380859375, loss: 33.683719903998146\n",
      "Epoch: 1710/5000\n",
      "w1: 22.09697151184082, w2: -18.764375686645508, bias: 16.76488494873047, loss: 33.64204475601858\n",
      "Epoch: 1720/5000\n",
      "w1: 22.129852294921875, w2: -18.819168090820312, bias: 16.764389038085938, loss: 33.60114898896441\n",
      "Epoch: 1730/5000\n",
      "w1: 22.16243553161621, w2: -18.873441696166992, bias: 16.763893127441406, loss: 33.56101666628377\n",
      "Epoch: 1740/5000\n",
      "w1: 22.19472312927246, w2: -18.92719841003418, bias: 16.763397216796875, loss: 33.52163500157649\n",
      "Epoch: 1750/5000\n",
      "w1: 22.226716995239258, w2: -18.980445861816406, bias: 16.762901306152344, loss: 33.482988553241135\n",
      "Epoch: 1760/5000\n",
      "w1: 22.2584171295166, w2: -19.033187866210938, bias: 16.762405395507812, loss: 33.44506417103644\n",
      "Epoch: 1770/5000\n",
      "w1: 22.289831161499023, w2: -19.085430145263672, bias: 16.761899948120117, loss: 33.4078476717967\n",
      "Epoch: 1780/5000\n",
      "w1: 22.320959091186523, w2: -19.137176513671875, bias: 16.761384963989258, loss: 33.371326124609006\n",
      "Epoch: 1790/5000\n",
      "w1: 22.351808547973633, w2: -19.188430786132812, bias: 16.7608699798584, loss: 33.33548707129736\n",
      "Epoch: 1800/5000\n",
      "w1: 22.382375717163086, w2: -19.23919677734375, bias: 16.76035499572754, loss: 33.30031703883684\n",
      "Epoch: 1810/5000\n",
      "w1: 22.412668228149414, w2: -19.28948402404785, bias: 16.75984001159668, loss: 33.265802596231424\n",
      "Epoch: 1820/5000\n",
      "w1: 22.442684173583984, w2: -19.33929443359375, bias: 16.75932502746582, loss: 33.23193206832447\n",
      "Epoch: 1830/5000\n",
      "w1: 22.472429275512695, w2: -19.388628005981445, bias: 16.75881004333496, loss: 33.19869583338976\n",
      "Epoch: 1840/5000\n",
      "w1: 22.501903533935547, w2: -19.4374942779541, bias: 16.7582950592041, loss: 33.16608066366576\n",
      "Epoch: 1850/5000\n",
      "w1: 22.531110763549805, w2: -19.485897064208984, bias: 16.757780075073242, loss: 33.134073942475\n",
      "Epoch: 1860/5000\n",
      "w1: 22.5600528717041, w2: -19.53384017944336, bias: 16.75726318359375, loss: 33.102664741447875\n",
      "Epoch: 1870/5000\n",
      "w1: 22.588733673095703, w2: -19.58132553100586, bias: 16.756729125976562, loss: 33.07184310366072\n",
      "Epoch: 1880/5000\n",
      "w1: 22.617155075073242, w2: -19.628360748291016, bias: 16.756195068359375, loss: 33.04159564630516\n",
      "Epoch: 1890/5000\n",
      "w1: 22.645320892333984, w2: -19.67494773864746, bias: 16.755661010742188, loss: 33.011914304223374\n",
      "Epoch: 1900/5000\n",
      "w1: 22.67323112487793, w2: -19.721092224121094, bias: 16.755126953125, loss: 32.98278652252081\n",
      "Epoch: 1910/5000\n",
      "w1: 22.700889587402344, w2: -19.76679801940918, bias: 16.754592895507812, loss: 32.954202780878504\n",
      "Epoch: 1920/5000\n",
      "w1: 22.72829818725586, w2: -19.812068939208984, bias: 16.754058837890625, loss: 32.92615265730432\n",
      "Epoch: 1930/5000\n",
      "w1: 22.75545883178711, w2: -19.856910705566406, bias: 16.753524780273438, loss: 32.89862642141647\n",
      "Epoch: 1940/5000\n",
      "w1: 22.782373428344727, w2: -19.90132713317871, bias: 16.75299072265625, loss: 32.87161365389631\n",
      "Epoch: 1950/5000\n",
      "w1: 22.809045791625977, w2: -19.94532012939453, bias: 16.752456665039062, loss: 32.84510579107239\n",
      "Epoch: 1960/5000\n",
      "w1: 22.83547592163086, w2: -19.988893508911133, bias: 16.751922607421875, loss: 32.81909305805414\n",
      "Epoch: 1970/5000\n",
      "w1: 22.861669540405273, w2: -20.032052993774414, bias: 16.751379013061523, loss: 32.793565483049846\n",
      "Epoch: 1980/5000\n",
      "w1: 22.887622833251953, w2: -20.074804306030273, bias: 16.75082778930664, loss: 32.76851459714518\n",
      "Epoch: 1990/5000\n",
      "w1: 22.913345336914062, w2: -20.11714744567871, bias: 16.750274658203125, loss: 32.74393068561232\n",
      "Epoch: 2000/5000\n",
      "w1: 22.93883514404297, w2: -20.159086227416992, bias: 16.74972152709961, loss: 32.71980716371297\n",
      "Epoch: 2010/5000\n",
      "w1: 22.964096069335938, w2: -20.20062828063965, bias: 16.749168395996094, loss: 32.69613342448917\n",
      "Epoch: 2020/5000\n",
      "w1: 22.9891300201416, w2: -20.241769790649414, bias: 16.748615264892578, loss: 32.67290266847491\n",
      "Epoch: 2030/5000\n",
      "w1: 23.013938903808594, w2: -20.28252601623535, bias: 16.748062133789062, loss: 32.65010275538566\n",
      "Epoch: 2040/5000\n",
      "w1: 23.038524627685547, w2: -20.322885513305664, bias: 16.747509002685547, loss: 32.627733027044385\n",
      "Epoch: 2050/5000\n",
      "w1: 23.06288719177246, w2: -20.362863540649414, bias: 16.74695587158203, loss: 32.60577960291768\n",
      "Epoch: 2060/5000\n",
      "w1: 23.087032318115234, w2: -20.4024600982666, bias: 16.746402740478516, loss: 32.58423618250061\n",
      "Epoch: 2070/5000\n",
      "w1: 23.1109561920166, w2: -20.441682815551758, bias: 16.745849609375, loss: 32.563094756539\n",
      "Epoch: 2080/5000\n",
      "w1: 23.13466453552246, w2: -20.48053550720215, bias: 16.745296478271484, loss: 32.54234626737753\n",
      "Epoch: 2090/5000\n",
      "w1: 23.158161163330078, w2: -20.51901626586914, bias: 16.74474334716797, loss: 32.52198606252508\n",
      "Epoch: 2100/5000\n",
      "w1: 23.18144416809082, w2: -20.557130813598633, bias: 16.744178771972656, loss: 32.50200615441371\n",
      "Epoch: 2110/5000\n",
      "w1: 23.204519271850586, w2: -20.59488296508789, bias: 16.743610382080078, loss: 32.48239845754813\n",
      "Epoch: 2120/5000\n",
      "w1: 23.227388381958008, w2: -20.632274627685547, bias: 16.743040084838867, loss: 32.4631566182657\n",
      "Epoch: 2130/5000\n",
      "w1: 23.25005340576172, w2: -20.669307708740234, bias: 16.742469787597656, loss: 32.44427436420978\n",
      "Epoch: 2140/5000\n",
      "w1: 23.272512435913086, w2: -20.705989837646484, bias: 16.741897583007812, loss: 32.42574537934572\n",
      "Epoch: 2150/5000\n",
      "w1: 23.29477310180664, w2: -20.742321014404297, bias: 16.74132537841797, loss: 32.40756146353786\n",
      "Epoch: 2160/5000\n",
      "w1: 23.316831588745117, w2: -20.77830696105957, bias: 16.740753173828125, loss: 32.38971739443304\n",
      "Epoch: 2170/5000\n",
      "w1: 23.338699340820312, w2: -20.813949584960938, bias: 16.74018096923828, loss: 32.37220479890129\n",
      "Epoch: 2180/5000\n",
      "w1: 23.360363006591797, w2: -20.84925079345703, bias: 16.739608764648438, loss: 32.35502082999956\n",
      "Epoch: 2190/5000\n",
      "w1: 23.3818302154541, w2: -20.88421630859375, bias: 16.739036560058594, loss: 32.3381580301905\n",
      "Epoch: 2200/5000\n",
      "w1: 23.403106689453125, w2: -20.91884994506836, bias: 16.73846435546875, loss: 32.32160959855354\n",
      "Epoch: 2210/5000\n",
      "w1: 23.424192428588867, w2: -20.95315170288086, bias: 16.737892150878906, loss: 32.305370462684095\n",
      "Epoch: 2220/5000\n",
      "w1: 23.445087432861328, w2: -20.987125396728516, bias: 16.737319946289062, loss: 32.28943497162323\n",
      "Epoch: 2230/5000\n",
      "w1: 23.465801239013672, w2: -21.020774841308594, bias: 16.73674774169922, loss: 32.27379599113846\n",
      "Epoch: 2240/5000\n",
      "w1: 23.4863338470459, w2: -21.054105758666992, bias: 16.736175537109375, loss: 32.25844667985679\n",
      "Epoch: 2250/5000\n",
      "w1: 23.506675720214844, w2: -21.08711814880371, bias: 16.73560333251953, loss: 32.24338530845436\n",
      "Epoch: 2260/5000\n",
      "w1: 23.526836395263672, w2: -21.119815826416016, bias: 16.735031127929688, loss: 32.2286056514227\n",
      "Epoch: 2270/5000\n",
      "w1: 23.546815872192383, w2: -21.15220069885254, bias: 16.734451293945312, loss: 32.214101711560645\n",
      "Epoch: 2280/5000\n",
      "w1: 23.566619873046875, w2: -21.184276580810547, bias: 16.73386573791504, loss: 32.19986739254339\n",
      "Epoch: 2290/5000\n",
      "w1: 23.586244583129883, w2: -21.216045379638672, bias: 16.733280181884766, loss: 32.185899932711266\n",
      "Epoch: 2300/5000\n",
      "w1: 23.605695724487305, w2: -21.247512817382812, bias: 16.73269271850586, loss: 32.17219264587288\n",
      "Epoch: 2310/5000\n",
      "w1: 23.62497329711914, w2: -21.278676986694336, bias: 16.73210334777832, loss: 32.15874128338992\n",
      "Epoch: 2320/5000\n",
      "w1: 23.644079208374023, w2: -21.30954360961914, bias: 16.73151397705078, loss: 32.14554071572783\n",
      "Epoch: 2330/5000\n",
      "w1: 23.663015365600586, w2: -21.340116500854492, bias: 16.73092269897461, loss: 32.132586463701855\n",
      "Epoch: 2340/5000\n",
      "w1: 23.68178367614746, w2: -21.37039566040039, bias: 16.730331420898438, loss: 32.119874138698734\n",
      "Epoch: 2350/5000\n",
      "w1: 23.70038414001465, w2: -21.400388717651367, bias: 16.729740142822266, loss: 32.107398053265634\n",
      "Epoch: 2360/5000\n",
      "w1: 23.718820571899414, w2: -21.430095672607422, bias: 16.729148864746094, loss: 32.095154545657316\n",
      "Epoch: 2370/5000\n",
      "w1: 23.737092971801758, w2: -21.459516525268555, bias: 16.728557586669922, loss: 32.083139650578794\n",
      "Epoch: 2380/5000\n",
      "w1: 23.75520133972168, w2: -21.4886531829834, bias: 16.72796630859375, loss: 32.07135028108892\n",
      "Epoch: 2390/5000\n",
      "w1: 23.773149490356445, w2: -21.517513275146484, bias: 16.727375030517578, loss: 32.05978023561324\n",
      "Epoch: 2400/5000\n",
      "w1: 23.790939331054688, w2: -21.546096801757812, bias: 16.726783752441406, loss: 32.048425687065695\n",
      "Epoch: 2410/5000\n",
      "w1: 23.808568954467773, w2: -21.57440757751465, bias: 16.726192474365234, loss: 32.037283607256974\n",
      "Epoch: 2420/5000\n",
      "w1: 23.82604217529297, w2: -21.602447509765625, bias: 16.725601196289062, loss: 32.02634893575488\n",
      "Epoch: 2430/5000\n",
      "w1: 23.843360900878906, w2: -21.630220413208008, bias: 16.72500991821289, loss: 32.01561803282472\n",
      "Epoch: 2440/5000\n",
      "w1: 23.860525131225586, w2: -21.657726287841797, bias: 16.72441864013672, loss: 32.00508770749426\n",
      "Epoch: 2450/5000\n",
      "w1: 23.877534866333008, w2: -21.68497085571289, bias: 16.723827362060547, loss: 31.99475357850339\n",
      "Epoch: 2460/5000\n",
      "w1: 23.89439582824707, w2: -21.71195411682129, bias: 16.723236083984375, loss: 31.984612439717413\n",
      "Epoch: 2470/5000\n",
      "w1: 23.91110610961914, w2: -21.738676071166992, bias: 16.722644805908203, loss: 31.974661190248145\n",
      "Epoch: 2480/5000\n",
      "w1: 23.92766761779785, w2: -21.76514434814453, bias: 16.72205352783203, loss: 31.964895198221228\n",
      "Epoch: 2490/5000\n",
      "w1: 23.944082260131836, w2: -21.791358947753906, bias: 16.72146224975586, loss: 31.955311362713932\n",
      "Epoch: 2500/5000\n",
      "w1: 23.960351943969727, w2: -21.817323684692383, bias: 16.720870971679688, loss: 31.945905640218633\n",
      "Epoch: 2510/5000\n",
      "w1: 23.97647476196289, w2: -21.843040466308594, bias: 16.720273971557617, loss: 31.936675680963862\n",
      "Epoch: 2520/5000\n",
      "w1: 23.99245834350586, w2: -21.868511199951172, bias: 16.71967315673828, loss: 31.927617925563396\n",
      "Epoch: 2530/5000\n",
      "w1: 24.008298873901367, w2: -21.893735885620117, bias: 16.719070434570312, loss: 31.91872901255555\n",
      "Epoch: 2540/5000\n",
      "w1: 24.02400016784668, w2: -21.918718338012695, bias: 16.718469619750977, loss: 31.910006091805663\n",
      "Epoch: 2550/5000\n",
      "w1: 24.039562225341797, w2: -21.943462371826172, bias: 16.71786880493164, loss: 31.9014460467561\n",
      "Epoch: 2560/5000\n",
      "w1: 24.054988861083984, w2: -21.967967987060547, bias: 16.71726417541504, loss: 31.893045075990354\n",
      "Epoch: 2570/5000\n",
      "w1: 24.07027816772461, w2: -21.992238998413086, bias: 16.71666145324707, loss: 31.884800936170038\n",
      "Epoch: 2580/5000\n",
      "w1: 24.085433959960938, w2: -22.016277313232422, bias: 16.71605682373047, loss: 31.876710526311907\n",
      "Epoch: 2590/5000\n",
      "w1: 24.10045623779297, w2: -22.040084838867188, bias: 16.715452194213867, loss: 31.868770910943258\n",
      "Epoch: 2600/5000\n",
      "w1: 24.115346908569336, w2: -22.063663482666016, bias: 16.714847564697266, loss: 31.860979554121872\n",
      "Epoch: 2610/5000\n",
      "w1: 24.130104064941406, w2: -22.087017059326172, bias: 16.714242935180664, loss: 31.853333198945965\n",
      "Epoch: 2620/5000\n",
      "w1: 24.144733428955078, w2: -22.11014747619629, bias: 16.71363639831543, loss: 31.845828779226167\n",
      "Epoch: 2630/5000\n",
      "w1: 24.15923500061035, w2: -22.133054733276367, bias: 16.713029861450195, loss: 31.838464762691153\n",
      "Epoch: 2640/5000\n",
      "w1: 24.17361068725586, w2: -22.15574073791504, bias: 16.71242332458496, loss: 31.831237872782562\n",
      "Epoch: 2650/5000\n",
      "w1: 24.18785858154297, w2: -22.17820930480957, bias: 16.711814880371094, loss: 31.824145899525845\n",
      "Epoch: 2660/5000\n",
      "w1: 24.20197868347168, w2: -22.200464248657227, bias: 16.71120834350586, loss: 31.817185963833875\n",
      "Epoch: 2670/5000\n",
      "w1: 24.215978622436523, w2: -22.222503662109375, bias: 16.710599899291992, loss: 31.810355390054575\n",
      "Epoch: 2680/5000\n",
      "w1: 24.229854583740234, w2: -22.24433135986328, bias: 16.709993362426758, loss: 31.80365263682566\n",
      "Epoch: 2690/5000\n",
      "w1: 24.243608474731445, w2: -22.265949249267578, bias: 16.709383010864258, loss: 31.797074554259115\n",
      "Epoch: 2700/5000\n",
      "w1: 24.25724220275879, w2: -22.2873592376709, bias: 16.70877456665039, loss: 31.790619649788734\n",
      "Epoch: 2710/5000\n",
      "w1: 24.270755767822266, w2: -22.308561325073242, bias: 16.708166122436523, loss: 31.784285119488185\n",
      "Epoch: 2720/5000\n",
      "w1: 24.284151077270508, w2: -22.329561233520508, bias: 16.707557678222656, loss: 31.778068216053942\n",
      "Epoch: 2730/5000\n",
      "w1: 24.297428131103516, w2: -22.350360870361328, bias: 16.706947326660156, loss: 31.771967026311867\n",
      "Epoch: 2740/5000\n",
      "w1: 24.310590744018555, w2: -22.370956420898438, bias: 16.70633888244629, loss: 31.76597994588421\n",
      "Epoch: 2750/5000\n",
      "w1: 24.323638916015625, w2: -22.391355514526367, bias: 16.70572853088379, loss: 31.7601043172432\n",
      "Epoch: 2760/5000\n",
      "w1: 24.336572647094727, w2: -22.411563873291016, bias: 16.70511817932129, loss: 31.754337049517822\n",
      "Epoch: 2770/5000\n",
      "w1: 24.349393844604492, w2: -22.43157196044922, bias: 16.704509735107422, loss: 31.7486781655558\n",
      "Epoch: 2780/5000\n",
      "w1: 24.362102508544922, w2: -22.451379776000977, bias: 16.703899383544922, loss: 31.74312637371779\n",
      "Epoch: 2790/5000\n",
      "w1: 24.37470054626465, w2: -22.470996856689453, bias: 16.703289031982422, loss: 31.737677916619393\n",
      "Epoch: 2800/5000\n",
      "w1: 24.387187957763672, w2: -22.490436553955078, bias: 16.702678680419922, loss: 31.732329126484156\n",
      "Epoch: 2810/5000\n",
      "w1: 24.399566650390625, w2: -22.50969123840332, bias: 16.702068328857422, loss: 31.727079123858683\n",
      "Epoch: 2820/5000\n",
      "w1: 24.41183853149414, w2: -22.52875518798828, bias: 16.701457977294922, loss: 31.721927979017188\n",
      "Epoch: 2830/5000\n",
      "w1: 24.424001693725586, w2: -22.547630310058594, bias: 16.700847625732422, loss: 31.716874053967768\n",
      "Epoch: 2840/5000\n",
      "w1: 24.436059951782227, w2: -22.56633186340332, bias: 16.700237274169922, loss: 31.711912410662713\n",
      "Epoch: 2850/5000\n",
      "w1: 24.448013305664062, w2: -22.584848403930664, bias: 16.699626922607422, loss: 31.70704413711498\n",
      "Epoch: 2860/5000\n",
      "w1: 24.459863662719727, w2: -22.603187561035156, bias: 16.699016571044922, loss: 31.70226603980883\n",
      "Epoch: 2870/5000\n",
      "w1: 24.47161102294922, w2: -22.62135124206543, bias: 16.698406219482422, loss: 31.697576755707367\n",
      "Epoch: 2880/5000\n",
      "w1: 24.48325538635254, w2: -22.63933753967285, bias: 16.697795867919922, loss: 31.69297547916398\n",
      "Epoch: 2890/5000\n",
      "w1: 24.49479866027832, w2: -22.657150268554688, bias: 16.697185516357422, loss: 31.688459847108405\n",
      "Epoch: 2900/5000\n",
      "w1: 24.506242752075195, w2: -22.67479133605957, bias: 16.696575164794922, loss: 31.684028102241175\n",
      "Epoch: 2910/5000\n",
      "w1: 24.517587661743164, w2: -22.692262649536133, bias: 16.695964813232422, loss: 31.679678519144776\n",
      "Epoch: 2920/5000\n",
      "w1: 24.528831481933594, w2: -22.709564208984375, bias: 16.695354461669922, loss: 31.67541058704156\n",
      "Epoch: 2930/5000\n",
      "w1: 24.539979934692383, w2: -22.72669792175293, bias: 16.694744110107422, loss: 31.67122197566539\n",
      "Epoch: 2940/5000\n",
      "w1: 24.55103302001953, w2: -22.743669509887695, bias: 16.694133758544922, loss: 31.667110925434894\n",
      "Epoch: 2950/5000\n",
      "w1: 24.56199073791504, w2: -22.760475158691406, bias: 16.693523406982422, loss: 31.663076350529945\n",
      "Epoch: 2960/5000\n",
      "w1: 24.572853088378906, w2: -22.77712059020996, bias: 16.692913055419922, loss: 31.659116859235883\n",
      "Epoch: 2970/5000\n",
      "w1: 24.583620071411133, w2: -22.793601989746094, bias: 16.692302703857422, loss: 31.6552313952582\n",
      "Epoch: 2980/5000\n",
      "w1: 24.59429359436035, w2: -22.809926986694336, bias: 16.691692352294922, loss: 31.651418292320688\n",
      "Epoch: 2990/5000\n",
      "w1: 24.60487937927246, w2: -22.826093673706055, bias: 16.691082000732422, loss: 31.647675526771796\n",
      "Epoch: 3000/5000\n",
      "w1: 24.61536979675293, w2: -22.842103958129883, bias: 16.690471649169922, loss: 31.644002808610242\n",
      "Epoch: 3010/5000\n",
      "w1: 24.62577247619629, w2: -22.85795783996582, bias: 16.689861297607422, loss: 31.640398356944104\n",
      "Epoch: 3020/5000\n",
      "w1: 24.636083602905273, w2: -22.873659133911133, bias: 16.689250946044922, loss: 31.636861202648152\n",
      "Epoch: 3030/5000\n",
      "w1: 24.64630699157715, w2: -22.88920783996582, bias: 16.688640594482422, loss: 31.633389619515505\n",
      "Epoch: 3040/5000\n",
      "w1: 24.656442642211914, w2: -22.90460777282715, bias: 16.688030242919922, loss: 31.629982295046908\n",
      "Epoch: 3050/5000\n",
      "w1: 24.66649055480957, w2: -22.919857025146484, bias: 16.687419891357422, loss: 31.62663881023627\n",
      "Epoch: 3060/5000\n",
      "w1: 24.67645263671875, w2: -22.93495750427246, bias: 16.686809539794922, loss: 31.62335740608714\n",
      "Epoch: 3070/5000\n",
      "w1: 24.686328887939453, w2: -22.949913024902344, bias: 16.686199188232422, loss: 31.62013730110685\n",
      "Epoch: 3080/5000\n",
      "w1: 24.69611930847168, w2: -22.964723587036133, bias: 16.685588836669922, loss: 31.6169766947953\n",
      "Epoch: 3090/5000\n",
      "w1: 24.705825805664062, w2: -22.97939109802246, bias: 16.684978485107422, loss: 31.61387521840977\n",
      "Epoch: 3100/5000\n",
      "w1: 24.715452194213867, w2: -22.993915557861328, bias: 16.684368133544922, loss: 31.610830934914564\n",
      "Epoch: 3110/5000\n",
      "w1: 24.724994659423828, w2: -23.00830078125, bias: 16.683757781982422, loss: 31.607843226981842\n",
      "Epoch: 3120/5000\n",
      "w1: 24.73445701599121, w2: -23.022544860839844, bias: 16.683147430419922, loss: 31.604911298783126\n",
      "Epoch: 3130/5000\n",
      "w1: 24.743837356567383, w2: -23.036649703979492, bias: 16.682537078857422, loss: 31.6020338262696\n",
      "Epoch: 3140/5000\n",
      "w1: 24.753137588500977, w2: -23.05061912536621, bias: 16.681926727294922, loss: 31.599209867590965\n",
      "Epoch: 3150/5000\n",
      "w1: 24.762357711791992, w2: -23.064453125, bias: 16.681316375732422, loss: 31.596438406646897\n",
      "Epoch: 3160/5000\n",
      "w1: 24.771499633789062, w2: -23.07815170288086, bias: 16.680706024169922, loss: 31.593718357439627\n",
      "Epoch: 3170/5000\n",
      "w1: 24.780563354492188, w2: -23.091718673706055, bias: 16.680095672607422, loss: 31.59104882644863\n",
      "Epoch: 3180/5000\n",
      "w1: 24.78955078125, w2: -23.105152130126953, bias: 16.679485321044922, loss: 31.588429017614935\n",
      "Epoch: 3190/5000\n",
      "w1: 24.7984619140625, w2: -23.11845588684082, bias: 16.678874969482422, loss: 31.58585772398922\n",
      "Epoch: 3200/5000\n",
      "w1: 24.807296752929688, w2: -23.131629943847656, bias: 16.678264617919922, loss: 31.583334515489994\n",
      "Epoch: 3210/5000\n",
      "w1: 24.816057205200195, w2: -23.144676208496094, bias: 16.677654266357422, loss: 31.58085796775014\n",
      "Epoch: 3220/5000\n",
      "w1: 24.824743270874023, w2: -23.157594680786133, bias: 16.677043914794922, loss: 31.578427343105258\n",
      "Epoch: 3230/5000\n",
      "w1: 24.833356857299805, w2: -23.170387268066406, bias: 16.676433563232422, loss: 31.576041995984006\n",
      "Epoch: 3240/5000\n",
      "w1: 24.841896057128906, w2: -23.183055877685547, bias: 16.675825119018555, loss: 31.573700727475824\n",
      "Epoch: 3250/5000\n",
      "w1: 24.85036277770996, w2: -23.195600509643555, bias: 16.675216674804688, loss: 31.571403259672362\n",
      "Epoch: 3260/5000\n",
      "w1: 24.85875701904297, w2: -23.208023071289062, bias: 16.674610137939453, loss: 31.569148241469172\n",
      "Epoch: 3270/5000\n",
      "w1: 24.867082595825195, w2: -23.220325469970703, bias: 16.67400360107422, loss: 31.566935154491883\n",
      "Epoch: 3280/5000\n",
      "w1: 24.875335693359375, w2: -23.23250961303711, bias: 16.673397064208984, loss: 31.56476294501147\n",
      "Epoch: 3290/5000\n",
      "w1: 24.883520126342773, w2: -23.24457359313965, bias: 16.67279052734375, loss: 31.56263111468893\n",
      "Epoch: 3300/5000\n",
      "w1: 24.89163589477539, w2: -23.256519317626953, bias: 16.67218589782715, loss: 31.56053903302323\n",
      "Epoch: 3310/5000\n",
      "w1: 24.899681091308594, w2: -23.268348693847656, bias: 16.671581268310547, loss: 31.55848552523012\n",
      "Epoch: 3320/5000\n",
      "w1: 24.90765953063965, w2: -23.280061721801758, bias: 16.670976638793945, loss: 31.556470497409894\n",
      "Epoch: 3330/5000\n",
      "w1: 24.915571212768555, w2: -23.29166030883789, bias: 16.67037010192871, loss: 31.5544926501355\n",
      "Epoch: 3340/5000\n",
      "w1: 24.923416137695312, w2: -23.30314826965332, bias: 16.669767379760742, loss: 31.5525515653246\n",
      "Epoch: 3350/5000\n",
      "w1: 24.931194305419922, w2: -23.31452178955078, bias: 16.66916275024414, loss: 31.550646430131128\n",
      "Epoch: 3360/5000\n",
      "w1: 24.93890953063965, w2: -23.325786590576172, bias: 16.66855812072754, loss: 31.548776288987717\n",
      "Epoch: 3370/5000\n",
      "w1: 24.946556091308594, w2: -23.336938858032227, bias: 16.66795539855957, loss: 31.546941246169713\n",
      "Epoch: 3380/5000\n",
      "w1: 24.954137802124023, w2: -23.34798240661621, bias: 16.667354583740234, loss: 31.545140371648742\n",
      "Epoch: 3390/5000\n",
      "w1: 24.96165657043457, w2: -23.358919143676758, bias: 16.6667537689209, loss: 31.543372909108374\n",
      "Epoch: 3400/5000\n",
      "w1: 24.9691104888916, w2: -23.369749069213867, bias: 16.666152954101562, loss: 31.541637977063598\n",
      "Epoch: 3410/5000\n",
      "w1: 24.97650718688965, w2: -23.380474090576172, bias: 16.665550231933594, loss: 31.539935010081855\n",
      "Epoch: 3420/5000\n",
      "w1: 24.983840942382812, w2: -23.39109230041504, bias: 16.664949417114258, loss: 31.538263671216892\n",
      "Epoch: 3430/5000\n",
      "w1: 24.991111755371094, w2: -23.401607513427734, bias: 16.664350509643555, loss: 31.536623503795504\n",
      "Epoch: 3440/5000\n",
      "w1: 24.998323440551758, w2: -23.412019729614258, bias: 16.66374969482422, loss: 31.53501340884045\n",
      "Epoch: 3450/5000\n",
      "w1: 25.00547218322754, w2: -23.42232894897461, bias: 16.663150787353516, loss: 31.533433472588122\n",
      "Epoch: 3460/5000\n",
      "w1: 25.012563705444336, w2: -23.432538986206055, bias: 16.66254997253418, loss: 31.53188257234051\n",
      "Epoch: 3470/5000\n",
      "w1: 25.019594192504883, w2: -23.44264793395996, bias: 16.66195297241211, loss: 31.530360617536406\n",
      "Epoch: 3480/5000\n",
      "w1: 25.026567459106445, w2: -23.452659606933594, bias: 16.66135597229004, loss: 31.528866374513203\n",
      "Epoch: 3490/5000\n",
      "w1: 25.03348159790039, w2: -23.462568283081055, bias: 16.660757064819336, loss: 31.52740051027599\n",
      "Epoch: 3500/5000\n",
      "w1: 25.04033851623535, w2: -23.472381591796875, bias: 16.660160064697266, loss: 31.52596157555627\n",
      "Epoch: 3510/5000\n",
      "w1: 25.047138214111328, w2: -23.482099533081055, bias: 16.659563064575195, loss: 31.52454918362663\n",
      "Epoch: 3520/5000\n",
      "w1: 25.053882598876953, w2: -23.491722106933594, bias: 16.658966064453125, loss: 31.523162846031052\n",
      "Epoch: 3530/5000\n",
      "w1: 25.060569763183594, w2: -23.501249313354492, bias: 16.658370971679688, loss: 31.52180230184211\n",
      "Epoch: 3540/5000\n",
      "w1: 25.067201614379883, w2: -23.51068115234375, bias: 16.657773971557617, loss: 31.520467083147107\n",
      "Epoch: 3550/5000\n",
      "w1: 25.073780059814453, w2: -23.52002716064453, bias: 16.657180786132812, loss: 31.51915611069569\n",
      "Epoch: 3560/5000\n",
      "w1: 25.080303192138672, w2: -23.529277801513672, bias: 16.656585693359375, loss: 31.517869465151367\n",
      "Epoch: 3570/5000\n",
      "w1: 25.08677101135254, w2: -23.538436889648438, bias: 16.65599250793457, loss: 31.516606729328416\n",
      "Epoch: 3580/5000\n",
      "w1: 25.093185424804688, w2: -23.54750633239746, bias: 16.6554012298584, loss: 31.515367305574845\n",
      "Epoch: 3590/5000\n",
      "w1: 25.09954833984375, w2: -23.556488037109375, bias: 16.654808044433594, loss: 31.51415071206534\n",
      "Epoch: 3600/5000\n",
      "w1: 25.105857849121094, w2: -23.565380096435547, bias: 16.654216766357422, loss: 31.51295672216799\n",
      "Epoch: 3610/5000\n",
      "w1: 25.11211395263672, w2: -23.57418441772461, bias: 16.65362548828125, loss: 31.51178493538161\n",
      "Epoch: 3620/5000\n",
      "w1: 25.11832046508789, w2: -23.582901000976562, bias: 16.653034210205078, loss: 31.510634588261105\n",
      "Epoch: 3630/5000\n",
      "w1: 25.124475479125977, w2: -23.591533660888672, bias: 16.652442932128906, loss: 31.50950549904482\n",
      "Epoch: 3640/5000\n",
      "w1: 25.130578994750977, w2: -23.600080490112305, bias: 16.651851654052734, loss: 31.50839732262137\n",
      "Epoch: 3650/5000\n",
      "w1: 25.136634826660156, w2: -23.608543395996094, bias: 16.651260375976562, loss: 31.507309465437725\n",
      "Epoch: 3660/5000\n",
      "w1: 25.14263916015625, w2: -23.61692237854004, bias: 16.65066909790039, loss: 31.506241781037\n",
      "Epoch: 3670/5000\n",
      "w1: 25.14859390258789, w2: -23.62521743774414, bias: 16.65007781982422, loss: 31.505193961144926\n",
      "Epoch: 3680/5000\n",
      "w1: 25.154502868652344, w2: -23.633432388305664, bias: 16.649486541748047, loss: 31.50416525573766\n",
      "Epoch: 3690/5000\n",
      "w1: 25.160362243652344, w2: -23.641563415527344, bias: 16.648895263671875, loss: 31.503155553727552\n",
      "Epoch: 3700/5000\n",
      "w1: 25.166175842285156, w2: -23.649616241455078, bias: 16.648303985595703, loss: 31.502164434582014\n",
      "Epoch: 3710/5000\n",
      "w1: 25.171939849853516, w2: -23.657588958740234, bias: 16.64771270751953, loss: 31.501191637751557\n",
      "Epoch: 3720/5000\n",
      "w1: 25.177658081054688, w2: -23.665481567382812, bias: 16.64712142944336, loss: 31.500236905145613\n",
      "Epoch: 3730/5000\n",
      "w1: 25.18332862854004, w2: -23.673297882080078, bias: 16.646530151367188, loss: 31.499299576571225\n",
      "Epoch: 3740/5000\n",
      "w1: 25.188955307006836, w2: -23.681034088134766, bias: 16.645938873291016, loss: 31.498379704125714\n",
      "Epoch: 3750/5000\n",
      "w1: 25.194536209106445, w2: -23.68869400024414, bias: 16.645347595214844, loss: 31.497476748658034\n",
      "Epoch: 3760/5000\n",
      "w1: 25.200071334838867, w2: -23.696277618408203, bias: 16.644756317138672, loss: 31.49659036779337\n",
      "Epoch: 3770/5000\n",
      "w1: 25.205564498901367, w2: -23.703784942626953, bias: 16.6441650390625, loss: 31.49572032913685\n",
      "Epoch: 3780/5000\n",
      "w1: 25.21101188659668, w2: -23.711219787597656, bias: 16.643573760986328, loss: 31.49486622371214\n",
      "Epoch: 3790/5000\n",
      "w1: 25.216415405273438, w2: -23.71858024597168, bias: 16.642982482910156, loss: 31.49402776379844\n",
      "Epoch: 3800/5000\n",
      "w1: 25.221776962280273, w2: -23.725866317749023, bias: 16.642391204833984, loss: 31.493204832035488\n",
      "Epoch: 3810/5000\n",
      "w1: 25.227094650268555, w2: -23.73307991027832, bias: 16.641799926757812, loss: 31.492397071838894\n",
      "Epoch: 3820/5000\n",
      "w1: 25.232372283935547, w2: -23.74022102355957, bias: 16.64120864868164, loss: 31.491604204859122\n",
      "Epoch: 3830/5000\n",
      "w1: 25.237606048583984, w2: -23.747289657592773, bias: 16.6406192779541, loss: 31.490825862516246\n",
      "Epoch: 3840/5000\n",
      "w1: 25.2427978515625, w2: -23.754289627075195, bias: 16.64003562927246, loss: 31.490061818645998\n",
      "Epoch: 3850/5000\n",
      "w1: 25.247947692871094, w2: -23.761220932006836, bias: 16.639453887939453, loss: 31.489311907441923\n",
      "Epoch: 3860/5000\n",
      "w1: 25.2530574798584, w2: -23.768081665039062, bias: 16.638870239257812, loss: 31.48857568016333\n",
      "Epoch: 3870/5000\n",
      "w1: 25.25812530517578, w2: -23.77487564086914, bias: 16.638288497924805, loss: 31.48785304206578\n",
      "Epoch: 3880/5000\n",
      "w1: 25.263153076171875, w2: -23.781599044799805, bias: 16.637706756591797, loss: 31.487143706036846\n",
      "Epoch: 3890/5000\n",
      "w1: 25.268138885498047, w2: -23.788259506225586, bias: 16.637126922607422, loss: 31.486447231770004\n",
      "Epoch: 3900/5000\n",
      "w1: 25.27308464050293, w2: -23.794849395751953, bias: 16.636547088623047, loss: 31.485763807691313\n",
      "Epoch: 3910/5000\n",
      "w1: 25.277992248535156, w2: -23.801376342773438, bias: 16.635967254638672, loss: 31.485092779958027\n",
      "Epoch: 3920/5000\n",
      "w1: 25.282859802246094, w2: -23.807838439941406, bias: 16.635391235351562, loss: 31.48443396809513\n",
      "Epoch: 3930/5000\n",
      "w1: 25.287687301635742, w2: -23.814237594604492, bias: 16.634815216064453, loss: 31.483787324124105\n",
      "Epoch: 3940/5000\n",
      "w1: 25.292476654052734, w2: -23.820573806762695, bias: 16.634239196777344, loss: 31.483152454167215\n",
      "Epoch: 3950/5000\n",
      "w1: 25.29722785949707, w2: -23.826845169067383, bias: 16.633663177490234, loss: 31.482529194146622\n",
      "Epoch: 3960/5000\n",
      "w1: 25.30194091796875, w2: -23.833053588867188, bias: 16.633089065551758, loss: 31.481917510453705\n",
      "Epoch: 3970/5000\n",
      "w1: 25.306615829467773, w2: -23.83919906616211, bias: 16.63251495361328, loss: 31.48131711077176\n",
      "Epoch: 3980/5000\n",
      "w1: 25.311256408691406, w2: -23.845279693603516, bias: 16.631940841674805, loss: 31.480727659304282\n",
      "Epoch: 3990/5000\n",
      "w1: 25.315858840942383, w2: -23.851303100585938, bias: 16.63136863708496, loss: 31.48014895966557\n",
      "Epoch: 4000/5000\n",
      "w1: 25.320425033569336, w2: -23.857263565063477, bias: 16.630796432495117, loss: 31.47958107760936\n",
      "Epoch: 4010/5000\n",
      "w1: 25.324954986572266, w2: -23.86316680908203, bias: 16.63022232055664, loss: 31.47902333868879\n",
      "Epoch: 4020/5000\n",
      "w1: 25.329448699951172, w2: -23.86901092529297, bias: 16.629650115966797, loss: 31.478475822200025\n",
      "Epoch: 4030/5000\n",
      "w1: 25.333906173706055, w2: -23.87479591369629, bias: 16.629077911376953, loss: 31.47793837370935\n",
      "Epoch: 4040/5000\n",
      "w1: 25.338329315185547, w2: -23.880521774291992, bias: 16.62850570678711, loss: 31.477410767159544\n",
      "Epoch: 4050/5000\n",
      "w1: 25.34271812438965, w2: -23.88619041442871, bias: 16.627933502197266, loss: 31.476892756278396\n",
      "Epoch: 4060/5000\n",
      "w1: 25.347074508666992, w2: -23.891801834106445, bias: 16.627361297607422, loss: 31.476384206996563\n",
      "Epoch: 4070/5000\n",
      "w1: 25.351394653320312, w2: -23.897356033325195, bias: 16.626789093017578, loss: 31.475884986505108\n",
      "Epoch: 4080/5000\n",
      "w1: 25.355682373046875, w2: -23.902854919433594, bias: 16.626216888427734, loss: 31.47539477736603\n",
      "Epoch: 4090/5000\n",
      "w1: 25.359935760498047, w2: -23.90829849243164, bias: 16.62564468383789, loss: 31.474913535207897\n",
      "Epoch: 4100/5000\n",
      "w1: 25.364158630371094, w2: -23.913684844970703, bias: 16.625072479248047, loss: 31.47444115615614\n",
      "Epoch: 4110/5000\n",
      "w1: 25.36834716796875, w2: -23.919017791748047, bias: 16.624500274658203, loss: 31.47397731169196\n",
      "Epoch: 4120/5000\n",
      "w1: 25.37250328063965, w2: -23.924297332763672, bias: 16.62392807006836, loss: 31.473521983437372\n",
      "Epoch: 4130/5000\n",
      "w1: 25.376628875732422, w2: -23.929523468017578, bias: 16.623355865478516, loss: 31.473074852398582\n",
      "Epoch: 4140/5000\n",
      "w1: 25.380722045898438, w2: -23.934696197509766, bias: 16.622783660888672, loss: 31.472635825636438\n",
      "Epoch: 4150/5000\n",
      "w1: 25.384784698486328, w2: -23.939815521240234, bias: 16.622211456298828, loss: 31.472204789633516\n",
      "Epoch: 4160/5000\n",
      "w1: 25.388816833496094, w2: -23.944883346557617, bias: 16.621639251708984, loss: 31.471781612343367\n",
      "Epoch: 4170/5000\n",
      "w1: 25.39281463623047, w2: -23.949899673461914, bias: 16.62106704711914, loss: 31.47136627882276\n",
      "Epoch: 4180/5000\n",
      "w1: 25.396785736083984, w2: -23.954864501953125, bias: 16.620494842529297, loss: 31.470958262106482\n",
      "Epoch: 4190/5000\n",
      "w1: 25.400724411010742, w2: -23.959779739379883, bias: 16.619922637939453, loss: 31.470557761736558\n",
      "Epoch: 4200/5000\n",
      "w1: 25.404634475708008, w2: -23.964643478393555, bias: 16.61935806274414, loss: 31.470164602034785\n",
      "Epoch: 4210/5000\n",
      "w1: 25.40851402282715, w2: -23.969459533691406, bias: 16.618797302246094, loss: 31.46977850252489\n",
      "Epoch: 4220/5000\n",
      "w1: 25.412363052368164, w2: -23.974225997924805, bias: 16.618234634399414, loss: 31.469399480843602\n",
      "Epoch: 4230/5000\n",
      "w1: 25.41618537902832, w2: -23.978940963745117, bias: 16.617671966552734, loss: 31.469027357411623\n",
      "Epoch: 4240/5000\n",
      "w1: 25.41997528076172, w2: -23.983610153198242, bias: 16.617111206054688, loss: 31.468661997119597\n",
      "Epoch: 4250/5000\n",
      "w1: 25.423738479614258, w2: -23.988229751586914, bias: 16.61655044555664, loss: 31.46830325487105\n",
      "Epoch: 4260/5000\n",
      "w1: 25.42746925354004, w2: -23.99280548095703, bias: 16.615991592407227, loss: 31.467951022229844\n",
      "Epoch: 4270/5000\n",
      "w1: 25.431175231933594, w2: -23.99733543395996, bias: 16.615432739257812, loss: 31.467604971055614\n",
      "Epoch: 4280/5000\n",
      "w1: 25.43484878540039, w2: -24.00181770324707, bias: 16.61487579345703, loss: 31.46726542321392\n",
      "Epoch: 4290/5000\n",
      "w1: 25.43849754333496, w2: -24.006256103515625, bias: 16.61431884765625, loss: 31.46693188242749\n",
      "Epoch: 4300/5000\n",
      "w1: 25.442115783691406, w2: -24.01064682006836, bias: 16.6137638092041, loss: 31.466604429287717\n",
      "Epoch: 4310/5000\n",
      "w1: 25.445709228515625, w2: -24.01499366760254, bias: 16.613208770751953, loss: 31.466282899095688\n",
      "Epoch: 4320/5000\n",
      "w1: 25.44927406311035, w2: -24.019296646118164, bias: 16.612651824951172, loss: 31.465967116123174\n",
      "Epoch: 4330/5000\n",
      "w1: 25.45281219482422, w2: -24.0235538482666, bias: 16.612098693847656, loss: 31.46565709977353\n",
      "Epoch: 4340/5000\n",
      "w1: 25.456323623657227, w2: -24.027769088745117, bias: 16.61154556274414, loss: 31.465352614336624\n",
      "Epoch: 4350/5000\n",
      "w1: 25.459806442260742, w2: -24.03194236755371, bias: 16.610992431640625, loss: 31.46505370227624\n",
      "Epoch: 4360/5000\n",
      "w1: 25.46326446533203, w2: -24.036069869995117, bias: 16.61043930053711, loss: 31.464760166480808\n",
      "Epoch: 4370/5000\n",
      "w1: 25.46669578552246, w2: -24.0401554107666, bias: 16.609886169433594, loss: 31.46447193140799\n",
      "Epoch: 4380/5000\n",
      "w1: 25.47010040283203, w2: -24.044200897216797, bias: 16.609333038330078, loss: 31.46418889814565\n",
      "Epoch: 4390/5000\n",
      "w1: 25.473482131958008, w2: -24.04820442199707, bias: 16.608779907226562, loss: 31.46391087764617\n",
      "Epoch: 4400/5000\n",
      "w1: 25.476837158203125, w2: -24.052165985107422, bias: 16.608226776123047, loss: 31.463637864067778\n",
      "Epoch: 4410/5000\n",
      "w1: 25.480167388916016, w2: -24.056087493896484, bias: 16.60767364501953, loss: 31.463369775894762\n",
      "Epoch: 4420/5000\n",
      "w1: 25.483474731445312, w2: -24.059967041015625, bias: 16.607120513916016, loss: 31.463106418613407\n",
      "Epoch: 4430/5000\n",
      "w1: 25.486757278442383, w2: -24.063806533813477, bias: 16.6065673828125, loss: 31.462847799728262\n",
      "Epoch: 4440/5000\n",
      "w1: 25.490013122558594, w2: -24.06760597229004, bias: 16.606014251708984, loss: 31.46259396584051\n",
      "Epoch: 4450/5000\n",
      "w1: 25.49324607849121, w2: -24.071365356445312, bias: 16.60546112060547, loss: 31.462344664277936\n",
      "Epoch: 4460/5000\n",
      "w1: 25.496458053588867, w2: -24.07508659362793, bias: 16.604907989501953, loss: 31.462099699835793\n",
      "Epoch: 4470/5000\n",
      "w1: 25.49964714050293, w2: -24.07876968383789, bias: 16.604354858398438, loss: 31.461859073723183\n",
      "Epoch: 4480/5000\n",
      "w1: 25.502809524536133, w2: -24.082414627075195, bias: 16.603801727294922, loss: 31.46162278593264\n",
      "Epoch: 4490/5000\n",
      "w1: 25.505950927734375, w2: -24.086021423339844, bias: 16.603248596191406, loss: 31.461390715835474\n",
      "Epoch: 4500/5000\n",
      "w1: 25.509069442749023, w2: -24.089590072631836, bias: 16.60269546508789, loss: 31.461162804399912\n",
      "Epoch: 4510/5000\n",
      "w1: 25.51216697692871, w2: -24.093120574951172, bias: 16.602144241333008, loss: 31.460938944640926\n",
      "Epoch: 4520/5000\n",
      "w1: 25.515241622924805, w2: -24.09661293029785, bias: 16.601600646972656, loss: 31.46071916937546\n",
      "Epoch: 4530/5000\n",
      "w1: 25.518293380737305, w2: -24.10007095336914, bias: 16.601055145263672, loss: 31.460503180051642\n",
      "Epoch: 4540/5000\n",
      "w1: 25.52132225036621, w2: -24.103492736816406, bias: 16.600513458251953, loss: 31.460291191024755\n",
      "Epoch: 4550/5000\n",
      "w1: 25.524328231811523, w2: -24.10687828063965, bias: 16.599971771240234, loss: 31.460082958991205\n",
      "Epoch: 4560/5000\n",
      "w1: 25.527313232421875, w2: -24.110227584838867, bias: 16.599430084228516, loss: 31.45987843908502\n",
      "Epoch: 4570/5000\n",
      "w1: 25.530275344848633, w2: -24.113542556762695, bias: 16.59889030456543, loss: 31.4596775623186\n",
      "Epoch: 4580/5000\n",
      "w1: 25.53321647644043, w2: -24.116823196411133, bias: 16.598352432250977, loss: 31.459480222016694\n",
      "Epoch: 4590/5000\n",
      "w1: 25.536136627197266, w2: -24.120071411132812, bias: 16.597814559936523, loss: 31.459286317843063\n",
      "Epoch: 4600/5000\n",
      "w1: 25.539033889770508, w2: -24.1232852935791, bias: 16.597274780273438, loss: 31.45909591475172\n",
      "Epoch: 4610/5000\n",
      "w1: 25.54191017150879, w2: -24.126462936401367, bias: 16.596738815307617, loss: 31.458908956988274\n",
      "Epoch: 4620/5000\n",
      "w1: 25.54476547241211, w2: -24.129610061645508, bias: 16.596202850341797, loss: 31.458725232289503\n",
      "Epoch: 4630/5000\n",
      "w1: 25.54759979248047, w2: -24.132722854614258, bias: 16.59566879272461, loss: 31.4585448153867\n",
      "Epoch: 4640/5000\n",
      "w1: 25.550413131713867, w2: -24.13580322265625, bias: 16.595134735107422, loss: 31.458367649116646\n",
      "Epoch: 4650/5000\n",
      "w1: 25.553207397460938, w2: -24.138851165771484, bias: 16.594600677490234, loss: 31.458193518281114\n",
      "Epoch: 4660/5000\n",
      "w1: 25.555980682373047, w2: -24.141868591308594, bias: 16.594066619873047, loss: 31.45802243452242\n",
      "Epoch: 4670/5000\n",
      "w1: 25.558734893798828, w2: -24.144853591918945, bias: 16.59353256225586, loss: 31.45785441695066\n",
      "Epoch: 4680/5000\n",
      "w1: 25.56146812438965, w2: -24.14780616760254, bias: 16.592998504638672, loss: 31.45768936149812\n",
      "Epoch: 4690/5000\n",
      "w1: 25.56418228149414, w2: -24.150728225708008, bias: 16.592464447021484, loss: 31.457527175509554\n",
      "Epoch: 4700/5000\n",
      "w1: 25.566877365112305, w2: -24.15361785888672, bias: 16.591930389404297, loss: 31.457367929176087\n",
      "Epoch: 4710/5000\n",
      "w1: 25.56955337524414, w2: -24.156478881835938, bias: 16.59139633178711, loss: 31.457211362642678\n",
      "Epoch: 4720/5000\n",
      "w1: 25.57221031188965, w2: -24.15930938720703, bias: 16.590862274169922, loss: 31.457057601727364\n",
      "Epoch: 4730/5000\n",
      "w1: 25.57485008239746, w2: -24.162107467651367, bias: 16.590328216552734, loss: 31.45690650293129\n",
      "Epoch: 4740/5000\n",
      "w1: 25.577470779418945, w2: -24.16487693786621, bias: 16.589794158935547, loss: 31.456758026497756\n",
      "Epoch: 4750/5000\n",
      "w1: 25.580074310302734, w2: -24.16761589050293, bias: 16.58926010131836, loss: 31.45661219091562\n",
      "Epoch: 4760/5000\n",
      "w1: 25.582658767700195, w2: -24.170326232910156, bias: 16.588726043701172, loss: 31.456468852987417\n",
      "Epoch: 4770/5000\n",
      "w1: 25.58522605895996, w2: -24.173006057739258, bias: 16.588191986083984, loss: 31.456328082897567\n",
      "Epoch: 4780/5000\n",
      "w1: 25.58777618408203, w2: -24.1756591796875, bias: 16.587657928466797, loss: 31.45618964197487\n",
      "Epoch: 4790/5000\n",
      "w1: 25.590309143066406, w2: -24.178281784057617, bias: 16.58712387084961, loss: 31.45605369900774\n",
      "Epoch: 4800/5000\n",
      "w1: 25.592824935913086, w2: -24.180877685546875, bias: 16.586589813232422, loss: 31.45592002034902\n",
      "Epoch: 4810/5000\n",
      "w1: 25.59532356262207, w2: -24.18344497680664, bias: 16.586057662963867, loss: 31.45578877986064\n",
      "Epoch: 4820/5000\n",
      "w1: 25.59780502319336, w2: -24.185983657836914, bias: 16.585533142089844, loss: 31.45565977137621\n",
      "Epoch: 4830/5000\n",
      "w1: 25.60026741027832, w2: -24.188495635986328, bias: 16.58500862121582, loss: 31.455533096319687\n",
      "Epoch: 4840/5000\n",
      "w1: 25.60271453857422, w2: -24.190982818603516, bias: 16.58448600769043, loss: 31.45540850416102\n",
      "Epoch: 4850/5000\n",
      "w1: 25.605144500732422, w2: -24.19344139099121, bias: 16.58396339416504, loss: 31.45528608997391\n",
      "Epoch: 4860/5000\n",
      "w1: 25.60755729675293, w2: -24.195873260498047, bias: 16.58344078063965, loss: 31.455165839881868\n",
      "Epoch: 4870/5000\n",
      "w1: 25.609952926635742, w2: -24.19828224182129, bias: 16.582921981811523, loss: 31.45504760562278\n",
      "Epoch: 4880/5000\n",
      "w1: 25.61233139038086, w2: -24.200660705566406, bias: 16.582401275634766, loss: 31.454931486244046\n",
      "Epoch: 4890/5000\n",
      "w1: 25.614694595336914, w2: -24.20301628112793, bias: 16.58188247680664, loss: 31.454817326773796\n",
      "Epoch: 4900/5000\n",
      "w1: 25.617040634155273, w2: -24.205347061157227, bias: 16.58136558532715, loss: 31.454705189895932\n",
      "Epoch: 4910/5000\n",
      "w1: 25.619369506835938, w2: -24.20764923095703, bias: 16.580848693847656, loss: 31.454594992629954\n",
      "Epoch: 4920/5000\n",
      "w1: 25.621681213378906, w2: -24.209928512573242, bias: 16.580331802368164, loss: 31.454486708249476\n",
      "Epoch: 4930/5000\n",
      "w1: 25.623977661132812, w2: -24.21218490600586, bias: 16.579816818237305, loss: 31.454380233584246\n",
      "Epoch: 4940/5000\n",
      "w1: 25.626258850097656, w2: -24.21441650390625, bias: 16.579301834106445, loss: 31.454275534502802\n",
      "Epoch: 4950/5000\n",
      "w1: 25.62852668762207, w2: -24.21662139892578, bias: 16.578786849975586, loss: 31.454172669643047\n",
      "Epoch: 4960/5000\n",
      "w1: 25.63077735900879, w2: -24.21880340576172, bias: 16.578271865844727, loss: 31.454071616181654\n",
      "Epoch: 4970/5000\n",
      "w1: 25.633010864257812, w2: -24.220962524414062, bias: 16.577756881713867, loss: 31.453972263923283\n",
      "Epoch: 4980/5000\n",
      "w1: 25.635231018066406, w2: -24.22309684753418, bias: 16.577241897583008, loss: 31.45387454743256\n",
      "Epoch: 4990/5000\n",
      "w1: 25.63743782043457, w2: -24.225208282470703, bias: 16.57672691345215, loss: 31.453778484630373\n",
      "Epoch: 5000/5000\n",
      "w1: 25.639631271362305, w2: -24.227296829223633, bias: 16.57621192932129, loss: 31.45368405425686\n",
      "##### 최종 w1, w2, bias #######\n",
      "25.639631271362305 -24.227296829223633 16.57621192932129\n"
     ]
    }
   ],
   "source": [
    "tr_features_ts = torch.from_numpy(tr_features)\n",
    "tr_targets_ts = torch.from_numpy(tr_target)\n",
    "\n",
    "w1, w2, bias = gradient_descent(tr_features_ts, tr_targets_ts, iter_epochs=5000, verbose=True)\n",
    "print('##### 최종 w1, w2, bias #######')\n",
    "print(w1.item(), w2.item(), bias.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>PREDICTED_PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.504311</td>\n",
       "      <td>0.546082</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.276481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727534</td>\n",
       "      <td>0.082781</td>\n",
       "      <td>31.5</td>\n",
       "      <td>33.224345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.442422</td>\n",
       "      <td>0.348786</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.469608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443380</td>\n",
       "      <td>0.197296</td>\n",
       "      <td>50.0</td>\n",
       "      <td>23.164367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.519640</td>\n",
       "      <td>0.139349</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26.523540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>20.1</td>\n",
       "      <td>22.200871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.575347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.049669</td>\n",
       "      <td>32.4</td>\n",
       "      <td>31.078924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.623683</td>\n",
       "      <td>0.061258</td>\n",
       "      <td>31.6</td>\n",
       "      <td>31.083084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.533940</td>\n",
       "      <td>10.9</td>\n",
       "      <td>18.299919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>21.7</td>\n",
       "      <td>16.174123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.550297</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>24.5</td>\n",
       "      <td>25.477825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.489366</td>\n",
       "      <td>0.212472</td>\n",
       "      <td>20.5</td>\n",
       "      <td>23.975738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.399885</td>\n",
       "      <td>0.341336</td>\n",
       "      <td>20.8</td>\n",
       "      <td>18.559479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.110558</td>\n",
       "      <td>0.596302</td>\n",
       "      <td>11.9</td>\n",
       "      <td>4.964072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.381347</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.444427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.972682</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.282834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.143488</td>\n",
       "      <td>36.1</td>\n",
       "      <td>31.154238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625216</td>\n",
       "      <td>0.579746</td>\n",
       "      <td>8.4</td>\n",
       "      <td>18.560827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.548764</td>\n",
       "      <td>0.284216</td>\n",
       "      <td>16.1</td>\n",
       "      <td>23.760528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RM     LSTAT  PRICE  PREDICTED_PRICE\n",
       "0   0.504311  0.546082   11.0        16.276481\n",
       "1   0.727534  0.082781   31.5        33.224345\n",
       "2   0.442422  0.348786   22.0        19.469608\n",
       "3   0.443380  0.197296   50.0        23.164367\n",
       "4   0.519640  0.139349   24.1        26.523540\n",
       "5   0.511401  0.309051   20.1        22.200871\n",
       "6   0.425752  0.450607   22.5        16.575347\n",
       "7   0.612569  0.049669   32.4        31.078924\n",
       "8   0.623683  0.061258   31.6        31.083084\n",
       "9   0.571757  0.533940   10.9        18.299919\n",
       "10  0.498755  0.544426   21.7        16.174123\n",
       "11  0.550297  0.214956   24.5        25.477825\n",
       "12  0.489366  0.212472   20.5        23.975738\n",
       "13  0.399885  0.341336   20.8        18.559479\n",
       "14  0.110558  0.596302   11.9         4.964072\n",
       "15  0.511209  0.381347   19.4        20.444427\n",
       "16  0.183560  0.972682    7.0        -2.282834\n",
       "17  0.704158  0.143488   36.1        31.154238\n",
       "18  0.625216  0.579746    8.4        18.560827\n",
       "19  0.548764  0.284216   16.1        23.760528"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features_ts = torch.from_numpy(test_features)\n",
    "test_predicted_ts = test_features_ts[:, 0]*w1 + test_features_ts[:, 1]*w2 + bias\n",
    "\n",
    "boston_test_df = pd.DataFrame({\n",
    "    'RM': test_features[:, 0],\n",
    "    'LSTAT': test_features[:, 1],\n",
    "    'PRICE': test_target,\n",
    "    'PREDICTED_PRICE': test_predicted_ts.cpu().numpy()\n",
    "})\n",
    "\n",
    "boston_test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.44255102479557\n"
     ]
    }
   ],
   "source": [
    "test_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE'])\n",
    "print(test_total_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent와 Mini Batch Gradient Descent 구현\n",
    "* SGD 는 전체 데이터에서 한건만 임의로 선택하여 Gradient Descent 로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용\n",
    "* Mini Batch GD는 전체 데이터에서 Batch 건수만큼 데이터를 선택하여 Gradient Descent로 Weight/Bias Update 계산한 뒤 Weight/Bias 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD 기반으로 Weight/Bias update 값 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# get_update_weights_value() 함수와 거의 유사. \n",
    "# 인자로 들어오는 rm_sgd, lstat_sgd, target_sgd은 단 1개의 원소를 가지는 tensor임. \n",
    "def get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, target_sgd, learning_rate=0.01):\n",
    "    # 데이터 건수\n",
    "    N = target_sgd.shape[0]\n",
    "    # 예측 값. \n",
    "    predicted_sgd = w1 * rm_sgd + w2 * lstat_sgd + bias\n",
    "    # 실제값과 예측값의 차이\n",
    "    diff_sgd = target_sgd - predicted_sgd \n",
    "    \n",
    "    # weight와 bias를 얼마나 update할 것인지를 계산.  \n",
    "    w1_update = -(2/N) * learning_rate * (torch.matmul(rm_sgd, diff_sgd))\n",
    "    w2_update = -(2/N) * learning_rate * (torch.matmul(lstat_sgd, diff_sgd))\n",
    "    bias_update = -(2/N) * learning_rate * torch.sum(diff_sgd)\n",
    "    \n",
    "    # weight와 bias가 update되어야 할 값 반환. \n",
    "    return bias_update, w1_update, w2_update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# RM, LSTAT feature tensor와 PRICE target tensor를 입력 받아서 iter_epochs수만큼 반복적으로 Weight와 Bias를 update적용. \n",
    "def st_gradient_descent(features, target, iter_epochs=1000, learning_rate=0.01, verbose=True):\n",
    "    # random seed 값 설정. \n",
    "    torch.manual_seed(2025)\n",
    "    # w1, w2는 1차원 tensor로 변환하되 초기 값은 0으로 설정\n",
    "    # bias도 1차원 tensor로 변환하되 초기 값은 1로 설정.\n",
    "    w1 = torch.zeros(1, dtype=torch.float32)\n",
    "    w2 = torch.zeros(1, dtype=torch.float32)\n",
    "    bias = torch.ones(1, dtype=torch.float32)\n",
    "    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n",
    "    \n",
    "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 tensor형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
    "    rm = features[:, 0]\n",
    "    lstat = features[:, 1]\n",
    "    \n",
    "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
    "    for i in range(1, iter_epochs+1):\n",
    "        # iteration 시마다 stochastic gradient descent 를 수행할 데이터를 한개만 추출. \n",
    "        #추출할 데이터의 인덱스를  로 선택. \n",
    "        stochastic_index = torch.randint(0, target.shape[0], size=(1,))\n",
    "        rm_sgd = rm[stochastic_index]\n",
    "        lstat_sgd = lstat[stochastic_index]\n",
    "        target_sgd = target[stochastic_index]\n",
    "        # weight/bias update 값 계산. loss 반환 없음. \n",
    "        bias_update, w1_update, w2_update = get_update_weights_value_sgd(bias, w1, w2, rm_sgd, lstat_sgd, \n",
    "                                                                     target_sgd, learning_rate=0.01)\n",
    "        # weight/bias의 update 적용.\n",
    "        w1 = w1 - w1_update\n",
    "        w2 = w2 - w2_update\n",
    "        bias = bias - bias_update\n",
    "        if verbose: # 100회 iteration 시마다 출력\n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch: {i}/{iter_epochs}')\n",
    "                # Loss는 전체 학습 데이터 기반으로 구해야 함. 아래는 전체 학습 feature 기반의 예측 및 loss임.  \n",
    "                predicted = w1 * rm + w2*lstat + bias\n",
    "                diff = target - predicted\n",
    "                loss = torch.mean(diff ** 2)\n",
    "                print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n",
    "        \n",
    "    return w1, w2, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_ftr_ts shape:torch.Size([354, 2]) tr_tgt_ts shape:torch.Size([354])\n",
      "test_ftr_ts shape:torch.Size([152, 2]) test_tgt_ts shape: torch.Size([152])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 학습과 테스트용 feature와 target 분리. \n",
    "def get_scaled_train_test_feature_target_ts(data_df):\n",
    "    # RM, LSTAT Feature에 Scaling 적용\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features_np = scaler.fit_transform(data_df[['RM', 'LSTAT']])\n",
    "    # 학습 feature, 테스트 feature, 학습 target, test_target으로 분리. \n",
    "    tr_features, test_features, tr_target, test_target = train_test_split(scaled_features_np, \n",
    "                                                                          data_df['PRICE'].values, \n",
    "                                                                          test_size=0.3, random_state=2025)\n",
    "    # 학습 feature와 target을 tensor로 변환. \n",
    "    tr_ftr_ts = torch.from_numpy(tr_features)\n",
    "    tr_tgt_ts = torch.from_numpy(tr_target)\n",
    "    test_ftr_ts = torch.from_numpy(test_features)\n",
    "    test_tgt_ts = torch.from_numpy(test_target)\n",
    "    \n",
    "    return tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts\n",
    "\n",
    "tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts(data_df=boston_df)\n",
    "\n",
    "print(f\"tr_ftr_ts shape:{tr_ftr_ts.shape} tr_tgt_ts shape:{tr_tgt_ts.shape}\")\n",
    "print(f\"test_ftr_ts shape:{test_ftr_ts.shape} test_tgt_ts shape: {test_tgt_ts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최초 w1, w2, bias: 0.0 0.0 1.0\n",
      "Epoch: 100/5000\n",
      "w1: 9.10193920135498, w2: 2.380099058151245, bias: 16.32122802734375, loss: 79.20577943326704\n",
      "Epoch: 200/5000\n",
      "w1: 11.38884162902832, w2: 0.6377108693122864, bias: 17.6075439453125, loss: 72.33473168920669\n",
      "Epoch: 300/5000\n",
      "w1: 11.43453311920166, w2: -1.9443591833114624, bias: 15.634873390197754, loss: 67.57221355061512\n",
      "Epoch: 400/5000\n",
      "w1: 13.544651985168457, w2: -3.477250814437866, bias: 16.884212493896484, loss: 59.43184224469005\n",
      "Epoch: 500/5000\n",
      "w1: 14.300559997558594, w2: -5.690975189208984, bias: 16.270288467407227, loss: 54.669107955151254\n",
      "Epoch: 600/5000\n",
      "w1: 15.722941398620605, w2: -7.305695056915283, bias: 16.813228607177734, loss: 50.19557187179603\n",
      "Epoch: 700/5000\n",
      "w1: 16.91908073425293, w2: -8.649571418762207, bias: 16.845500946044922, loss: 47.19992843074546\n",
      "Epoch: 800/5000\n",
      "w1: 17.07891273498535, w2: -10.086063385009766, bias: 16.271669387817383, loss: 45.02960586768405\n",
      "Epoch: 900/5000\n",
      "w1: 17.974334716796875, w2: -10.912721633911133, bias: 16.860803604125977, loss: 43.13543139188085\n",
      "Epoch: 1000/5000\n",
      "w1: 19.121231079101562, w2: -12.040255546569824, bias: 17.315248489379883, loss: 41.94179084124044\n",
      "Epoch: 1100/5000\n",
      "w1: 20.00320053100586, w2: -12.928214073181152, bias: 17.46938133239746, loss: 41.243879894381635\n",
      "Epoch: 1200/5000\n",
      "w1: 20.05638313293457, w2: -14.566816329956055, bias: 16.150402069091797, loss: 37.80478263287195\n",
      "Epoch: 1300/5000\n",
      "w1: 20.772939682006836, w2: -15.309930801391602, bias: 16.36229705810547, loss: 36.61034035781944\n",
      "Epoch: 1400/5000\n",
      "w1: 21.104564666748047, w2: -15.909236907958984, bias: 16.537328720092773, loss: 35.97110925138094\n",
      "Epoch: 1500/5000\n",
      "w1: 21.57444190979004, w2: -16.55977439880371, bias: 17.088714599609375, loss: 35.77495375347575\n",
      "Epoch: 1600/5000\n",
      "w1: 22.142423629760742, w2: -17.44516372680664, bias: 16.611204147338867, loss: 34.487227514314\n",
      "Epoch: 1700/5000\n",
      "w1: 22.47092056274414, w2: -18.099706649780273, bias: 16.778505325317383, loss: 34.061999716999175\n",
      "Epoch: 1800/5000\n",
      "w1: 22.78106689453125, w2: -18.82854652404785, bias: 16.28932762145996, loss: 33.4037884312053\n",
      "Epoch: 1900/5000\n",
      "w1: 23.08369255065918, w2: -18.89620590209961, bias: 16.6461181640625, loss: 33.390545273306934\n",
      "Epoch: 2000/5000\n",
      "w1: 24.051511764526367, w2: -19.0889835357666, bias: 17.2291202545166, loss: 34.84234860959225\n",
      "Epoch: 2100/5000\n",
      "w1: 23.762985229492188, w2: -19.80048942565918, bias: 16.149364471435547, loss: 32.66716410320566\n",
      "Epoch: 2200/5000\n",
      "w1: 24.598045349121094, w2: -20.556318283081055, bias: 15.910174369812012, loss: 32.21486544929298\n",
      "Epoch: 2300/5000\n",
      "w1: 24.60232162475586, w2: -20.54950523376465, bias: 15.71094036102295, loss: 32.2891678277514\n",
      "Epoch: 2400/5000\n",
      "w1: 24.818513870239258, w2: -20.363372802734375, bias: 16.16160774230957, loss: 32.3458161438385\n",
      "Epoch: 2500/5000\n",
      "w1: 24.697647094726562, w2: -20.653650283813477, bias: 15.435808181762695, loss: 32.4503059893132\n",
      "Epoch: 2600/5000\n",
      "w1: 25.4434814453125, w2: -20.53604507446289, bias: 16.401548385620117, loss: 32.79919474983877\n",
      "Epoch: 2700/5000\n",
      "w1: 25.788400650024414, w2: -21.01487159729004, bias: 16.360788345336914, loss: 32.606421312207004\n",
      "Epoch: 2800/5000\n",
      "w1: 25.21722984313965, w2: -20.946958541870117, bias: 15.393991470336914, loss: 32.14667826013999\n",
      "Epoch: 2900/5000\n",
      "w1: 25.390108108520508, w2: -21.061168670654297, bias: 15.430625915527344, loss: 32.025750437446575\n",
      "Epoch: 3000/5000\n",
      "w1: 25.172073364257812, w2: -21.104637145996094, bias: 14.925098419189453, loss: 32.8343641424038\n",
      "Epoch: 3100/5000\n",
      "w1: 25.386938095092773, w2: -21.047924041748047, bias: 15.322564125061035, loss: 32.10604528089874\n",
      "Epoch: 3200/5000\n",
      "w1: 25.70531463623047, w2: -20.95286750793457, bias: 15.742814064025879, loss: 31.96027078098366\n",
      "Epoch: 3300/5000\n",
      "w1: 25.53109359741211, w2: -20.67476463317871, bias: 15.849411964416504, loss: 32.11213460860367\n",
      "Epoch: 3400/5000\n",
      "w1: 25.728376388549805, w2: -21.227209091186523, bias: 15.922232627868652, loss: 31.942119637675113\n",
      "Epoch: 3500/5000\n",
      "w1: 25.518665313720703, w2: -22.177976608276367, bias: 14.984235763549805, loss: 32.7213100120021\n",
      "Epoch: 3600/5000\n",
      "w1: 25.772300720214844, w2: -22.36690902709961, bias: 15.414904594421387, loss: 31.88385087189684\n",
      "Epoch: 3700/5000\n",
      "w1: 26.16721534729004, w2: -22.69513511657715, bias: 15.926135063171387, loss: 31.549508619910775\n",
      "Epoch: 3800/5000\n",
      "w1: 26.23098373413086, w2: -22.985837936401367, bias: 15.591504096984863, loss: 31.592325404461306\n",
      "Epoch: 3900/5000\n",
      "w1: 26.651042938232422, w2: -22.549259185791016, bias: 16.273193359375, loss: 32.08472303866568\n",
      "Epoch: 4000/5000\n",
      "w1: 26.042081832885742, w2: -22.9172420501709, bias: 15.747186660766602, loss: 31.56816360109441\n",
      "Epoch: 4100/5000\n",
      "w1: 25.85407066345215, w2: -22.375118255615234, bias: 16.286142349243164, loss: 31.755787483232492\n",
      "Epoch: 4200/5000\n",
      "w1: 26.224689483642578, w2: -22.50276756286621, bias: 16.39626693725586, loss: 31.990426417269326\n",
      "Epoch: 4300/5000\n",
      "w1: 25.857444763183594, w2: -22.9141902923584, bias: 16.264211654663086, loss: 31.573671850259654\n",
      "Epoch: 4400/5000\n",
      "w1: 26.24478530883789, w2: -22.943714141845703, bias: 16.3443660736084, loss: 31.738931356081054\n",
      "Epoch: 4500/5000\n",
      "w1: 26.42640495300293, w2: -23.301311492919922, bias: 16.27486801147461, loss: 31.630737663742323\n",
      "Epoch: 4600/5000\n",
      "w1: 26.559219360351562, w2: -23.77950096130371, bias: 16.16883659362793, loss: 31.4964109866385\n",
      "Epoch: 4700/5000\n",
      "w1: 26.367219924926758, w2: -23.969533920288086, bias: 15.635705947875977, loss: 31.67656482317143\n",
      "Epoch: 4800/5000\n",
      "w1: 26.740177154541016, w2: -23.788829803466797, bias: 15.994451522827148, loss: 31.4679915938718\n",
      "Epoch: 4900/5000\n",
      "w1: 27.248546600341797, w2: -23.64810562133789, bias: 16.62917709350586, loss: 32.61397646568801\n",
      "Epoch: 5000/5000\n",
      "w1: 27.299787521362305, w2: -24.172941207885742, bias: 16.071279525756836, loss: 31.611807798171824\n",
      "##### 최종 w1, w2, bias #######\n",
      "tensor([27.2998]) tensor([-24.1729]) tensor([16.0713])\n"
     ]
    }
   ],
   "source": [
    "# 학습 feature와 target으로 Stochastic Gradient Descent 수행. \n",
    "w1, w2, bias = st_gradient_descent(tr_ftr_ts, tr_tgt_ts, iter_epochs=5000, verbose=True)\n",
    "print('##### 최종 w1, w2, bias #######')\n",
    "print(w1, w2, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>PREDICTED_PRICE_SGD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.504311</td>\n",
       "      <td>0.546082</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.638467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727534</td>\n",
       "      <td>0.082781</td>\n",
       "      <td>31.5</td>\n",
       "      <td>33.931732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.442422</td>\n",
       "      <td>0.348786</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.718124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443380</td>\n",
       "      <td>0.197296</td>\n",
       "      <td>50.0</td>\n",
       "      <td>23.406238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.519640</td>\n",
       "      <td>0.139349</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26.888865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>20.1</td>\n",
       "      <td>22.561742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.801722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.049669</td>\n",
       "      <td>32.4</td>\n",
       "      <td>31.593653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.623683</td>\n",
       "      <td>0.061258</td>\n",
       "      <td>31.6</td>\n",
       "      <td>31.616892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.533940</td>\n",
       "      <td>10.9</td>\n",
       "      <td>18.773215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>21.7</td>\n",
       "      <td>16.526794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.550297</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>24.5</td>\n",
       "      <td>25.898155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.489366</td>\n",
       "      <td>0.212472</td>\n",
       "      <td>20.5</td>\n",
       "      <td>24.294778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.399885</td>\n",
       "      <td>0.341336</td>\n",
       "      <td>20.8</td>\n",
       "      <td>18.736972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.110558</td>\n",
       "      <td>0.596302</td>\n",
       "      <td>11.9</td>\n",
       "      <td>4.675094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.381347</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.808909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.972682</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.430157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.143488</td>\n",
       "      <td>36.1</td>\n",
       "      <td>31.826117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625216</td>\n",
       "      <td>0.579746</td>\n",
       "      <td>8.4</td>\n",
       "      <td>19.125362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.548764</td>\n",
       "      <td>0.284216</td>\n",
       "      <td>16.1</td>\n",
       "      <td>24.182079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RM     LSTAT  PRICE  PREDICTED_PRICE_SGD\n",
       "0   0.504311  0.546082   11.0            16.638467\n",
       "1   0.727534  0.082781   31.5            33.931732\n",
       "2   0.442422  0.348786   22.0            19.718124\n",
       "3   0.443380  0.197296   50.0            23.406238\n",
       "4   0.519640  0.139349   24.1            26.888865\n",
       "5   0.511401  0.309051   20.1            22.561742\n",
       "6   0.425752  0.450607   22.5            16.801722\n",
       "7   0.612569  0.049669   32.4            31.593653\n",
       "8   0.623683  0.061258   31.6            31.616892\n",
       "9   0.571757  0.533940   10.9            18.773215\n",
       "10  0.498755  0.544426   21.7            16.526794\n",
       "11  0.550297  0.214956   24.5            25.898155\n",
       "12  0.489366  0.212472   20.5            24.294778\n",
       "13  0.399885  0.341336   20.8            18.736972\n",
       "14  0.110558  0.596302   11.9             4.675094\n",
       "15  0.511209  0.381347   19.4            20.808909\n",
       "16  0.183560  0.972682    7.0            -2.430157\n",
       "17  0.704158  0.143488   36.1            31.826117\n",
       "18  0.625216  0.579746    8.4            19.125362\n",
       "19  0.548764  0.284216   16.1            24.182079"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터에서 예측 수행 및 결과를 DataFrame으로 생성. \n",
    "test_predicted_ts = test_ftr_ts[:, 0]*w1 + test_ftr_ts[:, 1]*w2 + bias\n",
    "\n",
    "boston_test_df = pd.DataFrame({\n",
    "    'RM': test_features[:, 0],\n",
    "    'LSTAT': test_ftr_ts[:, 1],\n",
    "    'PRICE': test_tgt_ts,\n",
    "    'PREDICTED_PRICE_SGD': test_predicted_ts.cpu().numpy()\n",
    "})\n",
    "\n",
    "boston_test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.984664879873662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE_SGD'])\n",
    "print(test_total_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iteration시마다 일정한 batch 크기만큼의 데이터를 random하게 가져와서 GD를 수행하는 Mini-Batch GD 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_update_weights_value_batch(bias, w1, w2, rm_batch, lstat_batch, target_batch, learning_rate=0.01):\n",
    "    # 데이터 건수\n",
    "    N = target_batch.shape[0]\n",
    "    # 예측 값. \n",
    "    predicted_batch = w1 * rm_batch + w2 * lstat_batch + bias\n",
    "    # 실제값과 예측값의 차이\n",
    "    diff_batch = target_batch - predicted_batch \n",
    "    \n",
    "    # weight와 bias를 얼마나 update할 것인지를 계산.  \n",
    "    w1_update = -(2/N) * learning_rate * (torch.matmul(rm_batch, diff_batch))\n",
    "    w2_update = -(2/N) * learning_rate * (torch.matmul(lstat_batch, diff_batch))\n",
    "    bias_update = -(2/N) * learning_rate * torch.sum(diff_batch)\n",
    "    \n",
    "    # weight와 bias가 update되어야 할 값 반환. \n",
    "    return bias_update, w1_update, w2_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([213, 127,  30, 201, 134, 201,  72, 284, 128,  42, 247,  18, 118, 208,\n",
      "        276, 236, 271, 166,  12, 136, 290, 224,  88,  49,  22,  56,  25, 178,\n",
      "        202,   3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.5965, 0.6541, 0.4208, 0.4397, 0.4532, 0.4397, 0.6256, 0.7394, 0.6978,\n",
       "        0.3531, 0.6206, 0.5882, 0.5281, 0.6066, 0.3631, 0.4110, 0.4369, 0.4681,\n",
       "        0.7586, 0.5557, 0.3915, 0.4451, 0.6902, 0.4800, 0.4509, 0.5392, 0.2686,\n",
       "        0.4645, 0.2826, 0.4263], dtype=torch.float64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_indexes = torch.randint(0, 300, size=(30,))\n",
    "print(batch_indexes)\n",
    "\n",
    "tr_ftr_ts[batch_indexes, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \n",
    "def batch_random_gradient_descent(features, target, iter_epochs=5000, batch_size=30, verbose=True):\n",
    "    # random seed 값 설정. \n",
    "    torch.manual_seed(2025)\n",
    "    # w1, w2는 1차원 tensor로 변환하되 초기 값은 0으로 설정\n",
    "    # bias도 1차원 tensor로 변환하되 초기 값은 1로 설정.\n",
    "    w1 = torch.zeros(1, dtype=torch.float32)\n",
    "    w2 = torch.zeros(1, dtype=torch.float32)\n",
    "    bias = torch.ones(1, dtype=torch.float32)\n",
    "    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n",
    "    \n",
    "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 tensor 형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
    "    learning_rate = 0.01\n",
    "    rm = features[:, 0]\n",
    "    lstat = features[:, 1]\n",
    "    \n",
    "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
    "    for i in range(1, iter_epochs+1):\n",
    "        # batch_size 갯수만큼 데이터를 임의로 선택. \n",
    "        batch_indexes = torch.randint(0, target.shape[0], size=(batch_size,))\n",
    "        rm_batch = rm[batch_indexes]\n",
    "        lstat_batch = lstat[batch_indexes]\n",
    "        target_batch = target[batch_indexes]\n",
    "        # Batch GD 기반으로 Weight/Bias의 Update를 구함. \n",
    "        bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, \n",
    "                                                                           rm_batch, lstat_batch, \n",
    "                                                                           target_batch, learning_rate)\n",
    "        \n",
    "        # Batch GD로 구한 weight/bias의 update 적용. \n",
    "        w1 = w1 - w1_update\n",
    "        w2 = w2 - w2_update\n",
    "        bias = bias - bias_update\n",
    "        if verbose: # 100회 iteration 시마다 출력\n",
    "            if i % 100 == 0:\n",
    "                print(f'Epoch: {i}/{iter_epochs}')\n",
    "                # Loss는 전체 학습 데이터 기반으로 구해야 함. 아래는 전체 학습 feature 기반의 예측 및 loss임.  \n",
    "                predicted = w1 * rm + w2*lstat + bias\n",
    "                diff = target - predicted\n",
    "                loss = torch.mean(diff ** 2)\n",
    "                print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n",
    "        \n",
    "    return w1, w2, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최초 w1, w2, bias: 0.0 0.0 1.0\n",
      "Epoch: 100/5000\n",
      "w1: 9.532495498657227, w2: 1.938775897026062, bias: 15.881488800048828, loss: 78.11859371095791\n",
      "Epoch: 200/5000\n",
      "w1: 11.555778503417969, w2: -0.21187061071395874, bias: 16.998315811157227, loss: 68.93165545311753\n",
      "Epoch: 300/5000\n",
      "w1: 12.718533515930176, w2: -2.56358003616333, bias: 16.81089210510254, loss: 62.173577384915774\n",
      "Epoch: 400/5000\n",
      "w1: 13.90304183959961, w2: -4.585747241973877, bias: 16.800025939941406, loss: 56.79182986963833\n",
      "Epoch: 500/5000\n",
      "w1: 14.967144012451172, w2: -6.40493106842041, bias: 16.75732421875, loss: 52.39103174204248\n",
      "Epoch: 600/5000\n",
      "w1: 15.9356050491333, w2: -8.12226390838623, bias: 16.799663543701172, loss: 48.65957528994758\n",
      "Epoch: 700/5000\n",
      "w1: 16.72344398498535, w2: -9.702632904052734, bias: 16.679140090942383, loss: 45.64700262630224\n",
      "Epoch: 800/5000\n",
      "w1: 17.431224822998047, w2: -11.052210807800293, bias: 16.579404830932617, loss: 43.32511744180553\n",
      "Epoch: 900/5000\n",
      "w1: 18.30120849609375, w2: -12.218840599060059, bias: 16.790555953979492, loss: 41.19309302820594\n",
      "Epoch: 1000/5000\n",
      "w1: 18.896020889282227, w2: -13.377056121826172, bias: 16.724212646484375, loss: 39.51507512617034\n",
      "Epoch: 1100/5000\n",
      "w1: 19.497303009033203, w2: -14.332029342651367, bias: 16.762632369995117, loss: 38.17990703040178\n",
      "Epoch: 1200/5000\n",
      "w1: 20.02489471435547, w2: -15.361384391784668, bias: 16.616548538208008, loss: 36.97981449118922\n",
      "Epoch: 1300/5000\n",
      "w1: 20.65569496154785, w2: -16.16663932800293, bias: 16.70962905883789, loss: 35.959378076769646\n",
      "Epoch: 1400/5000\n",
      "w1: 21.1413516998291, w2: -16.908586502075195, bias: 16.729860305786133, loss: 35.180607353272876\n",
      "Epoch: 1500/5000\n",
      "w1: 21.553415298461914, w2: -17.55459976196289, bias: 16.74453353881836, loss: 34.5695058645609\n",
      "Epoch: 1600/5000\n",
      "w1: 21.698522567749023, w2: -18.186511993408203, bias: 16.61150550842285, loss: 34.14430463349321\n",
      "Epoch: 1700/5000\n",
      "w1: 22.15851593017578, w2: -18.657800674438477, bias: 16.823680877685547, loss: 33.692095627663555\n",
      "Epoch: 1800/5000\n",
      "w1: 22.52720832824707, w2: -19.241857528686523, bias: 16.762527465820312, loss: 33.25782926599931\n",
      "Epoch: 1900/5000\n",
      "w1: 22.855897903442383, w2: -19.710939407348633, bias: 16.727529525756836, loss: 32.939343067118514\n",
      "Epoch: 2000/5000\n",
      "w1: 23.066722869873047, w2: -20.182083129882812, bias: 16.678756713867188, loss: 32.675553559843735\n",
      "Epoch: 2100/5000\n",
      "w1: 23.253768920898438, w2: -20.498327255249023, bias: 16.64407730102539, loss: 32.50706745660395\n",
      "Epoch: 2200/5000\n",
      "w1: 23.58501625061035, w2: -20.8161678314209, bias: 16.72501564025879, loss: 32.330005054531966\n",
      "Epoch: 2300/5000\n",
      "w1: 23.83082389831543, w2: -21.120540618896484, bias: 16.78860092163086, loss: 32.21319391869349\n",
      "Epoch: 2400/5000\n",
      "w1: 23.94977378845215, w2: -21.4285945892334, bias: 16.633108139038086, loss: 32.05312445589569\n",
      "Epoch: 2500/5000\n",
      "w1: 24.06816291809082, w2: -21.6971378326416, bias: 16.649452209472656, loss: 31.95897951785007\n",
      "Epoch: 2600/5000\n",
      "w1: 24.331947326660156, w2: -22.057151794433594, bias: 16.63773536682129, loss: 31.83151185992353\n",
      "Epoch: 2700/5000\n",
      "w1: 24.382705688476562, w2: -22.33140754699707, bias: 16.420347213745117, loss: 31.818175098016845\n",
      "Epoch: 2800/5000\n",
      "w1: 24.668821334838867, w2: -22.400672912597656, bias: 16.683475494384766, loss: 31.739315101552524\n",
      "Epoch: 2900/5000\n",
      "w1: 24.742305755615234, w2: -22.561574935913086, bias: 16.646923065185547, loss: 31.688775652018908\n",
      "Epoch: 3000/5000\n",
      "w1: 24.79235076904297, w2: -22.73005485534668, bias: 16.508460998535156, loss: 31.64541580210269\n",
      "Epoch: 3100/5000\n",
      "w1: 24.866443634033203, w2: -22.746559143066406, bias: 16.579936981201172, loss: 31.635298840242978\n",
      "Epoch: 3200/5000\n",
      "w1: 25.01746940612793, w2: -22.97410011291504, bias: 16.53377342224121, loss: 31.585334301424556\n",
      "Epoch: 3300/5000\n",
      "w1: 25.048316955566406, w2: -23.127559661865234, bias: 16.59482765197754, loss: 31.56495389692983\n",
      "Epoch: 3400/5000\n",
      "w1: 25.10870933532715, w2: -23.22215461730957, bias: 16.50473976135254, loss: 31.54847951438084\n",
      "Epoch: 3500/5000\n",
      "w1: 25.232105255126953, w2: -23.319583892822266, bias: 16.570262908935547, loss: 31.531060791849153\n",
      "Epoch: 3600/5000\n",
      "w1: 25.319252014160156, w2: -23.397397994995117, bias: 16.599708557128906, loss: 31.52657141106204\n",
      "Epoch: 3700/5000\n",
      "w1: 25.34217643737793, w2: -23.519851684570312, bias: 16.500106811523438, loss: 31.502197430597036\n",
      "Epoch: 3800/5000\n",
      "w1: 25.406204223632812, w2: -23.71967887878418, bias: 16.45979118347168, loss: 31.490368260158625\n",
      "Epoch: 3900/5000\n",
      "w1: 25.473514556884766, w2: -23.76553726196289, bias: 16.53398895263672, loss: 31.477908854301642\n",
      "Epoch: 4000/5000\n",
      "w1: 25.5531063079834, w2: -23.864456176757812, bias: 16.5695858001709, loss: 31.472683506747565\n",
      "Epoch: 4100/5000\n",
      "w1: 25.572641372680664, w2: -23.975610733032227, bias: 16.544342041015625, loss: 31.46333456984218\n",
      "Epoch: 4200/5000\n",
      "w1: 25.5407772064209, w2: -24.059768676757812, bias: 16.405149459838867, loss: 31.489693659320668\n",
      "Epoch: 4300/5000\n",
      "w1: 25.717731475830078, w2: -24.045671463012695, bias: 16.52581024169922, loss: 31.45852336292435\n",
      "Epoch: 4400/5000\n",
      "w1: 25.670425415039062, w2: -24.158939361572266, bias: 16.418031692504883, loss: 31.46842270879547\n",
      "Epoch: 4500/5000\n",
      "w1: 25.674362182617188, w2: -24.164718627929688, bias: 16.491012573242188, loss: 31.456175607191526\n",
      "Epoch: 4600/5000\n",
      "w1: 25.817031860351562, w2: -24.170637130737305, bias: 16.583324432373047, loss: 31.46518883851858\n",
      "Epoch: 4700/5000\n",
      "w1: 25.874303817749023, w2: -24.193317413330078, bias: 16.65890884399414, loss: 31.497176078945756\n",
      "Epoch: 4800/5000\n",
      "w1: 25.717987060546875, w2: -24.23040771484375, bias: 16.460956573486328, loss: 31.45707963297639\n",
      "Epoch: 4900/5000\n",
      "w1: 25.86789894104004, w2: -24.19508171081543, bias: 16.586801528930664, loss: 31.469817588047118\n",
      "Epoch: 5000/5000\n",
      "w1: 25.834089279174805, w2: -24.221487045288086, bias: 16.4411678314209, loss: 31.450539146039315\n",
      "##### 최종 w1, w2, bias #######\n",
      "tensor([25.8341]) tensor([-24.2215]) tensor([16.4412])\n"
     ]
    }
   ],
   "source": [
    "tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts(data_df=boston_df)\n",
    "\n",
    "# 학습 feature와 target으로 Stochastic Gradient Descent 수행. \n",
    "w1, w2, bias = batch_random_gradient_descent(tr_ftr_ts, tr_tgt_ts, iter_epochs=5000, batch_size=30, verbose=True)\n",
    "print('##### 최종 w1, w2, bias #######')\n",
    "print(w1, w2, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 데이터 세트의 MSE: 28.42650203860407\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>PREDICTED_PRICE_RANDOM_BATCH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.504311</td>\n",
       "      <td>0.546082</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.242677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727534</td>\n",
       "      <td>0.082781</td>\n",
       "      <td>31.5</td>\n",
       "      <td>33.231256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.442422</td>\n",
       "      <td>0.348786</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.422623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443380</td>\n",
       "      <td>0.197296</td>\n",
       "      <td>50.0</td>\n",
       "      <td>23.116687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.519640</td>\n",
       "      <td>0.139349</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26.490353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>20.1</td>\n",
       "      <td>22.167069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.525711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.049669</td>\n",
       "      <td>32.4</td>\n",
       "      <td>31.063288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.623683</td>\n",
       "      <td>0.061258</td>\n",
       "      <td>31.6</td>\n",
       "      <td>31.069676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.533940</td>\n",
       "      <td>10.9</td>\n",
       "      <td>18.279160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>21.7</td>\n",
       "      <td>16.139229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.550297</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>24.5</td>\n",
       "      <td>25.451039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.489366</td>\n",
       "      <td>0.212472</td>\n",
       "      <td>20.5</td>\n",
       "      <td>23.937089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.399885</td>\n",
       "      <td>0.341336</td>\n",
       "      <td>20.8</td>\n",
       "      <td>18.504179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.110558</td>\n",
       "      <td>0.596302</td>\n",
       "      <td>11.9</td>\n",
       "      <td>4.853991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.381347</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.411007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.972682</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-2.376532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.143488</td>\n",
       "      <td>36.1</td>\n",
       "      <td>31.156956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625216</td>\n",
       "      <td>0.579746</td>\n",
       "      <td>8.4</td>\n",
       "      <td>18.550729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.548764</td>\n",
       "      <td>0.284216</td>\n",
       "      <td>16.1</td>\n",
       "      <td>23.733847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RM     LSTAT  PRICE  PREDICTED_PRICE_RANDOM_BATCH\n",
       "0   0.504311  0.546082   11.0                     16.242677\n",
       "1   0.727534  0.082781   31.5                     33.231256\n",
       "2   0.442422  0.348786   22.0                     19.422623\n",
       "3   0.443380  0.197296   50.0                     23.116687\n",
       "4   0.519640  0.139349   24.1                     26.490353\n",
       "5   0.511401  0.309051   20.1                     22.167069\n",
       "6   0.425752  0.450607   22.5                     16.525711\n",
       "7   0.612569  0.049669   32.4                     31.063288\n",
       "8   0.623683  0.061258   31.6                     31.069676\n",
       "9   0.571757  0.533940   10.9                     18.279160\n",
       "10  0.498755  0.544426   21.7                     16.139229\n",
       "11  0.550297  0.214956   24.5                     25.451039\n",
       "12  0.489366  0.212472   20.5                     23.937089\n",
       "13  0.399885  0.341336   20.8                     18.504179\n",
       "14  0.110558  0.596302   11.9                      4.853991\n",
       "15  0.511209  0.381347   19.4                     20.411007\n",
       "16  0.183560  0.972682    7.0                     -2.376532\n",
       "17  0.704158  0.143488   36.1                     31.156956\n",
       "18  0.625216  0.579746    8.4                     18.550729\n",
       "19  0.548764  0.284216   16.1                     23.733847"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터에서 예측 수행 및 결과를 DataFrame으로 생성. \n",
    "test_predicted_ts = test_ftr_ts[:, 0]*w1 + test_ftr_ts[:, 1]*w2 + bias\n",
    "\n",
    "boston_test_df = pd.DataFrame({\n",
    "    'RM': test_features[:, 0],\n",
    "    'LSTAT': test_ftr_ts[:, 1],\n",
    "    'PRICE': test_tgt_ts,\n",
    "    'PREDICTED_PRICE_RANDOM_BATCH': test_predicted_ts.cpu().numpy()\n",
    "})\n",
    "\n",
    "test_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE_RANDOM_BATCH'])\n",
    "print(\"test 데이터 세트의 MSE:\", test_total_mse)\n",
    "\n",
    "boston_test_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iteration 시에 순차적으로 일정한 batch 크기만큼의 데이터를 전체 학습데이터에 걸쳐서 가져오는 Mini-Batch GD 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "120\n",
      "150\n",
      "180\n",
      "210\n",
      "240\n",
      "270\n",
      "300\n",
      "330\n",
      "360\n",
      "390\n",
      "420\n",
      "450\n",
      "480\n"
     ]
    }
   ],
   "source": [
    "for batch_step in range(0, 506, 30):\n",
    "    print(batch_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# batch_gradient_descent()는 인자로 batch_size(배치 크기)를 입력 받음. \n",
    "def batch_gradient_descent(features, target, epochs=300, batch_size=30, verbose=True):\n",
    "    # random seed 값 설정. \n",
    "    torch.manual_seed(2025)\n",
    "    # w1, w2는 1차원 tensor로 변환하되 초기 값은 0으로 설정\n",
    "    # bias도 1차원 tensor로 변환하되 초기 값은 1로 설정.\n",
    "    w1 = torch.zeros(1, dtype=torch.float32)\n",
    "    w2 = torch.zeros(1, dtype=torch.float32)\n",
    "    bias = torch.ones(1, dtype=torch.float32)\n",
    "    print('최초 w1, w2, bias:', w1.item(), w2.item(), bias.item())\n",
    "    \n",
    "    # learning_rate와 RM, LSTAT 피처 지정. 호출 시 numpy array형태로 RM과 LSTAT으로 된 2차원 feature가 입력됨.\n",
    "    learning_rate = 0.01\n",
    "    rm = features[:, 0]\n",
    "    lstat = features[:, 1]\n",
    "    \n",
    "    # iter_epochs 수만큼 반복하면서 weight와 bias update 수행. \n",
    "    for i in range(1, epochs+1):\n",
    "        # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n",
    "        for batch_step in range(0, target.shape[0], batch_size):\n",
    "            # batch_size만큼 순차적인 데이터를 가져옴. \n",
    "            rm_batch = rm[batch_step:batch_step + batch_size]\n",
    "            lstat_batch = lstat[batch_step:batch_step + batch_size]\n",
    "            target_batch = target[batch_step:batch_step + batch_size]\n",
    "        \n",
    "            bias_update, w1_update, w2_update = get_update_weights_value_batch(bias, w1, w2, \n",
    "                                                                               rm_batch, lstat_batch, target_batch, \n",
    "                                                                               learning_rate)\n",
    "            # Batch GD로 구한 weight/bias의 update 적용. \n",
    "            w1 = w1 - w1_update\n",
    "            w2 = w2 - w2_update\n",
    "            bias = bias - bias_update\n",
    "        \n",
    "        if verbose:\n",
    "            print(f'Epoch: {i}/{epochs}')\n",
    "            # Loss는 전체 학습 데이터 기반으로 구해야 함. 아래는 전체 학습 feature 기반의 예측 및 loss임.  \n",
    "            predicted = w1 * rm + w2*lstat + bias\n",
    "            diff = target - predicted\n",
    "            loss = torch.mean(diff ** 2)\n",
    "            print(f'w1: {w1.item()}, w2: {w2.item()}, bias: {bias.item()}, loss: {loss.item()}')\n",
    "        \n",
    "    return w1, w2, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최초 w1, w2, bias: 0.0 0.0 1.0\n",
      "Epoch: 1/300\n",
      "w1: 2.548649311065674, w2: 1.0510945320129395, bias: 5.483735084533691, loss: 324.92663814145334\n",
      "Epoch: 2/300\n",
      "w1: 4.426955699920654, w2: 1.7139111757278442, bias: 8.697149276733398, loss: 206.7792344029648\n",
      "Epoch: 3/300\n",
      "w1: 5.824299335479736, w2: 2.0995075702667236, bias: 11.000228881835938, loss: 145.6548750690044\n",
      "Epoch: 4/300\n",
      "w1: 6.876403331756592, w2: 2.2874581813812256, bias: 12.650941848754883, loss: 113.7959164674643\n",
      "Epoch: 5/300\n",
      "w1: 7.680532455444336, w2: 2.3347744941711426, bias: 13.834155082702637, loss: 96.95631171099302\n",
      "Epoch: 6/300\n",
      "w1: 8.306395530700684, w2: 2.2823007106781006, bias: 14.682345390319824, loss: 87.82619786802941\n",
      "Epoch: 7/300\n",
      "w1: 8.803953170776367, w2: 2.1592955589294434, bias: 15.290448188781738, loss: 82.65587677962309\n",
      "Epoch: 8/300\n",
      "w1: 9.209012985229492, w2: 1.9867151975631714, bias: 15.726494789123535, loss: 79.52265426532097\n",
      "Epoch: 9/300\n",
      "w1: 9.547245025634766, w2: 1.7795655727386475, bias: 16.03923797607422, loss: 77.44113859278626\n",
      "Epoch: 10/300\n",
      "w1: 9.837047576904297, w2: 1.548589825630188, bias: 16.263614654541016, loss: 75.90665671964658\n",
      "Epoch: 11/300\n",
      "w1: 10.091611862182617, w2: 1.3014757633209229, bias: 16.424659729003906, loss: 74.66093404836337\n",
      "Epoch: 12/300\n",
      "w1: 10.32039737701416, w2: 1.0437220335006714, bias: 16.540319442749023, loss: 73.5719017461377\n",
      "Epoch: 13/300\n",
      "w1: 10.530186653137207, w2: 0.7792585492134094, bias: 16.623445510864258, loss: 72.57209131291452\n",
      "Epoch: 14/300\n",
      "w1: 10.725850105285645, w2: 0.5108909606933594, bias: 16.68325424194336, loss: 71.6270248557248\n",
      "Epoch: 15/300\n",
      "w1: 10.910879135131836, w2: 0.24062007665634155, bias: 16.72634506225586, loss: 70.71907466992612\n",
      "Epoch: 16/300\n",
      "w1: 11.087787628173828, w2: -0.030130643397569656, bias: 16.7574520111084, loss: 69.83916732318556\n",
      "Epoch: 17/300\n",
      "w1: 11.258377075195312, w2: -0.30035117268562317, bias: 16.77996826171875, loss: 68.98258518523681\n",
      "Epoch: 18/300\n",
      "w1: 11.423945426940918, w2: -0.5693276524543762, bias: 16.79631996154785, loss: 68.14680103112423\n",
      "Epoch: 19/300\n",
      "w1: 11.585432052612305, w2: -0.8365581631660461, bias: 16.808250427246094, loss: 67.33038325950857\n",
      "Epoch: 20/300\n",
      "w1: 11.743510246276855, w2: -1.1016926765441895, bias: 16.81700897216797, loss: 66.53245714934336\n",
      "Epoch: 21/300\n",
      "w1: 11.898670196533203, w2: -1.364490270614624, bias: 16.823490142822266, loss: 65.75241369049327\n",
      "Epoch: 22/300\n",
      "w1: 12.05126953125, w2: -1.624787449836731, bias: 16.828330993652344, loss: 64.9897758028967\n",
      "Epoch: 23/300\n",
      "w1: 12.201570510864258, w2: -1.8824763298034668, bias: 16.831993103027344, loss: 64.24413100474025\n",
      "Epoch: 24/300\n",
      "w1: 12.349761962890625, w2: -2.1374895572662354, bias: 16.834806442260742, loss: 63.515102723292024\n",
      "Epoch: 25/300\n",
      "w1: 12.495986938476562, w2: -2.389786958694458, bias: 16.837011337280273, loss: 62.8023295369792\n",
      "Epoch: 26/300\n",
      "w1: 12.640355110168457, w2: -2.6393489837646484, bias: 16.838768005371094, loss: 62.105455473727815\n",
      "Epoch: 27/300\n",
      "w1: 12.782946586608887, w2: -2.8861711025238037, bias: 16.84020233154297, loss: 61.424136136474246\n",
      "Epoch: 28/300\n",
      "w1: 12.923829078674316, w2: -3.1302590370178223, bias: 16.841398239135742, loss: 60.75802566345072\n",
      "Epoch: 29/300\n",
      "w1: 13.063050270080566, w2: -3.37162446975708, bias: 16.842422485351562, loss: 60.106795136622715\n",
      "Epoch: 30/300\n",
      "w1: 13.200653076171875, w2: -3.6102852821350098, bias: 16.843318939208984, loss: 59.4701133787079\n",
      "Epoch: 31/300\n",
      "w1: 13.336668968200684, w2: -3.8462624549865723, bias: 16.844120025634766, loss: 58.847661412323106\n",
      "Epoch: 32/300\n",
      "w1: 13.47113037109375, w2: -4.079579830169678, bias: 16.8448429107666, loss: 58.239118575951665\n",
      "Epoch: 33/300\n",
      "w1: 13.604061126708984, w2: -4.310262203216553, bias: 16.845510482788086, loss: 57.64417849669788\n",
      "Epoch: 34/300\n",
      "w1: 13.735486030578613, w2: -4.538336277008057, bias: 16.84613609313965, loss: 57.062535269564925\n",
      "Epoch: 35/300\n",
      "w1: 13.865424156188965, w2: -4.763829231262207, bias: 16.846723556518555, loss: 56.49389330656106\n",
      "Epoch: 36/300\n",
      "w1: 13.993897438049316, w2: -4.98676872253418, bias: 16.84728240966797, loss: 55.93795944398699\n",
      "Epoch: 37/300\n",
      "w1: 14.120925903320312, w2: -5.20718240737915, bias: 16.84781837463379, loss: 55.39444797001354\n",
      "Epoch: 38/300\n",
      "w1: 14.246524810791016, w2: -5.4250969886779785, bias: 16.848325729370117, loss: 54.86308440017537\n",
      "Epoch: 39/300\n",
      "w1: 14.370711326599121, w2: -5.6405415534973145, bias: 16.84881591796875, loss: 54.34359478131271\n",
      "Epoch: 40/300\n",
      "w1: 14.49350357055664, w2: -5.853541851043701, bias: 16.849287033081055, loss: 53.83571454026766\n",
      "Epoch: 41/300\n",
      "w1: 14.614916801452637, w2: -6.064126968383789, bias: 16.849746704101562, loss: 53.339181925736085\n",
      "Epoch: 42/300\n",
      "w1: 14.734968185424805, w2: -6.272323131561279, bias: 16.85018539428711, loss: 52.853742124275584\n",
      "Epoch: 43/300\n",
      "w1: 14.853671073913574, w2: -6.478156566619873, bias: 16.850610733032227, loss: 52.379150426411286\n",
      "Epoch: 44/300\n",
      "w1: 14.97104263305664, w2: -6.681654453277588, bias: 16.851022720336914, loss: 51.91516096838398\n",
      "Epoch: 45/300\n",
      "w1: 15.087099075317383, w2: -6.882843971252441, bias: 16.851421356201172, loss: 51.46153367925099\n",
      "Epoch: 46/300\n",
      "w1: 15.20185375213623, w2: -7.081750869750977, bias: 16.851804733276367, loss: 51.018038158357825\n",
      "Epoch: 47/300\n",
      "w1: 15.315322875976562, w2: -7.278400897979736, bias: 16.8521728515625, loss: 50.584446040549295\n",
      "Epoch: 48/300\n",
      "w1: 15.427520751953125, w2: -7.472818374633789, bias: 16.852529525756836, loss: 50.16053787959218\n",
      "Epoch: 49/300\n",
      "w1: 15.538460731506348, w2: -7.665029048919678, bias: 16.852872848510742, loss: 49.74609653435571\n",
      "Epoch: 50/300\n",
      "w1: 15.648159980773926, w2: -7.855058670043945, bias: 16.853206634521484, loss: 49.340906246466666\n",
      "Epoch: 51/300\n",
      "w1: 15.756628036499023, w2: -8.042930603027344, bias: 16.85352325439453, loss: 48.94476510219093\n",
      "Epoch: 52/300\n",
      "w1: 15.863882064819336, w2: -8.228670120239258, bias: 16.853832244873047, loss: 48.55746683808859\n",
      "Epoch: 53/300\n",
      "w1: 15.96993350982666, w2: -8.412301063537598, bias: 16.854127883911133, loss: 48.17881521199889\n",
      "Epoch: 54/300\n",
      "w1: 16.074796676635742, w2: -8.593847274780273, bias: 16.854413986206055, loss: 47.8086161468939\n",
      "Epoch: 55/300\n",
      "w1: 16.178483963012695, w2: -8.773333549499512, bias: 16.854686737060547, loss: 47.44667891569247\n",
      "Epoch: 56/300\n",
      "w1: 16.281009674072266, w2: -8.950782775878906, bias: 16.854948043823242, loss: 47.0928183327649\n",
      "Epoch: 57/300\n",
      "w1: 16.382389068603516, w2: -9.126214981079102, bias: 16.855194091796875, loss: 46.74685589142862\n",
      "Epoch: 58/300\n",
      "w1: 16.482635498046875, w2: -9.299657821655273, bias: 16.85542869567871, loss: 46.40860742211338\n",
      "Epoch: 59/300\n",
      "w1: 16.581756591796875, w2: -9.471131324768066, bias: 16.85565757751465, loss: 46.07790839045995\n",
      "Epoch: 60/300\n",
      "w1: 16.67976951599121, w2: -9.640658378601074, bias: 16.855871200561523, loss: 45.75458446959982\n",
      "Epoch: 61/300\n",
      "w1: 16.776687622070312, w2: -9.808259963989258, bias: 16.856075286865234, loss: 45.43847175837149\n",
      "Epoch: 62/300\n",
      "w1: 16.87251853942871, w2: -9.973959922790527, bias: 16.856266021728516, loss: 45.12940917675758\n",
      "Epoch: 63/300\n",
      "w1: 16.967275619506836, w2: -10.137777328491211, bias: 16.85645294189453, loss: 44.82724213157652\n",
      "Epoch: 64/300\n",
      "w1: 17.060976028442383, w2: -10.29973316192627, bias: 16.856624603271484, loss: 44.531811876687755\n",
      "Epoch: 65/300\n",
      "w1: 17.153627395629883, w2: -10.459851264953613, bias: 16.856788635253906, loss: 44.24296753639514\n",
      "Epoch: 66/300\n",
      "w1: 17.245243072509766, w2: -10.618151664733887, bias: 16.856943130493164, loss: 43.96056141736629\n",
      "Epoch: 67/300\n",
      "w1: 17.335830688476562, w2: -10.774653434753418, bias: 16.857088088989258, loss: 43.684454308821444\n",
      "Epoch: 68/300\n",
      "w1: 17.42540740966797, w2: -10.929380416870117, bias: 16.857219696044922, loss: 43.414496176395474\n",
      "Epoch: 69/300\n",
      "w1: 17.51398468017578, w2: -11.08234691619873, bias: 16.857341766357422, loss: 43.15055723676509\n",
      "Epoch: 70/300\n",
      "w1: 17.601566314697266, w2: -11.2335786819458, bias: 16.857454299926758, loss: 42.89250038115004\n",
      "Epoch: 71/300\n",
      "w1: 17.688173294067383, w2: -11.383090019226074, bias: 16.857559204101562, loss: 42.64019406444556\n",
      "Epoch: 72/300\n",
      "w1: 17.773807525634766, w2: -11.530904769897461, bias: 16.857656478881836, loss: 42.393510260368515\n",
      "Epoch: 73/300\n",
      "w1: 17.858488082885742, w2: -11.677040100097656, bias: 16.857742309570312, loss: 42.15231981714098\n",
      "Epoch: 74/300\n",
      "w1: 17.942218780517578, w2: -11.821517944335938, bias: 16.857816696166992, loss: 41.91650084798235\n",
      "Epoch: 75/300\n",
      "w1: 18.025014877319336, w2: -11.964353561401367, bias: 16.85788345336914, loss: 41.68593466031366\n",
      "Epoch: 76/300\n",
      "w1: 18.106887817382812, w2: -12.105567932128906, bias: 16.857940673828125, loss: 41.460500041153686\n",
      "Epoch: 77/300\n",
      "w1: 18.18784523010254, w2: -12.245176315307617, bias: 16.857994079589844, loss: 41.24008811460075\n",
      "Epoch: 78/300\n",
      "w1: 18.26789665222168, w2: -12.383198738098145, bias: 16.858036041259766, loss: 41.02458417203076\n",
      "Epoch: 79/300\n",
      "w1: 18.347055435180664, w2: -12.519654273986816, bias: 16.85806655883789, loss: 40.813875048196415\n",
      "Epoch: 80/300\n",
      "w1: 18.425331115722656, w2: -12.654557228088379, bias: 16.858091354370117, loss: 40.607858201159864\n",
      "Epoch: 81/300\n",
      "w1: 18.502731323242188, w2: -12.787928581237793, bias: 16.858112335205078, loss: 40.406426283792854\n",
      "Epoch: 82/300\n",
      "w1: 18.579265594482422, w2: -12.919784545898438, bias: 16.858121871948242, loss: 40.20947763768259\n",
      "Epoch: 83/300\n",
      "w1: 18.65494728088379, w2: -13.050142288208008, bias: 16.858121871948242, loss: 40.01690987134708\n",
      "Epoch: 84/300\n",
      "w1: 18.729782104492188, w2: -13.179017066955566, bias: 16.858112335205078, loss: 39.82862920798283\n",
      "Epoch: 85/300\n",
      "w1: 18.803781509399414, w2: -13.306428909301758, bias: 16.858097076416016, loss: 39.644535277662776\n",
      "Epoch: 86/300\n",
      "w1: 18.876956939697266, w2: -13.432391166687012, bias: 16.858070373535156, loss: 39.46453632627726\n",
      "Epoch: 87/300\n",
      "w1: 18.949316024780273, w2: -13.556922912597656, bias: 16.8580379486084, loss: 39.288539261048804\n",
      "Epoch: 88/300\n",
      "w1: 19.0208683013916, w2: -13.680038452148438, bias: 16.857999801635742, loss: 39.116456401072284\n",
      "Epoch: 89/300\n",
      "w1: 19.09161949157715, w2: -13.801755905151367, bias: 16.85795021057129, loss: 38.94819951526458\n",
      "Epoch: 90/300\n",
      "w1: 19.161582946777344, w2: -13.922090530395508, bias: 16.85789680480957, loss: 38.78368172672798\n",
      "Epoch: 91/300\n",
      "w1: 19.230770111083984, w2: -14.041057586669922, bias: 16.85783576965332, loss: 38.62281816638957\n",
      "Epoch: 92/300\n",
      "w1: 19.299184799194336, w2: -14.158669471740723, bias: 16.85776710510254, loss: 38.465532998795624\n",
      "Epoch: 93/300\n",
      "w1: 19.366836547851562, w2: -14.274946212768555, bias: 16.857688903808594, loss: 38.31174086814142\n",
      "Epoch: 94/300\n",
      "w1: 19.43372917175293, w2: -14.389901161193848, bias: 16.857608795166016, loss: 38.161368552845865\n",
      "Epoch: 95/300\n",
      "w1: 19.4998779296875, w2: -14.503548622131348, bias: 16.857521057128906, loss: 38.01433623543923\n",
      "Epoch: 96/300\n",
      "w1: 19.565290451049805, w2: -14.615903854370117, bias: 16.857425689697266, loss: 37.8705692109181\n",
      "Epoch: 97/300\n",
      "w1: 19.629974365234375, w2: -14.726981163024902, bias: 16.857324600219727, loss: 37.729995284426906\n",
      "Epoch: 98/300\n",
      "w1: 19.693937301635742, w2: -14.836796760559082, bias: 16.857215881347656, loss: 37.5925415950645\n",
      "Epoch: 99/300\n",
      "w1: 19.75718879699707, w2: -14.945364952087402, bias: 16.857101440429688, loss: 37.45813782218721\n",
      "Epoch: 100/300\n",
      "w1: 19.819730758666992, w2: -15.052696228027344, bias: 16.856979370117188, loss: 37.32672213748886\n",
      "Epoch: 101/300\n",
      "w1: 19.88157844543457, w2: -15.158809661865234, bias: 16.856847763061523, loss: 37.198219291263506\n",
      "Epoch: 102/300\n",
      "w1: 19.942737579345703, w2: -15.263714790344238, bias: 16.85671043395996, loss: 37.07256940954945\n",
      "Epoch: 103/300\n",
      "w1: 20.003215789794922, w2: -15.367424964904785, bias: 16.856571197509766, loss: 36.94970933413601\n",
      "Epoch: 104/300\n",
      "w1: 20.063024520874023, w2: -15.46995735168457, bias: 16.85642433166504, loss: 36.82957128871753\n",
      "Epoch: 105/300\n",
      "w1: 20.12216567993164, w2: -15.57132339477539, bias: 16.85626983642578, loss: 36.71209886323102\n",
      "Epoch: 106/300\n",
      "w1: 20.180648803710938, w2: -15.671536445617676, bias: 16.856109619140625, loss: 36.59723154574861\n",
      "Epoch: 107/300\n",
      "w1: 20.238479614257812, w2: -15.770609855651855, bias: 16.855947494506836, loss: 36.48491193396707\n",
      "Epoch: 108/300\n",
      "w1: 20.29566764831543, w2: -15.868555068969727, bias: 16.855775833129883, loss: 36.3750828443045\n",
      "Epoch: 109/300\n",
      "w1: 20.352218627929688, w2: -15.965386390686035, bias: 16.85559844970703, loss: 36.26768829693978\n",
      "Epoch: 110/300\n",
      "w1: 20.40814208984375, w2: -16.061113357543945, bias: 16.855419158935547, loss: 36.16267566941129\n",
      "Epoch: 111/300\n",
      "w1: 20.463443756103516, w2: -16.15575408935547, bias: 16.85523223876953, loss: 36.05998735855757\n",
      "Epoch: 112/300\n",
      "w1: 20.51812744140625, w2: -16.24931526184082, bias: 16.855037689208984, loss: 35.959577478375216\n",
      "Epoch: 113/300\n",
      "w1: 20.57220458984375, w2: -16.34181785583496, bias: 16.854841232299805, loss: 35.861386491415224\n",
      "Epoch: 114/300\n",
      "w1: 20.625680923461914, w2: -16.433263778686523, bias: 16.854639053344727, loss: 35.76537320924731\n",
      "Epoch: 115/300\n",
      "w1: 20.678560256958008, w2: -16.523672103881836, bias: 16.854429244995117, loss: 35.671484501760105\n",
      "Epoch: 116/300\n",
      "w1: 20.73085594177246, w2: -16.613046646118164, bias: 16.854217529296875, loss: 35.57967673116419\n",
      "Epoch: 117/300\n",
      "w1: 20.782567977905273, w2: -16.701404571533203, bias: 16.85399627685547, loss: 35.48990144815517\n",
      "Epoch: 118/300\n",
      "w1: 20.83370590209961, w2: -16.78875732421875, bias: 16.853771209716797, loss: 35.40211248718913\n",
      "Epoch: 119/300\n",
      "w1: 20.884275436401367, w2: -16.875118255615234, bias: 16.853540420532227, loss: 35.3162642378686\n",
      "Epoch: 120/300\n",
      "w1: 20.93429183959961, w2: -16.960493087768555, bias: 16.85330581665039, loss: 35.23231410590175\n",
      "Epoch: 121/300\n",
      "w1: 20.983745574951172, w2: -17.044897079467773, bias: 16.85306739807129, loss: 35.15022300876471\n",
      "Epoch: 122/300\n",
      "w1: 21.032655715942383, w2: -17.128339767456055, bias: 16.852821350097656, loss: 35.069945256522125\n",
      "Epoch: 123/300\n",
      "w1: 21.081016540527344, w2: -17.210834503173828, bias: 16.85257339477539, loss: 34.99144361969015\n",
      "Epoch: 124/300\n",
      "w1: 21.128847122192383, w2: -17.292388916015625, bias: 16.852319717407227, loss: 34.9146754954683\n",
      "Epoch: 125/300\n",
      "w1: 21.1761417388916, w2: -17.37301254272461, bias: 16.852062225341797, loss: 34.83960759425355\n",
      "Epoch: 126/300\n",
      "w1: 21.22291374206543, w2: -17.45271873474121, bias: 16.85179901123047, loss: 34.76619711037602\n",
      "Epoch: 127/300\n",
      "w1: 21.269168853759766, w2: -17.531518936157227, bias: 16.851531982421875, loss: 34.694406570386704\n",
      "Epoch: 128/300\n",
      "w1: 21.31490707397461, w2: -17.609420776367188, bias: 16.85126304626465, loss: 34.62420394335712\n",
      "Epoch: 129/300\n",
      "w1: 21.360139846801758, w2: -17.68643569946289, bias: 16.85098648071289, loss: 34.55555053592399\n",
      "Epoch: 130/300\n",
      "w1: 21.404869079589844, w2: -17.762575149536133, bias: 16.8507080078125, loss: 34.4884126135374\n",
      "Epoch: 131/300\n",
      "w1: 21.449106216430664, w2: -17.83784294128418, bias: 16.850427627563477, loss: 34.422758387329374\n",
      "Epoch: 132/300\n",
      "w1: 21.492855072021484, w2: -17.912256240844727, bias: 16.85013771057129, loss: 34.358550133322396\n",
      "Epoch: 133/300\n",
      "w1: 21.536113739013672, w2: -17.98581886291504, bias: 16.8498477935791, loss: 34.29576262255064\n",
      "Epoch: 134/300\n",
      "w1: 21.578895568847656, w2: -18.058549880981445, bias: 16.84955406188965, loss: 34.23435518017485\n",
      "Epoch: 135/300\n",
      "w1: 21.621204376220703, w2: -18.130447387695312, bias: 16.849254608154297, loss: 34.17430472453891\n",
      "Epoch: 136/300\n",
      "w1: 21.663042068481445, w2: -18.201526641845703, bias: 16.848949432373047, loss: 34.115578478620655\n",
      "Epoch: 137/300\n",
      "w1: 21.70442008972168, w2: -18.271793365478516, bias: 16.84864616394043, loss: 34.05814756943586\n",
      "Epoch: 138/300\n",
      "w1: 21.745342254638672, w2: -18.341262817382812, bias: 16.84833526611328, loss: 34.00197930407412\n",
      "Epoch: 139/300\n",
      "w1: 21.785808563232422, w2: -18.409936904907227, bias: 16.848020553588867, loss: 33.947051824854306\n",
      "Epoch: 140/300\n",
      "w1: 21.825830459594727, w2: -18.47783088684082, bias: 16.84770393371582, loss: 33.893331796391074\n",
      "Epoch: 141/300\n",
      "w1: 21.865407943725586, w2: -18.544954299926758, bias: 16.847381591796875, loss: 33.84079333160132\n",
      "Epoch: 142/300\n",
      "w1: 21.90454864501953, w2: -18.611305236816406, bias: 16.847055435180664, loss: 33.7894156743766\n",
      "Epoch: 143/300\n",
      "w1: 21.94325828552246, w2: -18.676902770996094, bias: 16.846725463867188, loss: 33.739166677942976\n",
      "Epoch: 144/300\n",
      "w1: 21.981534957885742, w2: -18.741750717163086, bias: 16.846389770507812, loss: 33.6900259283809\n",
      "Epoch: 145/300\n",
      "w1: 22.01938819885254, w2: -18.805862426757812, bias: 16.84605598449707, loss: 33.64196500569081\n",
      "Epoch: 146/300\n",
      "w1: 22.056827545166016, w2: -18.86924171447754, bias: 16.845718383789062, loss: 33.594960686517744\n",
      "Epoch: 147/300\n",
      "w1: 22.093852996826172, w2: -18.931900024414062, bias: 16.84537696838379, loss: 33.548989111306376\n",
      "Epoch: 148/300\n",
      "w1: 22.130468368530273, w2: -18.993839263916016, bias: 16.84503173828125, loss: 33.504031006999966\n",
      "Epoch: 149/300\n",
      "w1: 22.16667938232422, w2: -19.05507469177246, bias: 16.844680786132812, loss: 33.460059478024675\n",
      "Epoch: 150/300\n",
      "w1: 22.20249366760254, w2: -19.115615844726562, bias: 16.844329833984375, loss: 33.41705123537661\n",
      "Epoch: 151/300\n",
      "w1: 22.237911224365234, w2: -19.175460815429688, bias: 16.843975067138672, loss: 33.37499115555812\n",
      "Epoch: 152/300\n",
      "w1: 22.27293586730957, w2: -19.23462677001953, bias: 16.843616485595703, loss: 33.33385339246648\n",
      "Epoch: 153/300\n",
      "w1: 22.307580947875977, w2: -19.293113708496094, bias: 16.8432559967041, loss: 33.29361911190465\n",
      "Epoch: 154/300\n",
      "w1: 22.341838836669922, w2: -19.350934982299805, bias: 16.842891693115234, loss: 33.25426846575724\n",
      "Epoch: 155/300\n",
      "w1: 22.37571907043457, w2: -19.408098220825195, bias: 16.8425235748291, loss: 33.21578035617637\n",
      "Epoch: 156/300\n",
      "w1: 22.40922737121582, w2: -19.464603424072266, bias: 16.842153549194336, loss: 33.17813910796834\n",
      "Epoch: 157/300\n",
      "w1: 22.442367553710938, w2: -19.52046775817871, bias: 16.841785430908203, loss: 33.14132134372176\n",
      "Epoch: 158/300\n",
      "w1: 22.475141525268555, w2: -19.57569694519043, bias: 16.841407775878906, loss: 33.10530949104688\n",
      "Epoch: 159/300\n",
      "w1: 22.50755500793457, w2: -19.630294799804688, bias: 16.841026306152344, loss: 33.07008723424718\n",
      "Epoch: 160/300\n",
      "w1: 22.53961181640625, w2: -19.684267044067383, bias: 16.840646743774414, loss: 33.03563811981118\n",
      "Epoch: 161/300\n",
      "w1: 22.57131576538086, w2: -19.737625122070312, bias: 16.84026336669922, loss: 33.001942598349814\n",
      "Epoch: 162/300\n",
      "w1: 22.60266876220703, w2: -19.79037094116211, bias: 16.83987808227539, loss: 32.96898697216579\n",
      "Epoch: 163/300\n",
      "w1: 22.633676528930664, w2: -19.84251594543457, bias: 16.83949089050293, loss: 32.93675223236347\n",
      "Epoch: 164/300\n",
      "w1: 22.66434669494629, w2: -19.89406394958496, bias: 16.839101791381836, loss: 32.90522285965767\n",
      "Epoch: 165/300\n",
      "w1: 22.694677352905273, w2: -19.945024490356445, bias: 16.838706970214844, loss: 32.874383097542434\n",
      "Epoch: 166/300\n",
      "w1: 22.724674224853516, w2: -19.995399475097656, bias: 16.83831024169922, loss: 32.844219557879455\n",
      "Epoch: 167/300\n",
      "w1: 22.754343032836914, w2: -20.045202255249023, bias: 16.837909698486328, loss: 32.81471362170165\n",
      "Epoch: 168/300\n",
      "w1: 22.7836856842041, w2: -20.094432830810547, bias: 16.837509155273438, loss: 32.785854251723016\n",
      "Epoch: 169/300\n",
      "w1: 22.812702178955078, w2: -20.14310646057129, bias: 16.837106704711914, loss: 32.757623973649\n",
      "Epoch: 170/300\n",
      "w1: 22.84140396118164, w2: -20.19122314453125, bias: 16.836702346801758, loss: 32.73000982603587\n",
      "Epoch: 171/300\n",
      "w1: 22.869792938232422, w2: -20.238788604736328, bias: 16.8362979888916, loss: 32.702998895472234\n",
      "Epoch: 172/300\n",
      "w1: 22.897869110107422, w2: -20.285810470581055, bias: 16.835887908935547, loss: 32.67657768381835\n",
      "Epoch: 173/300\n",
      "w1: 22.92563247680664, w2: -20.33229637145996, bias: 16.83547592163086, loss: 32.65073349328594\n",
      "Epoch: 174/300\n",
      "w1: 22.953096389770508, w2: -20.378250122070312, bias: 16.835063934326172, loss: 32.625452422459226\n",
      "Epoch: 175/300\n",
      "w1: 22.980257034301758, w2: -20.42367935180664, bias: 16.83464813232422, loss: 32.600722695282\n",
      "Epoch: 176/300\n",
      "w1: 23.00712013244629, w2: -20.46858787536621, bias: 16.834232330322266, loss: 32.57653268404769\n",
      "Epoch: 177/300\n",
      "w1: 23.03369140625, w2: -20.512985229492188, bias: 16.833812713623047, loss: 32.55286824190575\n",
      "Epoch: 178/300\n",
      "w1: 23.059965133666992, w2: -20.556875228881836, bias: 16.83339500427246, loss: 32.529720761936666\n",
      "Epoch: 179/300\n",
      "w1: 23.085956573486328, w2: -20.600257873535156, bias: 16.832971572875977, loss: 32.50707814120929\n",
      "Epoch: 180/300\n",
      "w1: 23.111658096313477, w2: -20.64314842224121, bias: 16.832550048828125, loss: 32.484928118225575\n",
      "Epoch: 181/300\n",
      "w1: 23.137081146240234, w2: -20.685546875, bias: 16.832122802734375, loss: 32.463259866540035\n",
      "Epoch: 182/300\n",
      "w1: 23.16222381591797, w2: -20.727458953857422, bias: 16.83169174194336, loss: 32.44206378596545\n",
      "Epoch: 183/300\n",
      "w1: 23.18709945678711, w2: -20.768890380859375, bias: 16.831260681152344, loss: 32.42132752390696\n",
      "Epoch: 184/300\n",
      "w1: 23.211698532104492, w2: -20.80984878540039, bias: 16.83082389831543, loss: 32.40104225627138\n",
      "Epoch: 185/300\n",
      "w1: 23.236026763916016, w2: -20.850337982177734, bias: 16.830394744873047, loss: 32.3811988133209\n",
      "Epoch: 186/300\n",
      "w1: 23.260089874267578, w2: -20.890363693237305, bias: 16.829957962036133, loss: 32.36178590347404\n",
      "Epoch: 187/300\n",
      "w1: 23.283893585205078, w2: -20.929931640625, bias: 16.829519271850586, loss: 32.34279363817036\n",
      "Epoch: 188/300\n",
      "w1: 23.307437896728516, w2: -20.96904754638672, bias: 16.82908058166504, loss: 32.32421320024153\n",
      "Epoch: 189/300\n",
      "w1: 23.330720901489258, w2: -21.00771713256836, bias: 16.82863998413086, loss: 32.306036075179925\n",
      "Epoch: 190/300\n",
      "w1: 23.353748321533203, w2: -21.045942306518555, bias: 16.828197479248047, loss: 32.288253943424046\n",
      "Epoch: 191/300\n",
      "w1: 23.376529693603516, w2: -21.08373260498047, bias: 16.8277530670166, loss: 32.270855288257074\n",
      "Epoch: 192/300\n",
      "w1: 23.399059295654297, w2: -21.12108612060547, bias: 16.827308654785156, loss: 32.25383566513801\n",
      "Epoch: 193/300\n",
      "w1: 23.42134666442871, w2: -21.15801239013672, bias: 16.826862335205078, loss: 32.23718380736923\n",
      "Epoch: 194/300\n",
      "w1: 23.443387985229492, w2: -21.194517135620117, bias: 16.826414108276367, loss: 32.22089246444246\n",
      "Epoch: 195/300\n",
      "w1: 23.465190887451172, w2: -21.230602264404297, bias: 16.825963973999023, loss: 32.2049537920422\n",
      "Epoch: 196/300\n",
      "w1: 23.486753463745117, w2: -21.266273498535156, bias: 16.825511932373047, loss: 32.189360473675855\n",
      "Epoch: 197/300\n",
      "w1: 23.508085250854492, w2: -21.301538467407227, bias: 16.825057983398438, loss: 32.17410275614958\n",
      "Epoch: 198/300\n",
      "w1: 23.52918243408203, w2: -21.336397171020508, bias: 16.82460594177246, loss: 32.15917604593553\n",
      "Epoch: 199/300\n",
      "w1: 23.550050735473633, w2: -21.37085723876953, bias: 16.82415199279785, loss: 32.14457133339549\n",
      "Epoch: 200/300\n",
      "w1: 23.570693969726562, w2: -21.40492057800293, bias: 16.823694229125977, loss: 32.13028204678063\n",
      "Epoch: 201/300\n",
      "w1: 23.591110229492188, w2: -21.438594818115234, bias: 16.8232364654541, loss: 32.11630117549835\n",
      "Epoch: 202/300\n",
      "w1: 23.611309051513672, w2: -21.47187614440918, bias: 16.82277488708496, loss: 32.10262314827236\n",
      "Epoch: 203/300\n",
      "w1: 23.631284713745117, w2: -21.504783630371094, bias: 16.822315216064453, loss: 32.0892383815188\n",
      "Epoch: 204/300\n",
      "w1: 23.651042938232422, w2: -21.537315368652344, bias: 16.821855545043945, loss: 32.07614171487942\n",
      "Epoch: 205/300\n",
      "w1: 23.67058753967285, w2: -21.569469451904297, bias: 16.82139015197754, loss: 32.06332814520466\n",
      "Epoch: 206/300\n",
      "w1: 23.689922332763672, w2: -21.601253509521484, bias: 16.8209285736084, loss: 32.05079071866172\n",
      "Epoch: 207/300\n",
      "w1: 23.709047317504883, w2: -21.632678985595703, bias: 16.820466995239258, loss: 32.03852137411087\n",
      "Epoch: 208/300\n",
      "w1: 23.727962493896484, w2: -21.663742065429688, bias: 16.81999969482422, loss: 32.026516593095565\n",
      "Epoch: 209/300\n",
      "w1: 23.746673583984375, w2: -21.69444465637207, bias: 16.819534301757812, loss: 32.01477108541043\n",
      "Epoch: 210/300\n",
      "w1: 23.765182495117188, w2: -21.72480010986328, bias: 16.819068908691406, loss: 32.00327643345499\n",
      "Epoch: 211/300\n",
      "w1: 23.783491134643555, w2: -21.754806518554688, bias: 16.81859588623047, loss: 31.99202844784442\n",
      "Epoch: 212/300\n",
      "w1: 23.801599502563477, w2: -21.784461975097656, bias: 16.818124771118164, loss: 31.98102418415381\n",
      "Epoch: 213/300\n",
      "w1: 23.81951332092285, w2: -21.813779830932617, bias: 16.817657470703125, loss: 31.970255255056326\n",
      "Epoch: 214/300\n",
      "w1: 23.83723258972168, w2: -21.842763900756836, bias: 16.81718635559082, loss: 31.959716413143607\n",
      "Epoch: 215/300\n",
      "w1: 23.854761123657227, w2: -21.871414184570312, bias: 16.816713333129883, loss: 31.949403521744642\n",
      "Epoch: 216/300\n",
      "w1: 23.87209701538086, w2: -21.899736404418945, bias: 16.816240310668945, loss: 31.939311755036886\n",
      "Epoch: 217/300\n",
      "w1: 23.889244079589844, w2: -21.9277286529541, bias: 16.815765380859375, loss: 31.92943742874689\n",
      "Epoch: 218/300\n",
      "w1: 23.906211853027344, w2: -21.955402374267578, bias: 16.815290451049805, loss: 31.919772813266576\n",
      "Epoch: 219/300\n",
      "w1: 23.92299461364746, w2: -21.982757568359375, bias: 16.814815521240234, loss: 31.910315315802514\n",
      "Epoch: 220/300\n",
      "w1: 23.93959617614746, w2: -22.009798049926758, bias: 16.8143367767334, loss: 31.901059900136378\n",
      "Epoch: 221/300\n",
      "w1: 23.95602035522461, w2: -22.03652572631836, bias: 16.813859939575195, loss: 31.89200263195469\n",
      "Epoch: 222/300\n",
      "w1: 23.97226905822754, w2: -22.062944412231445, bias: 16.813379287719727, loss: 31.883138801490453\n",
      "Epoch: 223/300\n",
      "w1: 23.98834228515625, w2: -22.089061737060547, bias: 16.81290054321289, loss: 31.874463685113092\n",
      "Epoch: 224/300\n",
      "w1: 24.004241943359375, w2: -22.11487579345703, bias: 16.812423706054688, loss: 31.86597449359427\n",
      "Epoch: 225/300\n",
      "w1: 24.01996612548828, w2: -22.140396118164062, bias: 16.811941146850586, loss: 31.857665713984165\n",
      "Epoch: 226/300\n",
      "w1: 24.0355224609375, w2: -22.16562271118164, bias: 16.811458587646484, loss: 31.849533814840726\n",
      "Epoch: 227/300\n",
      "w1: 24.050912857055664, w2: -22.19055938720703, bias: 16.810976028442383, loss: 31.841574808994803\n",
      "Epoch: 228/300\n",
      "w1: 24.066139221191406, w2: -22.215211868286133, bias: 16.81049346923828, loss: 31.83378433252646\n",
      "Epoch: 229/300\n",
      "w1: 24.081201553344727, w2: -22.23957633972168, bias: 16.810012817382812, loss: 31.826160701978335\n",
      "Epoch: 230/300\n",
      "w1: 24.09610366821289, w2: -22.263660430908203, bias: 16.809526443481445, loss: 31.818698590390305\n",
      "Epoch: 231/300\n",
      "w1: 24.110843658447266, w2: -22.28746795654297, bias: 16.809040069580078, loss: 31.81139505576298\n",
      "Epoch: 232/300\n",
      "w1: 24.125425338745117, w2: -22.311002731323242, bias: 16.808557510375977, loss: 31.804246447737018\n",
      "Epoch: 233/300\n",
      "w1: 24.139846801757812, w2: -22.334266662597656, bias: 16.808073043823242, loss: 31.797249778346348\n",
      "Epoch: 234/300\n",
      "w1: 24.154117584228516, w2: -22.35725975036621, bias: 16.80759048461914, loss: 31.79040173966285\n",
      "Epoch: 235/300\n",
      "w1: 24.168235778808594, w2: -22.379987716674805, bias: 16.80710220336914, loss: 31.783698493740566\n",
      "Epoch: 236/300\n",
      "w1: 24.182201385498047, w2: -22.402454376220703, bias: 16.80661392211914, loss: 31.77713718890911\n",
      "Epoch: 237/300\n",
      "w1: 24.19601821899414, w2: -22.424663543701172, bias: 16.806121826171875, loss: 31.77071411071314\n",
      "Epoch: 238/300\n",
      "w1: 24.20969009399414, w2: -22.446613311767578, bias: 16.805635452270508, loss: 31.764427565813833\n",
      "Epoch: 239/300\n",
      "w1: 24.22321319580078, w2: -22.468311309814453, bias: 16.805150985717773, loss: 31.75827395225741\n",
      "Epoch: 240/300\n",
      "w1: 24.23659324645996, w2: -22.489757537841797, bias: 16.804658889770508, loss: 31.752250049537633\n",
      "Epoch: 241/300\n",
      "w1: 24.249826431274414, w2: -22.510957717895508, bias: 16.804168701171875, loss: 31.746353488060937\n",
      "Epoch: 242/300\n",
      "w1: 24.262922286987305, w2: -22.53191566467285, bias: 16.803680419921875, loss: 31.740580463732492\n",
      "Epoch: 243/300\n",
      "w1: 24.275876998901367, w2: -22.552631378173828, bias: 16.803190231323242, loss: 31.734929221222806\n",
      "Epoch: 244/300\n",
      "w1: 24.288692474365234, w2: -22.57310676574707, bias: 16.80270004272461, loss: 31.729397342852685\n",
      "Epoch: 245/300\n",
      "w1: 24.301372528076172, w2: -22.593341827392578, bias: 16.802207946777344, loss: 31.72398245092487\n",
      "Epoch: 246/300\n",
      "w1: 24.313915252685547, w2: -22.61334800720215, bias: 16.801715850830078, loss: 31.718680680275646\n",
      "Epoch: 247/300\n",
      "w1: 24.32632827758789, w2: -22.633121490478516, bias: 16.801225662231445, loss: 31.713490360216774\n",
      "Epoch: 248/300\n",
      "w1: 24.338605880737305, w2: -22.652666091918945, bias: 16.800735473632812, loss: 31.70840943769577\n",
      "Epoch: 249/300\n",
      "w1: 24.350757598876953, w2: -22.671985626220703, bias: 16.800241470336914, loss: 31.703434322928523\n",
      "Epoch: 250/300\n",
      "w1: 24.36277961730957, w2: -22.691083908081055, bias: 16.79974937438965, loss: 31.69856327823768\n",
      "Epoch: 251/300\n",
      "w1: 24.374675750732422, w2: -22.709957122802734, bias: 16.799259185791016, loss: 31.693795039107222\n",
      "Epoch: 252/300\n",
      "w1: 24.38644027709961, w2: -22.728614807128906, bias: 16.798765182495117, loss: 31.689126407936033\n",
      "Epoch: 253/300\n",
      "w1: 24.39808464050293, w2: -22.747053146362305, bias: 16.79827308654785, loss: 31.684555775630496\n",
      "Epoch: 254/300\n",
      "w1: 24.409603118896484, w2: -22.765283584594727, bias: 16.79777717590332, loss: 31.68007969826721\n",
      "Epoch: 255/300\n",
      "w1: 24.420997619628906, w2: -22.783300399780273, bias: 16.79728126525879, loss: 31.675697775527876\n",
      "Epoch: 256/300\n",
      "w1: 24.432273864746094, w2: -22.801105499267578, bias: 16.796789169311523, loss: 31.671407862843633\n",
      "Epoch: 257/300\n",
      "w1: 24.443431854248047, w2: -22.818708419799805, bias: 16.796293258666992, loss: 31.667206399150988\n",
      "Epoch: 258/300\n",
      "w1: 24.4544677734375, w2: -22.83610725402832, bias: 16.795793533325195, loss: 31.663092678221535\n",
      "Epoch: 259/300\n",
      "w1: 24.465391159057617, w2: -22.853307723999023, bias: 16.795297622680664, loss: 31.659063879891193\n",
      "Epoch: 260/300\n",
      "w1: 24.476200103759766, w2: -22.87030601501465, bias: 16.794803619384766, loss: 31.655119423130714\n",
      "Epoch: 261/300\n",
      "w1: 24.486894607543945, w2: -22.88710594177246, bias: 16.794309616088867, loss: 31.651257224175733\n",
      "Epoch: 262/300\n",
      "w1: 24.49747657775879, w2: -22.90371322631836, bias: 16.793813705444336, loss: 31.647474752537114\n",
      "Epoch: 263/300\n",
      "w1: 24.50794219970703, w2: -22.92012596130371, bias: 16.793317794799805, loss: 31.643771505188045\n",
      "Epoch: 264/300\n",
      "w1: 24.518301010131836, w2: -22.936351776123047, bias: 16.79282569885254, loss: 31.64014436798726\n",
      "Epoch: 265/300\n",
      "w1: 24.52855110168457, w2: -22.9523868560791, bias: 16.79233169555664, loss: 31.636592666638975\n",
      "Epoch: 266/300\n",
      "w1: 24.5386905670166, w2: -22.96823501586914, bias: 16.791833877563477, loss: 31.63311464207121\n",
      "Epoch: 267/300\n",
      "w1: 24.548723220825195, w2: -22.983898162841797, bias: 16.791336059570312, loss: 31.629708723384596\n",
      "Epoch: 268/300\n",
      "w1: 24.558652877807617, w2: -22.99938201904297, bias: 16.79084014892578, loss: 31.62637265759133\n",
      "Epoch: 269/300\n",
      "w1: 24.5684757232666, w2: -23.01468849182129, bias: 16.79034423828125, loss: 31.623105258554844\n",
      "Epoch: 270/300\n",
      "w1: 24.578195571899414, w2: -23.029813766479492, bias: 16.78985023498535, loss: 31.61990587994704\n",
      "Epoch: 271/300\n",
      "w1: 24.587814331054688, w2: -23.04476547241211, bias: 16.789356231689453, loss: 31.616772013033703\n",
      "Epoch: 272/300\n",
      "w1: 24.597332000732422, w2: -23.059545516967773, bias: 16.788860321044922, loss: 31.61370225215595\n",
      "Epoch: 273/300\n",
      "w1: 24.606752395629883, w2: -23.07415008544922, bias: 16.788362503051758, loss: 31.610695920256596\n",
      "Epoch: 274/300\n",
      "w1: 24.616071701049805, w2: -23.088586807250977, bias: 16.787864685058594, loss: 31.607751233562457\n",
      "Epoch: 275/300\n",
      "w1: 24.62529754638672, w2: -23.102853775024414, bias: 16.787368774414062, loss: 31.604867079375012\n",
      "Epoch: 276/300\n",
      "w1: 24.634424209594727, w2: -23.116958618164062, bias: 16.786874771118164, loss: 31.60204178229464\n",
      "Epoch: 277/300\n",
      "w1: 24.643455505371094, w2: -23.130895614624023, bias: 16.786378860473633, loss: 31.599274834340033\n",
      "Epoch: 278/300\n",
      "w1: 24.652393341064453, w2: -23.144672393798828, bias: 16.7858829498291, loss: 31.596564160648974\n",
      "Epoch: 279/300\n",
      "w1: 24.661237716674805, w2: -23.158287048339844, bias: 16.785388946533203, loss: 31.593909279587585\n",
      "Epoch: 280/300\n",
      "w1: 24.66999053955078, w2: -23.171743392944336, bias: 16.784893035888672, loss: 31.591308451068233\n",
      "Epoch: 281/300\n",
      "w1: 24.678651809692383, w2: -23.185041427612305, bias: 16.78439712524414, loss: 31.588760943783807\n",
      "Epoch: 282/300\n",
      "w1: 24.68722152709961, w2: -23.198184967041016, bias: 16.783899307250977, loss: 31.586265294201944\n",
      "Epoch: 283/300\n",
      "w1: 24.69569969177246, w2: -23.211177825927734, bias: 16.783401489257812, loss: 31.58382026226085\n",
      "Epoch: 284/300\n",
      "w1: 24.70409393310547, w2: -23.224021911621094, bias: 16.782907485961914, loss: 31.581424501355986\n",
      "Epoch: 285/300\n",
      "w1: 24.712400436401367, w2: -23.23671531677246, bias: 16.782411575317383, loss: 31.579077493910816\n",
      "Epoch: 286/300\n",
      "w1: 24.72062110900879, w2: -23.249258041381836, bias: 16.78192138671875, loss: 31.57677875893537\n",
      "Epoch: 287/300\n",
      "w1: 24.7287540435791, w2: -23.261655807495117, bias: 16.781429290771484, loss: 31.574526510443825\n",
      "Epoch: 288/300\n",
      "w1: 24.736804962158203, w2: -23.273908615112305, bias: 16.78093719482422, loss: 31.57231977721627\n",
      "Epoch: 289/300\n",
      "w1: 24.744773864746094, w2: -23.28601837158203, bias: 16.780441284179688, loss: 31.57015741936608\n",
      "Epoch: 290/300\n",
      "w1: 24.752660751342773, w2: -23.297985076904297, bias: 16.779945373535156, loss: 31.56803895607767\n",
      "Epoch: 291/300\n",
      "w1: 24.76046371459961, w2: -23.309812545776367, bias: 16.779449462890625, loss: 31.565963331140104\n",
      "Epoch: 292/300\n",
      "w1: 24.768184661865234, w2: -23.321502685546875, bias: 16.778953552246094, loss: 31.563929529514798\n",
      "Epoch: 293/300\n",
      "w1: 24.77582550048828, w2: -23.333053588867188, bias: 16.778461456298828, loss: 31.56193724677682\n",
      "Epoch: 294/300\n",
      "w1: 24.783390045166016, w2: -23.344470977783203, bias: 16.77796745300293, loss: 31.559984602275062\n",
      "Epoch: 295/300\n",
      "w1: 24.790882110595703, w2: -23.355756759643555, bias: 16.77747344970703, loss: 31.558070632037502\n",
      "Epoch: 296/300\n",
      "w1: 24.79829216003418, w2: -23.36690902709961, bias: 16.776979446411133, loss: 31.556195553515174\n",
      "Epoch: 297/300\n",
      "w1: 24.805627822875977, w2: -23.377931594848633, bias: 16.776485443115234, loss: 31.554357860273477\n",
      "Epoch: 298/300\n",
      "w1: 24.812889099121094, w2: -23.388826370239258, bias: 16.775991439819336, loss: 31.55255675596747\n",
      "Epoch: 299/300\n",
      "w1: 24.820072174072266, w2: -23.399595260620117, bias: 16.775497436523438, loss: 31.550791671618192\n",
      "Epoch: 300/300\n",
      "w1: 24.82718276977539, w2: -23.410232543945312, bias: 16.775005340576172, loss: 31.54906254796088\n",
      "##### 최종 w1, w2, bias #######\n",
      "tensor([24.8272]) tensor([-23.4102]) tensor([16.7750])\n"
     ]
    }
   ],
   "source": [
    "tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts(data_df=boston_df)\n",
    "\n",
    "# 학습 feature와 target으로 Mini Batch Gradient Descent 수행. \n",
    "w1, w2, bias = batch_gradient_descent(tr_ftr_ts, tr_tgt_ts, epochs=300, batch_size=30, verbose=True)\n",
    "print('##### 최종 w1, w2, bias #######')\n",
    "print(w1, w2, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test 데이터 세트의 MSE: 28.330330879942494\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>PREDICTED_PRICE_BATCH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.504311</td>\n",
       "      <td>0.546082</td>\n",
       "      <td>11.0</td>\n",
       "      <td>16.511732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727534</td>\n",
       "      <td>0.082781</td>\n",
       "      <td>31.5</td>\n",
       "      <td>32.899692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.442422</td>\n",
       "      <td>0.348786</td>\n",
       "      <td>22.0</td>\n",
       "      <td>19.593937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443380</td>\n",
       "      <td>0.197296</td>\n",
       "      <td>50.0</td>\n",
       "      <td>23.164140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.519640</td>\n",
       "      <td>0.139349</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26.414010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>20.1</td>\n",
       "      <td>22.236692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>22.5</td>\n",
       "      <td>16.796413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.049669</td>\n",
       "      <td>32.4</td>\n",
       "      <td>30.820619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.623683</td>\n",
       "      <td>0.061258</td>\n",
       "      <td>31.6</td>\n",
       "      <td>30.825219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.533940</td>\n",
       "      <td>10.9</td>\n",
       "      <td>18.470453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>21.7</td>\n",
       "      <td>16.412535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.550297</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>24.5</td>\n",
       "      <td>25.405163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.489366</td>\n",
       "      <td>0.212472</td>\n",
       "      <td>20.5</td>\n",
       "      <td>23.950551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.399885</td>\n",
       "      <td>0.341336</td>\n",
       "      <td>20.8</td>\n",
       "      <td>18.712280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.110558</td>\n",
       "      <td>0.596302</td>\n",
       "      <td>11.9</td>\n",
       "      <td>5.560260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.381347</td>\n",
       "      <td>19.4</td>\n",
       "      <td>20.539474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.972682</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-1.438430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.143488</td>\n",
       "      <td>36.1</td>\n",
       "      <td>30.898178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625216</td>\n",
       "      <td>0.579746</td>\n",
       "      <td>8.4</td>\n",
       "      <td>18.725354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.548764</td>\n",
       "      <td>0.284216</td>\n",
       "      <td>16.1</td>\n",
       "      <td>23.745702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RM     LSTAT  PRICE  PREDICTED_PRICE_BATCH\n",
       "0   0.504311  0.546082   11.0              16.511732\n",
       "1   0.727534  0.082781   31.5              32.899692\n",
       "2   0.442422  0.348786   22.0              19.593937\n",
       "3   0.443380  0.197296   50.0              23.164140\n",
       "4   0.519640  0.139349   24.1              26.414010\n",
       "5   0.511401  0.309051   20.1              22.236692\n",
       "6   0.425752  0.450607   22.5              16.796413\n",
       "7   0.612569  0.049669   32.4              30.820619\n",
       "8   0.623683  0.061258   31.6              30.825219\n",
       "9   0.571757  0.533940   10.9              18.470453\n",
       "10  0.498755  0.544426   21.7              16.412535\n",
       "11  0.550297  0.214956   24.5              25.405163\n",
       "12  0.489366  0.212472   20.5              23.950551\n",
       "13  0.399885  0.341336   20.8              18.712280\n",
       "14  0.110558  0.596302   11.9               5.560260\n",
       "15  0.511209  0.381347   19.4              20.539474\n",
       "16  0.183560  0.972682    7.0              -1.438430\n",
       "17  0.704158  0.143488   36.1              30.898178\n",
       "18  0.625216  0.579746    8.4              18.725354\n",
       "19  0.548764  0.284216   16.1              23.745702"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 테스트 데이터에서 예측 수행 및 결과를 DataFrame으로 생성. \n",
    "test_predicted_ts = test_ftr_ts[:, 0]*w1 + test_ftr_ts[:, 1]*w2 + bias\n",
    "\n",
    "boston_test_df = pd.DataFrame({\n",
    "    'RM': test_features[:, 0],\n",
    "    'LSTAT': test_ftr_ts[:, 1],\n",
    "    'PRICE': test_tgt_ts,\n",
    "    'PREDICTED_PRICE_BATCH': test_predicted_ts.cpu().numpy()\n",
    "})\n",
    "\n",
    "test_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED_PRICE_BATCH'])\n",
    "print(\"test 데이터 세트의 MSE:\", test_total_mse)\n",
    "\n",
    "boston_test_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pytorch를 이용하여 Simple Regression 모델 구축하기\n",
    "* Pytorch 모델은 torch.nn.Module 클래스를 상속하여 생성함\n",
    "* nn.Parameter()는 학습 파라미터(Learnable Parameter) tensor를 생성\n",
    "* Pytorch의 train 로직은 model의 출력값(feed forward)을 오차 역전파(Backpropagation)로 weight 값 update 수행\n",
    "* 손실 함수는 nn.MSELoss()로, Optimizer는 Adam 생성하고 모델 학습 수행."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([0., 0.], requires_grad=True), Parameter containing:\n",
      "tensor([1.], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRegression_01(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #w\n",
    "        self.weights = nn.Parameter(data=torch.zeros(size=(2, ), dtype=torch.float32),\n",
    "                                   requires_grad=True)\n",
    "        self.bias = nn.Parameter(data=torch.ones(size=(1,), dtype=torch.float32))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.matmul(self.weights, x.t()) + self.bias # w1*x1 + w2*x2 + b\n",
    "\n",
    "simple_model_01 = SimpleRegression_01()\n",
    "# simple_model의 학습 파라미터 출력(Learnable Parameter)\n",
    "print(list(simple_model_01.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_scaled_train_test_feature_target_ts_01(data_df):\n",
    "    # RM, LSTAT Feature에 Scaling 적용\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_features_np = scaler.fit_transform(data_df[['RM', 'LSTAT']])\n",
    "    # 학습 feature, 테스트 feature, 학습 target, test_target으로 분리. \n",
    "    tr_features, test_features, tr_target, test_target = train_test_split(scaled_features_np, \n",
    "                                                                          data_df['PRICE'].values, \n",
    "                                                                          test_size=0.3, random_state=2025)\n",
    "    # 학습 feature와 target을 tensor로 변환. dtype=torch.float32로 수정\n",
    "    tr_ftr_ts = torch.tensor(tr_features, dtype=torch.float32)\n",
    "    tr_tgt_ts = torch.tensor(tr_target, dtype=torch.float32)\n",
    "    test_ftr_ts = torch.tensor(test_features, dtype=torch.float32)\n",
    "    test_tgt_ts = torch.tensor(test_target, dtype=torch.float32)\n",
    "    \n",
    "    return tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32 torch.float32 torch.float32\n"
     ]
    }
   ],
   "source": [
    "tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts_01(data_df=boston_df)\n",
    "print(tr_ftr_ts.dtype, tr_tgt_ts.dtype, test_ftr_ts.dtype, test_tgt_ts.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# MSELoss 생성. \n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "# optimizer는 Adam으로, 생성 시 인자로 model의 모든 parameter 값과 learning rate가 필요. \n",
    "optimizer = torch.optim.Adam(simple_model_01.parameters(), lr=0.01)\n",
    "\n",
    "# train loop 수행. \n",
    "def train_loop(model, tr_ftr_ts, tr_tgt_ts, loss_fn, optimizer, epochs=300, batch_size=30, verbose=True):\n",
    "    #model.train()\n",
    "    for i in range(1, epochs+1):\n",
    "    # batch_size 만큼 데이터를 가져와서 weight/bias update를 수행하는 로직을 전체 데이터 건수만큼 반복\n",
    "        for batch_step in range(0, tr_tgt_ts.shape[0], batch_size):\n",
    "            # batch_size만큼 순차적인 데이터를 가져옴.\n",
    "            ftr_batch = tr_ftr_ts[batch_step:batch_step + batch_size]\n",
    "            target_batch = tr_tgt_ts[batch_step:batch_step + batch_size]\n",
    "            \n",
    "            # forward pass\n",
    "            output = model(ftr_batch).squeeze(-1)\n",
    "            \n",
    "            # mse loss 계산\n",
    "            loss = loss_fn(output, target_batch)\n",
    "\n",
    "            # backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose:\n",
    "                if batch_step == 330:\n",
    "                    print(f'Epoch: {i}/{epochs}, batch step:{batch_step}, loss: {loss.item()}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300, batch step:330, loss: 515.7574462890625\n",
      "Epoch: 2/300, batch step:330, loss: 506.6460876464844\n",
      "Epoch: 3/300, batch step:330, loss: 497.666259765625\n",
      "Epoch: 4/300, batch step:330, loss: 488.8221740722656\n",
      "Epoch: 5/300, batch step:330, loss: 480.1151123046875\n",
      "Epoch: 6/300, batch step:330, loss: 471.54443359375\n",
      "Epoch: 7/300, batch step:330, loss: 463.1086730957031\n",
      "Epoch: 8/300, batch step:330, loss: 454.8063049316406\n",
      "Epoch: 9/300, batch step:330, loss: 446.6358642578125\n",
      "Epoch: 10/300, batch step:330, loss: 438.5956115722656\n",
      "Epoch: 11/300, batch step:330, loss: 430.6841125488281\n",
      "Epoch: 12/300, batch step:330, loss: 422.8999938964844\n",
      "Epoch: 13/300, batch step:330, loss: 415.24169921875\n",
      "Epoch: 14/300, batch step:330, loss: 407.7079772949219\n",
      "Epoch: 15/300, batch step:330, loss: 400.29736328125\n",
      "Epoch: 16/300, batch step:330, loss: 393.0084533691406\n",
      "Epoch: 17/300, batch step:330, loss: 385.8399963378906\n",
      "Epoch: 18/300, batch step:330, loss: 378.79052734375\n",
      "Epoch: 19/300, batch step:330, loss: 371.8587951660156\n",
      "Epoch: 20/300, batch step:330, loss: 365.04345703125\n",
      "Epoch: 21/300, batch step:330, loss: 358.3432312011719\n",
      "Epoch: 22/300, batch step:330, loss: 351.7566833496094\n",
      "Epoch: 23/300, batch step:330, loss: 345.2826843261719\n",
      "Epoch: 24/300, batch step:330, loss: 338.91986083984375\n",
      "Epoch: 25/300, batch step:330, loss: 332.6669006347656\n",
      "Epoch: 26/300, batch step:330, loss: 326.5225830078125\n",
      "Epoch: 27/300, batch step:330, loss: 320.4855041503906\n",
      "Epoch: 28/300, batch step:330, loss: 314.5545349121094\n",
      "Epoch: 29/300, batch step:330, loss: 308.7283935546875\n",
      "Epoch: 30/300, batch step:330, loss: 303.0057373046875\n",
      "Epoch: 31/300, batch step:330, loss: 297.3853759765625\n",
      "Epoch: 32/300, batch step:330, loss: 291.8661193847656\n",
      "Epoch: 33/300, batch step:330, loss: 286.44659423828125\n",
      "Epoch: 34/300, batch step:330, loss: 281.12567138671875\n",
      "Epoch: 35/300, batch step:330, loss: 275.9020690917969\n",
      "Epoch: 36/300, batch step:330, loss: 270.7745666503906\n",
      "Epoch: 37/300, batch step:330, loss: 265.74200439453125\n",
      "Epoch: 38/300, batch step:330, loss: 260.8031311035156\n",
      "Epoch: 39/300, batch step:330, loss: 255.9567108154297\n",
      "Epoch: 40/300, batch step:330, loss: 251.2015380859375\n",
      "Epoch: 41/300, batch step:330, loss: 246.53643798828125\n",
      "Epoch: 42/300, batch step:330, loss: 241.9602508544922\n",
      "Epoch: 43/300, batch step:330, loss: 237.47174072265625\n",
      "Epoch: 44/300, batch step:330, loss: 233.06971740722656\n",
      "Epoch: 45/300, batch step:330, loss: 228.75311279296875\n",
      "Epoch: 46/300, batch step:330, loss: 224.52064514160156\n",
      "Epoch: 47/300, batch step:330, loss: 220.3712158203125\n",
      "Epoch: 48/300, batch step:330, loss: 216.3036346435547\n",
      "Epoch: 49/300, batch step:330, loss: 212.31675720214844\n",
      "Epoch: 50/300, batch step:330, loss: 208.409423828125\n",
      "Epoch: 51/300, batch step:330, loss: 204.5805206298828\n",
      "Epoch: 52/300, batch step:330, loss: 200.8289031982422\n",
      "Epoch: 53/300, batch step:330, loss: 197.1533966064453\n",
      "Epoch: 54/300, batch step:330, loss: 193.552978515625\n",
      "Epoch: 55/300, batch step:330, loss: 190.02647399902344\n",
      "Epoch: 56/300, batch step:330, loss: 186.57273864746094\n",
      "Epoch: 57/300, batch step:330, loss: 183.1907196044922\n",
      "Epoch: 58/300, batch step:330, loss: 179.8793182373047\n",
      "Epoch: 59/300, batch step:330, loss: 176.63739013671875\n",
      "Epoch: 60/300, batch step:330, loss: 173.46392822265625\n",
      "Epoch: 61/300, batch step:330, loss: 170.35777282714844\n",
      "Epoch: 62/300, batch step:330, loss: 167.3179168701172\n",
      "Epoch: 63/300, batch step:330, loss: 164.3432159423828\n",
      "Epoch: 64/300, batch step:330, loss: 161.43267822265625\n",
      "Epoch: 65/300, batch step:330, loss: 158.58522033691406\n",
      "Epoch: 66/300, batch step:330, loss: 155.79978942871094\n",
      "Epoch: 67/300, batch step:330, loss: 153.07537841796875\n",
      "Epoch: 68/300, batch step:330, loss: 150.41090393066406\n",
      "Epoch: 69/300, batch step:330, loss: 147.80535888671875\n",
      "Epoch: 70/300, batch step:330, loss: 145.25778198242188\n",
      "Epoch: 71/300, batch step:330, loss: 142.76710510253906\n",
      "Epoch: 72/300, batch step:330, loss: 140.332275390625\n",
      "Epoch: 73/300, batch step:330, loss: 137.952392578125\n",
      "Epoch: 74/300, batch step:330, loss: 135.62640380859375\n",
      "Epoch: 75/300, batch step:330, loss: 133.35340881347656\n",
      "Epoch: 76/300, batch step:330, loss: 131.1322784423828\n",
      "Epoch: 77/300, batch step:330, loss: 128.962158203125\n",
      "Epoch: 78/300, batch step:330, loss: 126.84209442138672\n",
      "Epoch: 79/300, batch step:330, loss: 124.77105712890625\n",
      "Epoch: 80/300, batch step:330, loss: 122.74820709228516\n",
      "Epoch: 81/300, batch step:330, loss: 120.77249145507812\n",
      "Epoch: 82/300, batch step:330, loss: 118.84305572509766\n",
      "Epoch: 83/300, batch step:330, loss: 116.95899200439453\n",
      "Epoch: 84/300, batch step:330, loss: 115.11935424804688\n",
      "Epoch: 85/300, batch step:330, loss: 113.3232421875\n",
      "Epoch: 86/300, batch step:330, loss: 111.56978607177734\n",
      "Epoch: 87/300, batch step:330, loss: 109.85809326171875\n",
      "Epoch: 88/300, batch step:330, loss: 108.187255859375\n",
      "Epoch: 89/300, batch step:330, loss: 106.55643463134766\n",
      "Epoch: 90/300, batch step:330, loss: 104.96477508544922\n",
      "Epoch: 91/300, batch step:330, loss: 103.41143035888672\n",
      "Epoch: 92/300, batch step:330, loss: 101.89553833007812\n",
      "Epoch: 93/300, batch step:330, loss: 100.41631317138672\n",
      "Epoch: 94/300, batch step:330, loss: 98.97290802001953\n",
      "Epoch: 95/300, batch step:330, loss: 97.5645523071289\n",
      "Epoch: 96/300, batch step:330, loss: 96.1904067993164\n",
      "Epoch: 97/300, batch step:330, loss: 94.84969329833984\n",
      "Epoch: 98/300, batch step:330, loss: 93.54158782958984\n",
      "Epoch: 99/300, batch step:330, loss: 92.26537322998047\n",
      "Epoch: 100/300, batch step:330, loss: 91.02027130126953\n",
      "Epoch: 101/300, batch step:330, loss: 89.8055419921875\n",
      "Epoch: 102/300, batch step:330, loss: 88.62042999267578\n",
      "Epoch: 103/300, batch step:330, loss: 87.4642333984375\n",
      "Epoch: 104/300, batch step:330, loss: 86.33624267578125\n",
      "Epoch: 105/300, batch step:330, loss: 85.23575592041016\n",
      "Epoch: 106/300, batch step:330, loss: 84.16203308105469\n",
      "Epoch: 107/300, batch step:330, loss: 83.11441802978516\n",
      "Epoch: 108/300, batch step:330, loss: 82.0921859741211\n",
      "Epoch: 109/300, batch step:330, loss: 81.09471893310547\n",
      "Epoch: 110/300, batch step:330, loss: 80.12139129638672\n",
      "Epoch: 111/300, batch step:330, loss: 79.17150115966797\n",
      "Epoch: 112/300, batch step:330, loss: 78.24449920654297\n",
      "Epoch: 113/300, batch step:330, loss: 77.33970642089844\n",
      "Epoch: 114/300, batch step:330, loss: 76.45651245117188\n",
      "Epoch: 115/300, batch step:330, loss: 75.59435272216797\n",
      "Epoch: 116/300, batch step:330, loss: 74.75261688232422\n",
      "Epoch: 117/300, batch step:330, loss: 73.93072509765625\n",
      "Epoch: 118/300, batch step:330, loss: 73.12812805175781\n",
      "Epoch: 119/300, batch step:330, loss: 72.34429168701172\n",
      "Epoch: 120/300, batch step:330, loss: 71.57864379882812\n",
      "Epoch: 121/300, batch step:330, loss: 70.83069610595703\n",
      "Epoch: 122/300, batch step:330, loss: 70.09989166259766\n",
      "Epoch: 123/300, batch step:330, loss: 69.3857192993164\n",
      "Epoch: 124/300, batch step:330, loss: 68.6877212524414\n",
      "Epoch: 125/300, batch step:330, loss: 68.0053939819336\n",
      "Epoch: 126/300, batch step:330, loss: 67.3382797241211\n",
      "Epoch: 127/300, batch step:330, loss: 66.68589782714844\n",
      "Epoch: 128/300, batch step:330, loss: 66.04779052734375\n",
      "Epoch: 129/300, batch step:330, loss: 65.42355346679688\n",
      "Epoch: 130/300, batch step:330, loss: 64.81271362304688\n",
      "Epoch: 131/300, batch step:330, loss: 64.2148666381836\n",
      "Epoch: 132/300, batch step:330, loss: 63.62961959838867\n",
      "Epoch: 133/300, batch step:330, loss: 63.05657958984375\n",
      "Epoch: 134/300, batch step:330, loss: 62.4953498840332\n",
      "Epoch: 135/300, batch step:330, loss: 61.945560455322266\n",
      "Epoch: 136/300, batch step:330, loss: 61.40684127807617\n",
      "Epoch: 137/300, batch step:330, loss: 60.87882614135742\n",
      "Epoch: 138/300, batch step:330, loss: 60.361175537109375\n",
      "Epoch: 139/300, batch step:330, loss: 59.853546142578125\n",
      "Epoch: 140/300, batch step:330, loss: 59.35564041137695\n",
      "Epoch: 141/300, batch step:330, loss: 58.86712646484375\n",
      "Epoch: 142/300, batch step:330, loss: 58.38771057128906\n",
      "Epoch: 143/300, batch step:330, loss: 57.91707992553711\n",
      "Epoch: 144/300, batch step:330, loss: 57.45493698120117\n",
      "Epoch: 145/300, batch step:330, loss: 57.001033782958984\n",
      "Epoch: 146/300, batch step:330, loss: 56.5550651550293\n",
      "Epoch: 147/300, batch step:330, loss: 56.11680603027344\n",
      "Epoch: 148/300, batch step:330, loss: 55.68595886230469\n",
      "Epoch: 149/300, batch step:330, loss: 55.26231384277344\n",
      "Epoch: 150/300, batch step:330, loss: 54.84564208984375\n",
      "Epoch: 151/300, batch step:330, loss: 54.43568801879883\n",
      "Epoch: 152/300, batch step:330, loss: 54.03224182128906\n",
      "Epoch: 153/300, batch step:330, loss: 53.63511276245117\n",
      "Epoch: 154/300, batch step:330, loss: 53.24406814575195\n",
      "Epoch: 155/300, batch step:330, loss: 52.85893630981445\n",
      "Epoch: 156/300, batch step:330, loss: 52.479496002197266\n",
      "Epoch: 157/300, batch step:330, loss: 52.1055793762207\n",
      "Epoch: 158/300, batch step:330, loss: 51.73701858520508\n",
      "Epoch: 159/300, batch step:330, loss: 51.37362289428711\n",
      "Epoch: 160/300, batch step:330, loss: 51.015228271484375\n",
      "Epoch: 161/300, batch step:330, loss: 50.66171646118164\n",
      "Epoch: 162/300, batch step:330, loss: 50.31290054321289\n",
      "Epoch: 163/300, batch step:330, loss: 49.968658447265625\n",
      "Epoch: 164/300, batch step:330, loss: 49.62882995605469\n",
      "Epoch: 165/300, batch step:330, loss: 49.29327392578125\n",
      "Epoch: 166/300, batch step:330, loss: 48.96187973022461\n",
      "Epoch: 167/300, batch step:330, loss: 48.634517669677734\n",
      "Epoch: 168/300, batch step:330, loss: 48.31106948852539\n",
      "Epoch: 169/300, batch step:330, loss: 47.99142837524414\n",
      "Epoch: 170/300, batch step:330, loss: 47.675472259521484\n",
      "Epoch: 171/300, batch step:330, loss: 47.36310958862305\n",
      "Epoch: 172/300, batch step:330, loss: 47.05424118041992\n",
      "Epoch: 173/300, batch step:330, loss: 46.74876022338867\n",
      "Epoch: 174/300, batch step:330, loss: 46.446563720703125\n",
      "Epoch: 175/300, batch step:330, loss: 46.14759826660156\n",
      "Epoch: 176/300, batch step:330, loss: 45.85175704956055\n",
      "Epoch: 177/300, batch step:330, loss: 45.558963775634766\n",
      "Epoch: 178/300, batch step:330, loss: 45.26913833618164\n",
      "Epoch: 179/300, batch step:330, loss: 44.98221969604492\n",
      "Epoch: 180/300, batch step:330, loss: 44.698116302490234\n",
      "Epoch: 181/300, batch step:330, loss: 44.41677474975586\n",
      "Epoch: 182/300, batch step:330, loss: 44.138126373291016\n",
      "Epoch: 183/300, batch step:330, loss: 43.86211013793945\n",
      "Epoch: 184/300, batch step:330, loss: 43.588680267333984\n",
      "Epoch: 185/300, batch step:330, loss: 43.3177604675293\n",
      "Epoch: 186/300, batch step:330, loss: 43.049312591552734\n",
      "Epoch: 187/300, batch step:330, loss: 42.78327560424805\n",
      "Epoch: 188/300, batch step:330, loss: 42.51960372924805\n",
      "Epoch: 189/300, batch step:330, loss: 42.25825119018555\n",
      "Epoch: 190/300, batch step:330, loss: 41.999176025390625\n",
      "Epoch: 191/300, batch step:330, loss: 41.7423210144043\n",
      "Epoch: 192/300, batch step:330, loss: 41.48765563964844\n",
      "Epoch: 193/300, batch step:330, loss: 41.23516082763672\n",
      "Epoch: 194/300, batch step:330, loss: 40.984771728515625\n",
      "Epoch: 195/300, batch step:330, loss: 40.73645782470703\n",
      "Epoch: 196/300, batch step:330, loss: 40.490196228027344\n",
      "Epoch: 197/300, batch step:330, loss: 40.24595260620117\n",
      "Epoch: 198/300, batch step:330, loss: 40.003692626953125\n",
      "Epoch: 199/300, batch step:330, loss: 39.76339340209961\n",
      "Epoch: 200/300, batch step:330, loss: 39.52501678466797\n",
      "Epoch: 201/300, batch step:330, loss: 39.28853225708008\n",
      "Epoch: 202/300, batch step:330, loss: 39.053932189941406\n",
      "Epoch: 203/300, batch step:330, loss: 38.8211669921875\n",
      "Epoch: 204/300, batch step:330, loss: 38.59024429321289\n",
      "Epoch: 205/300, batch step:330, loss: 38.361122131347656\n",
      "Epoch: 206/300, batch step:330, loss: 38.13378143310547\n",
      "Epoch: 207/300, batch step:330, loss: 37.908206939697266\n",
      "Epoch: 208/300, batch step:330, loss: 37.68437194824219\n",
      "Epoch: 209/300, batch step:330, loss: 37.46226501464844\n",
      "Epoch: 210/300, batch step:330, loss: 37.24186325073242\n",
      "Epoch: 211/300, batch step:330, loss: 37.02315902709961\n",
      "Epoch: 212/300, batch step:330, loss: 36.80612564086914\n",
      "Epoch: 213/300, batch step:330, loss: 36.590755462646484\n",
      "Epoch: 214/300, batch step:330, loss: 36.37704086303711\n",
      "Epoch: 215/300, batch step:330, loss: 36.164947509765625\n",
      "Epoch: 216/300, batch step:330, loss: 35.954471588134766\n",
      "Epoch: 217/300, batch step:330, loss: 35.74560546875\n",
      "Epoch: 218/300, batch step:330, loss: 35.538326263427734\n",
      "Epoch: 219/300, batch step:330, loss: 35.33262634277344\n",
      "Epoch: 220/300, batch step:330, loss: 35.12849044799805\n",
      "Epoch: 221/300, batch step:330, loss: 34.92591094970703\n",
      "Epoch: 222/300, batch step:330, loss: 34.724884033203125\n",
      "Epoch: 223/300, batch step:330, loss: 34.525386810302734\n",
      "Epoch: 224/300, batch step:330, loss: 34.327423095703125\n",
      "Epoch: 225/300, batch step:330, loss: 34.1309700012207\n",
      "Epoch: 226/300, batch step:330, loss: 33.936031341552734\n",
      "Epoch: 227/300, batch step:330, loss: 33.74259567260742\n",
      "Epoch: 228/300, batch step:330, loss: 33.5506477355957\n",
      "Epoch: 229/300, batch step:330, loss: 33.36018371582031\n",
      "Epoch: 230/300, batch step:330, loss: 33.171199798583984\n",
      "Epoch: 231/300, batch step:330, loss: 32.98367691040039\n",
      "Epoch: 232/300, batch step:330, loss: 32.797607421875\n",
      "Epoch: 233/300, batch step:330, loss: 32.61299514770508\n",
      "Epoch: 234/300, batch step:330, loss: 32.42982864379883\n",
      "Epoch: 235/300, batch step:330, loss: 32.248104095458984\n",
      "Epoch: 236/300, batch step:330, loss: 32.06781005859375\n",
      "Epoch: 237/300, batch step:330, loss: 31.888948440551758\n",
      "Epoch: 238/300, batch step:330, loss: 31.71150016784668\n",
      "Epoch: 239/300, batch step:330, loss: 31.535459518432617\n",
      "Epoch: 240/300, batch step:330, loss: 31.360849380493164\n",
      "Epoch: 241/300, batch step:330, loss: 31.187631607055664\n",
      "Epoch: 242/300, batch step:330, loss: 31.015817642211914\n",
      "Epoch: 243/300, batch step:330, loss: 30.84539222717285\n",
      "Epoch: 244/300, batch step:330, loss: 30.676353454589844\n",
      "Epoch: 245/300, batch step:330, loss: 30.508703231811523\n",
      "Epoch: 246/300, batch step:330, loss: 30.34242820739746\n",
      "Epoch: 247/300, batch step:330, loss: 30.177522659301758\n",
      "Epoch: 248/300, batch step:330, loss: 30.013992309570312\n",
      "Epoch: 249/300, batch step:330, loss: 29.851829528808594\n",
      "Epoch: 250/300, batch step:330, loss: 29.691015243530273\n",
      "Epoch: 251/300, batch step:330, loss: 29.53157615661621\n",
      "Epoch: 252/300, batch step:330, loss: 29.373472213745117\n",
      "Epoch: 253/300, batch step:330, loss: 29.216720581054688\n",
      "Epoch: 254/300, batch step:330, loss: 29.061307907104492\n",
      "Epoch: 255/300, batch step:330, loss: 28.90723419189453\n",
      "Epoch: 256/300, batch step:330, loss: 28.754499435424805\n",
      "Epoch: 257/300, batch step:330, loss: 28.60309410095215\n",
      "Epoch: 258/300, batch step:330, loss: 28.453018188476562\n",
      "Epoch: 259/300, batch step:330, loss: 28.30426597595215\n",
      "Epoch: 260/300, batch step:330, loss: 28.156829833984375\n",
      "Epoch: 261/300, batch step:330, loss: 28.010711669921875\n",
      "Epoch: 262/300, batch step:330, loss: 27.865896224975586\n",
      "Epoch: 263/300, batch step:330, loss: 27.722394943237305\n",
      "Epoch: 264/300, batch step:330, loss: 27.580202102661133\n",
      "Epoch: 265/300, batch step:330, loss: 27.439306259155273\n",
      "Epoch: 266/300, batch step:330, loss: 27.299707412719727\n",
      "Epoch: 267/300, batch step:330, loss: 27.161399841308594\n",
      "Epoch: 268/300, batch step:330, loss: 27.024389266967773\n",
      "Epoch: 269/300, batch step:330, loss: 26.88865852355957\n",
      "Epoch: 270/300, batch step:330, loss: 26.75421142578125\n",
      "Epoch: 271/300, batch step:330, loss: 26.621042251586914\n",
      "Epoch: 272/300, batch step:330, loss: 26.48915672302246\n",
      "Epoch: 273/300, batch step:330, loss: 26.35853385925293\n",
      "Epoch: 274/300, batch step:330, loss: 26.22918701171875\n",
      "Epoch: 275/300, batch step:330, loss: 26.101097106933594\n",
      "Epoch: 276/300, batch step:330, loss: 25.974279403686523\n",
      "Epoch: 277/300, batch step:330, loss: 25.848718643188477\n",
      "Epoch: 278/300, batch step:330, loss: 25.724416732788086\n",
      "Epoch: 279/300, batch step:330, loss: 25.601362228393555\n",
      "Epoch: 280/300, batch step:330, loss: 25.47956657409668\n",
      "Epoch: 281/300, batch step:330, loss: 25.359010696411133\n",
      "Epoch: 282/300, batch step:330, loss: 25.23969078063965\n",
      "Epoch: 283/300, batch step:330, loss: 25.121614456176758\n",
      "Epoch: 284/300, batch step:330, loss: 25.00477409362793\n",
      "Epoch: 285/300, batch step:330, loss: 24.889169692993164\n",
      "Epoch: 286/300, batch step:330, loss: 24.774789810180664\n",
      "Epoch: 287/300, batch step:330, loss: 24.66163444519043\n",
      "Epoch: 288/300, batch step:330, loss: 24.54970359802246\n",
      "Epoch: 289/300, batch step:330, loss: 24.438989639282227\n",
      "Epoch: 290/300, batch step:330, loss: 24.329490661621094\n",
      "Epoch: 291/300, batch step:330, loss: 24.22121238708496\n",
      "Epoch: 292/300, batch step:330, loss: 24.1141300201416\n",
      "Epoch: 293/300, batch step:330, loss: 24.008256912231445\n",
      "Epoch: 294/300, batch step:330, loss: 23.90358543395996\n",
      "Epoch: 295/300, batch step:330, loss: 23.80011558532715\n",
      "Epoch: 296/300, batch step:330, loss: 23.697837829589844\n",
      "Epoch: 297/300, batch step:330, loss: 23.59674644470215\n",
      "Epoch: 298/300, batch step:330, loss: 23.49684715270996\n",
      "Epoch: 299/300, batch step:330, loss: 23.39812660217285\n",
      "Epoch: 300/300, batch step:330, loss: 23.30059242248535\n"
     ]
    }
   ],
   "source": [
    "simple_model_01 = SimpleRegression_01()\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(simple_model_01.parameters(), lr=0.01)\n",
    "\n",
    "trained_model = train_loop(simple_model_01, tr_ftr_ts, tr_tgt_ts, loss_fn, optimizer, \n",
    "                         epochs=300, batch_size=30, verbose=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([152])\n",
      "test 데이터 세트의 MSE: 45.09147644042969\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>PREDICTED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.504311</td>\n",
       "      <td>0.546082</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>20.649502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727534</td>\n",
       "      <td>0.082781</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>27.032528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.442422</td>\n",
       "      <td>0.348786</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>20.653357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443380</td>\n",
       "      <td>0.197296</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>21.494732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.519640</td>\n",
       "      <td>0.139349</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>23.129108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>20.100000</td>\n",
       "      <td>22.062653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>19.810680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.049669</td>\n",
       "      <td>32.400002</td>\n",
       "      <td>25.224556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.623683</td>\n",
       "      <td>0.061258</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>25.353655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.533940</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>21.882050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>21.700001</td>\n",
       "      <td>20.562416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.550297</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>23.247658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.489366</td>\n",
       "      <td>0.212472</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>22.207403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.399885</td>\n",
       "      <td>0.341336</td>\n",
       "      <td>20.799999</td>\n",
       "      <td>19.958265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.110558</td>\n",
       "      <td>0.596302</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>13.566284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.381347</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>21.665718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.972682</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>12.779583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.143488</td>\n",
       "      <td>36.099998</td>\n",
       "      <td>26.297726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625216</td>\n",
       "      <td>0.579746</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>22.557194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.548764</td>\n",
       "      <td>0.284216</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>22.844051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RM     LSTAT      PRICE  PREDICTED\n",
       "0   0.504311  0.546082  11.000000  20.649502\n",
       "1   0.727534  0.082781  31.500000  27.032528\n",
       "2   0.442422  0.348786  22.000000  20.653357\n",
       "3   0.443380  0.197296  50.000000  21.494732\n",
       "4   0.519640  0.139349  24.100000  23.129108\n",
       "5   0.511401  0.309051  20.100000  22.062653\n",
       "6   0.425752  0.450607  22.500000  19.810680\n",
       "7   0.612569  0.049669  32.400002  25.224556\n",
       "8   0.623683  0.061258  31.600000  25.353655\n",
       "9   0.571757  0.533940  10.900000  21.882050\n",
       "10  0.498755  0.544426  21.700001  20.562416\n",
       "11  0.550297  0.214956  24.500000  23.247658\n",
       "12  0.489366  0.212472  20.500000  22.207403\n",
       "13  0.399885  0.341336  20.799999  19.958265\n",
       "14  0.110558  0.596302  11.900000  13.566284\n",
       "15  0.511209  0.381347  19.400000  21.665718\n",
       "16  0.183560  0.972682   7.000000  12.779583\n",
       "17  0.704158  0.143488  36.099998  26.297726\n",
       "18  0.625216  0.579746   8.400000  22.557194\n",
       "19  0.548764  0.284216  16.100000  22.844051"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_predicted_ts = trained_model(test_ftr_ts)\n",
    "print(test_predicted_ts.requires_grad, test_predicted_ts.shape)\n",
    "\n",
    "boston_test_df = pd.DataFrame({\n",
    "    'RM': test_features[:, 0],\n",
    "    'LSTAT': test_ftr_ts[:, 1],\n",
    "    'PRICE': test_tgt_ts,\n",
    "    'PREDICTED': test_predicted_ts.squeeze(-1).detach().numpy()\n",
    "})\n",
    "\n",
    "test_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED'])\n",
    "print(\"test 데이터 세트의 MSE:\", test_total_mse)\n",
    "\n",
    "boston_test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.2616, 0.6159]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.2969], requires_grad=True)]\n",
      "SimpleRegression_02(\n",
      "  (linear): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleRegression_02(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=2, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "simple_model_02 = SimpleRegression_02()\n",
    "# simple_model의 학습 파라미터 출력(Learnable Parameter)\n",
    "print(list(simple_model_02.parameters()))\n",
    "print(simple_model_02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300, batch step:330, loss: 561.0238037109375\n",
      "Epoch: 2/300, batch step:330, loss: 551.493896484375\n",
      "Epoch: 3/300, batch step:330, loss: 542.0956420898438\n",
      "Epoch: 4/300, batch step:330, loss: 532.8333129882812\n",
      "Epoch: 5/300, batch step:330, loss: 523.7081909179688\n",
      "Epoch: 6/300, batch step:330, loss: 514.719482421875\n",
      "Epoch: 7/300, batch step:330, loss: 505.8660888671875\n",
      "Epoch: 8/300, batch step:330, loss: 497.1463928222656\n",
      "Epoch: 9/300, batch step:330, loss: 488.5589599609375\n",
      "Epoch: 10/300, batch step:330, loss: 480.1021423339844\n",
      "Epoch: 11/300, batch step:330, loss: 471.7745666503906\n",
      "Epoch: 12/300, batch step:330, loss: 463.5748596191406\n",
      "Epoch: 13/300, batch step:330, loss: 455.5016174316406\n",
      "Epoch: 14/300, batch step:330, loss: 447.5534973144531\n",
      "Epoch: 15/300, batch step:330, loss: 439.7292175292969\n",
      "Epoch: 16/300, batch step:330, loss: 432.02734375\n",
      "Epoch: 17/300, batch step:330, loss: 424.4466857910156\n",
      "Epoch: 18/300, batch step:330, loss: 416.9859313964844\n",
      "Epoch: 19/300, batch step:330, loss: 409.6437683105469\n",
      "Epoch: 20/300, batch step:330, loss: 402.4189453125\n",
      "Epoch: 21/300, batch step:330, loss: 395.3101501464844\n",
      "Epoch: 22/300, batch step:330, loss: 388.3161926269531\n",
      "Epoch: 23/300, batch step:330, loss: 381.4358825683594\n",
      "Epoch: 24/300, batch step:330, loss: 374.6678466796875\n",
      "Epoch: 25/300, batch step:330, loss: 368.0108642578125\n",
      "Epoch: 26/300, batch step:330, loss: 361.4637756347656\n",
      "Epoch: 27/300, batch step:330, loss: 355.0252990722656\n",
      "Epoch: 28/300, batch step:330, loss: 348.6942138671875\n",
      "Epoch: 29/300, batch step:330, loss: 342.4693908691406\n",
      "Epoch: 30/300, batch step:330, loss: 336.3495788574219\n",
      "Epoch: 31/300, batch step:330, loss: 330.3335266113281\n",
      "Epoch: 32/300, batch step:330, loss: 324.42010498046875\n",
      "Epoch: 33/300, batch step:330, loss: 318.6080322265625\n",
      "Epoch: 34/300, batch step:330, loss: 312.896240234375\n",
      "Epoch: 35/300, batch step:330, loss: 307.2834777832031\n",
      "Epoch: 36/300, batch step:330, loss: 301.76861572265625\n",
      "Epoch: 37/300, batch step:330, loss: 296.3504333496094\n",
      "Epoch: 38/300, batch step:330, loss: 291.0277404785156\n",
      "Epoch: 39/300, batch step:330, loss: 285.7994689941406\n",
      "Epoch: 40/300, batch step:330, loss: 280.66448974609375\n",
      "Epoch: 41/300, batch step:330, loss: 275.62152099609375\n",
      "Epoch: 42/300, batch step:330, loss: 270.6695251464844\n",
      "Epoch: 43/300, batch step:330, loss: 265.8072814941406\n",
      "Epoch: 44/300, batch step:330, loss: 261.0337829589844\n",
      "Epoch: 45/300, batch step:330, loss: 256.3477783203125\n",
      "Epoch: 46/300, batch step:330, loss: 251.7481231689453\n",
      "Epoch: 47/300, batch step:330, loss: 247.23382568359375\n",
      "Epoch: 48/300, batch step:330, loss: 242.8037109375\n",
      "Epoch: 49/300, batch step:330, loss: 238.4566650390625\n",
      "Epoch: 50/300, batch step:330, loss: 234.19158935546875\n",
      "Epoch: 51/300, batch step:330, loss: 230.0074005126953\n",
      "Epoch: 52/300, batch step:330, loss: 225.9029541015625\n",
      "Epoch: 53/300, batch step:330, loss: 221.8772430419922\n",
      "Epoch: 54/300, batch step:330, loss: 217.9291534423828\n",
      "Epoch: 55/300, batch step:330, loss: 214.05763244628906\n",
      "Epoch: 56/300, batch step:330, loss: 210.2615509033203\n",
      "Epoch: 57/300, batch step:330, loss: 206.5398712158203\n",
      "Epoch: 58/300, batch step:330, loss: 202.89154052734375\n",
      "Epoch: 59/300, batch step:330, loss: 199.31553649902344\n",
      "Epoch: 60/300, batch step:330, loss: 195.8107147216797\n",
      "Epoch: 61/300, batch step:330, loss: 192.37615966796875\n",
      "Epoch: 62/300, batch step:330, loss: 189.0107421875\n",
      "Epoch: 63/300, batch step:330, loss: 185.7134246826172\n",
      "Epoch: 64/300, batch step:330, loss: 182.48326110839844\n",
      "Epoch: 65/300, batch step:330, loss: 179.3191680908203\n",
      "Epoch: 66/300, batch step:330, loss: 176.22015380859375\n",
      "Epoch: 67/300, batch step:330, loss: 173.1852264404297\n",
      "Epoch: 68/300, batch step:330, loss: 170.2133331298828\n",
      "Epoch: 69/300, batch step:330, loss: 167.3035430908203\n",
      "Epoch: 70/300, batch step:330, loss: 164.45481872558594\n",
      "Epoch: 71/300, batch step:330, loss: 161.6661834716797\n",
      "Epoch: 72/300, batch step:330, loss: 158.93666076660156\n",
      "Epoch: 73/300, batch step:330, loss: 156.26528930664062\n",
      "Epoch: 74/300, batch step:330, loss: 153.65110778808594\n",
      "Epoch: 75/300, batch step:330, loss: 151.0931396484375\n",
      "Epoch: 76/300, batch step:330, loss: 148.5904541015625\n",
      "Epoch: 77/300, batch step:330, loss: 146.14212036132812\n",
      "Epoch: 78/300, batch step:330, loss: 143.7471466064453\n",
      "Epoch: 79/300, batch step:330, loss: 141.4046173095703\n",
      "Epoch: 80/300, batch step:330, loss: 139.11361694335938\n",
      "Epoch: 81/300, batch step:330, loss: 136.87322998046875\n",
      "Epoch: 82/300, batch step:330, loss: 134.68255615234375\n",
      "Epoch: 83/300, batch step:330, loss: 132.54066467285156\n",
      "Epoch: 84/300, batch step:330, loss: 130.44668579101562\n",
      "Epoch: 85/300, batch step:330, loss: 128.3997039794922\n",
      "Epoch: 86/300, batch step:330, loss: 126.3989028930664\n",
      "Epoch: 87/300, batch step:330, loss: 124.4433364868164\n",
      "Epoch: 88/300, batch step:330, loss: 122.53215789794922\n",
      "Epoch: 89/300, batch step:330, loss: 120.66451263427734\n",
      "Epoch: 90/300, batch step:330, loss: 118.8395767211914\n",
      "Epoch: 91/300, batch step:330, loss: 117.05646514892578\n",
      "Epoch: 92/300, batch step:330, loss: 115.3143539428711\n",
      "Epoch: 93/300, batch step:330, loss: 113.61239624023438\n",
      "Epoch: 94/300, batch step:330, loss: 111.9498062133789\n",
      "Epoch: 95/300, batch step:330, loss: 110.3258285522461\n",
      "Epoch: 96/300, batch step:330, loss: 108.73958587646484\n",
      "Epoch: 97/300, batch step:330, loss: 107.19024658203125\n",
      "Epoch: 98/300, batch step:330, loss: 105.67709350585938\n",
      "Epoch: 99/300, batch step:330, loss: 104.19934844970703\n",
      "Epoch: 100/300, batch step:330, loss: 102.7562484741211\n",
      "Epoch: 101/300, batch step:330, loss: 101.34700775146484\n",
      "Epoch: 102/300, batch step:330, loss: 99.97087860107422\n",
      "Epoch: 103/300, batch step:330, loss: 98.62710571289062\n",
      "Epoch: 104/300, batch step:330, loss: 97.31497955322266\n",
      "Epoch: 105/300, batch step:330, loss: 96.03377532958984\n",
      "Epoch: 106/300, batch step:330, loss: 94.78276824951172\n",
      "Epoch: 107/300, batch step:330, loss: 93.5613021850586\n",
      "Epoch: 108/300, batch step:330, loss: 92.3686294555664\n",
      "Epoch: 109/300, batch step:330, loss: 91.20409393310547\n",
      "Epoch: 110/300, batch step:330, loss: 90.0669937133789\n",
      "Epoch: 111/300, batch step:330, loss: 88.9566879272461\n",
      "Epoch: 112/300, batch step:330, loss: 87.87252044677734\n",
      "Epoch: 113/300, batch step:330, loss: 86.8138198852539\n",
      "Epoch: 114/300, batch step:330, loss: 85.780029296875\n",
      "Epoch: 115/300, batch step:330, loss: 84.77042388916016\n",
      "Epoch: 116/300, batch step:330, loss: 83.78443908691406\n",
      "Epoch: 117/300, batch step:330, loss: 82.82144927978516\n",
      "Epoch: 118/300, batch step:330, loss: 81.88086700439453\n",
      "Epoch: 119/300, batch step:330, loss: 80.96210479736328\n",
      "Epoch: 120/300, batch step:330, loss: 80.06459045410156\n",
      "Epoch: 121/300, batch step:330, loss: 79.18775177001953\n",
      "Epoch: 122/300, batch step:330, loss: 78.3310317993164\n",
      "Epoch: 123/300, batch step:330, loss: 77.49391174316406\n",
      "Epoch: 124/300, batch step:330, loss: 76.6758041381836\n",
      "Epoch: 125/300, batch step:330, loss: 75.8762435913086\n",
      "Epoch: 126/300, batch step:330, loss: 75.09465789794922\n",
      "Epoch: 127/300, batch step:330, loss: 74.33062744140625\n",
      "Epoch: 128/300, batch step:330, loss: 73.58358001708984\n",
      "Epoch: 129/300, batch step:330, loss: 72.8530502319336\n",
      "Epoch: 130/300, batch step:330, loss: 72.13855743408203\n",
      "Epoch: 131/300, batch step:330, loss: 71.43965911865234\n",
      "Epoch: 132/300, batch step:330, loss: 70.75592803955078\n",
      "Epoch: 133/300, batch step:330, loss: 70.08684539794922\n",
      "Epoch: 134/300, batch step:330, loss: 69.43206024169922\n",
      "Epoch: 135/300, batch step:330, loss: 68.79107666015625\n",
      "Epoch: 136/300, batch step:330, loss: 68.16354370117188\n",
      "Epoch: 137/300, batch step:330, loss: 67.54901885986328\n",
      "Epoch: 138/300, batch step:330, loss: 66.94709014892578\n",
      "Epoch: 139/300, batch step:330, loss: 66.35741424560547\n",
      "Epoch: 140/300, batch step:330, loss: 65.7796401977539\n",
      "Epoch: 141/300, batch step:330, loss: 65.2133560180664\n",
      "Epoch: 142/300, batch step:330, loss: 64.65821075439453\n",
      "Epoch: 143/300, batch step:330, loss: 64.11386108398438\n",
      "Epoch: 144/300, batch step:330, loss: 63.57999038696289\n",
      "Epoch: 145/300, batch step:330, loss: 63.056243896484375\n",
      "Epoch: 146/300, batch step:330, loss: 62.54233932495117\n",
      "Epoch: 147/300, batch step:330, loss: 62.03794479370117\n",
      "Epoch: 148/300, batch step:330, loss: 61.54275131225586\n",
      "Epoch: 149/300, batch step:330, loss: 61.05647659301758\n",
      "Epoch: 150/300, batch step:330, loss: 60.578853607177734\n",
      "Epoch: 151/300, batch step:330, loss: 60.1096076965332\n",
      "Epoch: 152/300, batch step:330, loss: 59.6484489440918\n",
      "Epoch: 153/300, batch step:330, loss: 59.195133209228516\n",
      "Epoch: 154/300, batch step:330, loss: 58.7494010925293\n",
      "Epoch: 155/300, batch step:330, loss: 58.311004638671875\n",
      "Epoch: 156/300, batch step:330, loss: 57.87973403930664\n",
      "Epoch: 157/300, batch step:330, loss: 57.4553337097168\n",
      "Epoch: 158/300, batch step:330, loss: 57.037593841552734\n",
      "Epoch: 159/300, batch step:330, loss: 56.6263313293457\n",
      "Epoch: 160/300, batch step:330, loss: 56.22129821777344\n",
      "Epoch: 161/300, batch step:330, loss: 55.82232666015625\n",
      "Epoch: 162/300, batch step:330, loss: 55.429203033447266\n",
      "Epoch: 163/300, batch step:330, loss: 55.04176712036133\n",
      "Epoch: 164/300, batch step:330, loss: 54.65983581542969\n",
      "Epoch: 165/300, batch step:330, loss: 54.28321838378906\n",
      "Epoch: 166/300, batch step:330, loss: 53.91175842285156\n",
      "Epoch: 167/300, batch step:330, loss: 53.54531478881836\n",
      "Epoch: 168/300, batch step:330, loss: 53.1837272644043\n",
      "Epoch: 169/300, batch step:330, loss: 52.82682418823242\n",
      "Epoch: 170/300, batch step:330, loss: 52.474483489990234\n",
      "Epoch: 171/300, batch step:330, loss: 52.12656021118164\n",
      "Epoch: 172/300, batch step:330, loss: 51.78291702270508\n",
      "Epoch: 173/300, batch step:330, loss: 51.44345474243164\n",
      "Epoch: 174/300, batch step:330, loss: 51.1080207824707\n",
      "Epoch: 175/300, batch step:330, loss: 50.77653121948242\n",
      "Epoch: 176/300, batch step:330, loss: 50.44882583618164\n",
      "Epoch: 177/300, batch step:330, loss: 50.12485122680664\n",
      "Epoch: 178/300, batch step:330, loss: 49.80447006225586\n",
      "Epoch: 179/300, batch step:330, loss: 49.48757553100586\n",
      "Epoch: 180/300, batch step:330, loss: 49.17409896850586\n",
      "Epoch: 181/300, batch step:330, loss: 48.86393356323242\n",
      "Epoch: 182/300, batch step:330, loss: 48.556976318359375\n",
      "Epoch: 183/300, batch step:330, loss: 48.25318908691406\n",
      "Epoch: 184/300, batch step:330, loss: 47.95246505737305\n",
      "Epoch: 185/300, batch step:330, loss: 47.65470886230469\n",
      "Epoch: 186/300, batch step:330, loss: 47.35987854003906\n",
      "Epoch: 187/300, batch step:330, loss: 47.06787109375\n",
      "Epoch: 188/300, batch step:330, loss: 46.77864456176758\n",
      "Epoch: 189/300, batch step:330, loss: 46.49213790893555\n",
      "Epoch: 190/300, batch step:330, loss: 46.208248138427734\n",
      "Epoch: 191/300, batch step:330, loss: 45.92697525024414\n",
      "Epoch: 192/300, batch step:330, loss: 45.648223876953125\n",
      "Epoch: 193/300, batch step:330, loss: 45.371944427490234\n",
      "Epoch: 194/300, batch step:330, loss: 45.09809875488281\n",
      "Epoch: 195/300, batch step:330, loss: 44.82661437988281\n",
      "Epoch: 196/300, batch step:330, loss: 44.557464599609375\n",
      "Epoch: 197/300, batch step:330, loss: 44.29060363769531\n",
      "Epoch: 198/300, batch step:330, loss: 44.02598190307617\n",
      "Epoch: 199/300, batch step:330, loss: 43.763553619384766\n",
      "Epoch: 200/300, batch step:330, loss: 43.503292083740234\n",
      "Epoch: 201/300, batch step:330, loss: 43.24514389038086\n",
      "Epoch: 202/300, batch step:330, loss: 42.989070892333984\n",
      "Epoch: 203/300, batch step:330, loss: 42.73505783081055\n",
      "Epoch: 204/300, batch step:330, loss: 42.48305130004883\n",
      "Epoch: 205/300, batch step:330, loss: 42.2330322265625\n",
      "Epoch: 206/300, batch step:330, loss: 41.984981536865234\n",
      "Epoch: 207/300, batch step:330, loss: 41.73885726928711\n",
      "Epoch: 208/300, batch step:330, loss: 41.494632720947266\n",
      "Epoch: 209/300, batch step:330, loss: 41.25226974487305\n",
      "Epoch: 210/300, batch step:330, loss: 41.01176071166992\n",
      "Epoch: 211/300, batch step:330, loss: 40.7730827331543\n",
      "Epoch: 212/300, batch step:330, loss: 40.53620529174805\n",
      "Epoch: 213/300, batch step:330, loss: 40.301109313964844\n",
      "Epoch: 214/300, batch step:330, loss: 40.06776809692383\n",
      "Epoch: 215/300, batch step:330, loss: 39.8361701965332\n",
      "Epoch: 216/300, batch step:330, loss: 39.606300354003906\n",
      "Epoch: 217/300, batch step:330, loss: 39.378116607666016\n",
      "Epoch: 218/300, batch step:330, loss: 39.15163040161133\n",
      "Epoch: 219/300, batch step:330, loss: 38.92681884765625\n",
      "Epoch: 220/300, batch step:330, loss: 38.70366287231445\n",
      "Epoch: 221/300, batch step:330, loss: 38.48212432861328\n",
      "Epoch: 222/300, batch step:330, loss: 38.2622184753418\n",
      "Epoch: 223/300, batch step:330, loss: 38.04391098022461\n",
      "Epoch: 224/300, batch step:330, loss: 37.827213287353516\n",
      "Epoch: 225/300, batch step:330, loss: 37.612091064453125\n",
      "Epoch: 226/300, batch step:330, loss: 37.39854049682617\n",
      "Epoch: 227/300, batch step:330, loss: 37.18654251098633\n",
      "Epoch: 228/300, batch step:330, loss: 36.97609329223633\n",
      "Epoch: 229/300, batch step:330, loss: 36.76719284057617\n",
      "Epoch: 230/300, batch step:330, loss: 36.55980682373047\n",
      "Epoch: 231/300, batch step:330, loss: 36.353939056396484\n",
      "Epoch: 232/300, batch step:330, loss: 36.14958572387695\n",
      "Epoch: 233/300, batch step:330, loss: 35.94672775268555\n",
      "Epoch: 234/300, batch step:330, loss: 35.745357513427734\n",
      "Epoch: 235/300, batch step:330, loss: 35.54545974731445\n",
      "Epoch: 236/300, batch step:330, loss: 35.3470344543457\n",
      "Epoch: 237/300, batch step:330, loss: 35.15007781982422\n",
      "Epoch: 238/300, batch step:330, loss: 34.95457458496094\n",
      "Epoch: 239/300, batch step:330, loss: 34.76051712036133\n",
      "Epoch: 240/300, batch step:330, loss: 34.56791305541992\n",
      "Epoch: 241/300, batch step:330, loss: 34.37673568725586\n",
      "Epoch: 242/300, batch step:330, loss: 34.18698501586914\n",
      "Epoch: 243/300, batch step:330, loss: 33.998661041259766\n",
      "Epoch: 244/300, batch step:330, loss: 33.8117561340332\n",
      "Epoch: 245/300, batch step:330, loss: 33.62625503540039\n",
      "Epoch: 246/300, batch step:330, loss: 33.44215774536133\n",
      "Epoch: 247/300, batch step:330, loss: 33.259464263916016\n",
      "Epoch: 248/300, batch step:330, loss: 33.07815170288086\n",
      "Epoch: 249/300, batch step:330, loss: 32.898231506347656\n",
      "Epoch: 250/300, batch step:330, loss: 32.71969223022461\n",
      "Epoch: 251/300, batch step:330, loss: 32.54253387451172\n",
      "Epoch: 252/300, batch step:330, loss: 32.36674880981445\n",
      "Epoch: 253/300, batch step:330, loss: 32.19234085083008\n",
      "Epoch: 254/300, batch step:330, loss: 32.019287109375\n",
      "Epoch: 255/300, batch step:330, loss: 31.847593307495117\n",
      "Epoch: 256/300, batch step:330, loss: 31.67724609375\n",
      "Epoch: 257/300, batch step:330, loss: 31.50824546813965\n",
      "Epoch: 258/300, batch step:330, loss: 31.340606689453125\n",
      "Epoch: 259/300, batch step:330, loss: 31.174314498901367\n",
      "Epoch: 260/300, batch step:330, loss: 31.009353637695312\n",
      "Epoch: 261/300, batch step:330, loss: 30.845726013183594\n",
      "Epoch: 262/300, batch step:330, loss: 30.68343162536621\n",
      "Epoch: 263/300, batch step:330, loss: 30.5224552154541\n",
      "Epoch: 264/300, batch step:330, loss: 30.362802505493164\n",
      "Epoch: 265/300, batch step:330, loss: 30.20447540283203\n",
      "Epoch: 266/300, batch step:330, loss: 30.047460556030273\n",
      "Epoch: 267/300, batch step:330, loss: 29.891754150390625\n",
      "Epoch: 268/300, batch step:330, loss: 29.737363815307617\n",
      "Epoch: 269/300, batch step:330, loss: 29.584272384643555\n",
      "Epoch: 270/300, batch step:330, loss: 29.432485580444336\n",
      "Epoch: 271/300, batch step:330, loss: 29.28199577331543\n",
      "Epoch: 272/300, batch step:330, loss: 29.132797241210938\n",
      "Epoch: 273/300, batch step:330, loss: 28.984888076782227\n",
      "Epoch: 274/300, batch step:330, loss: 28.838272094726562\n",
      "Epoch: 275/300, batch step:330, loss: 28.69293785095215\n",
      "Epoch: 276/300, batch step:330, loss: 28.548891067504883\n",
      "Epoch: 277/300, batch step:330, loss: 28.4061222076416\n",
      "Epoch: 278/300, batch step:330, loss: 28.26462745666504\n",
      "Epoch: 279/300, batch step:330, loss: 28.12439727783203\n",
      "Epoch: 280/300, batch step:330, loss: 27.985437393188477\n",
      "Epoch: 281/300, batch step:330, loss: 27.847747802734375\n",
      "Epoch: 282/300, batch step:330, loss: 27.711318969726562\n",
      "Epoch: 283/300, batch step:330, loss: 27.576154708862305\n",
      "Epoch: 284/300, batch step:330, loss: 27.442245483398438\n",
      "Epoch: 285/300, batch step:330, loss: 27.309595108032227\n",
      "Epoch: 286/300, batch step:330, loss: 27.17818260192871\n",
      "Epoch: 287/300, batch step:330, loss: 27.048032760620117\n",
      "Epoch: 288/300, batch step:330, loss: 26.91912078857422\n",
      "Epoch: 289/300, batch step:330, loss: 26.79144287109375\n",
      "Epoch: 290/300, batch step:330, loss: 26.665008544921875\n",
      "Epoch: 291/300, batch step:330, loss: 26.53981590270996\n",
      "Epoch: 292/300, batch step:330, loss: 26.415842056274414\n",
      "Epoch: 293/300, batch step:330, loss: 26.293115615844727\n",
      "Epoch: 294/300, batch step:330, loss: 26.171606063842773\n",
      "Epoch: 295/300, batch step:330, loss: 26.05131721496582\n",
      "Epoch: 296/300, batch step:330, loss: 25.9322509765625\n",
      "Epoch: 297/300, batch step:330, loss: 25.81439781188965\n",
      "Epoch: 298/300, batch step:330, loss: 25.69776725769043\n",
      "Epoch: 299/300, batch step:330, loss: 25.58234214782715\n",
      "Epoch: 300/300, batch step:330, loss: 25.46812629699707\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(simple_model_02.parameters(), lr=0.01)\n",
    "tr_ftr_ts, tr_tgt_ts, test_ftr_ts, test_tgt_ts = get_scaled_train_test_feature_target_ts_01(data_df=boston_df)\n",
    "\n",
    "trained_model = train_loop(simple_model_02, tr_ftr_ts, tr_tgt_ts, loss_fn, optimizer, \n",
    "                         epochs=300, batch_size=30, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([152, 1])\n",
      "test 데이터 세트의 MSE: 47.48511505126953\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RM</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>PREDICTED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.504311</td>\n",
       "      <td>0.546082</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>21.008846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.727534</td>\n",
       "      <td>0.082781</td>\n",
       "      <td>31.500000</td>\n",
       "      <td>26.716106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.442422</td>\n",
       "      <td>0.348786</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>20.642385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.443380</td>\n",
       "      <td>0.197296</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>21.224915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.519640</td>\n",
       "      <td>0.139349</td>\n",
       "      <td>24.100000</td>\n",
       "      <td>22.800194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.511401</td>\n",
       "      <td>0.309051</td>\n",
       "      <td>20.100000</td>\n",
       "      <td>22.019934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.425752</td>\n",
       "      <td>0.450607</td>\n",
       "      <td>22.500000</td>\n",
       "      <td>19.965260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.612569</td>\n",
       "      <td>0.049669</td>\n",
       "      <td>32.400002</td>\n",
       "      <td>24.790985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.623683</td>\n",
       "      <td>0.061258</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>24.945768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.571757</td>\n",
       "      <td>0.533940</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>22.256081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.498755</td>\n",
       "      <td>0.544426</td>\n",
       "      <td>21.700001</td>\n",
       "      <td>20.916006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.550297</td>\n",
       "      <td>0.214956</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>23.064304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.489366</td>\n",
       "      <td>0.212472</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>21.987755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.399885</td>\n",
       "      <td>0.341336</td>\n",
       "      <td>20.799999</td>\n",
       "      <td>19.912169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.110558</td>\n",
       "      <td>0.596302</td>\n",
       "      <td>11.900000</td>\n",
       "      <td>13.804527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.511209</td>\n",
       "      <td>0.381347</td>\n",
       "      <td>19.400000</td>\n",
       "      <td>21.746666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.183560</td>\n",
       "      <td>0.972682</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>13.700577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.704158</td>\n",
       "      <td>0.143488</td>\n",
       "      <td>36.099998</td>\n",
       "      <td>26.072941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.625216</td>\n",
       "      <td>0.579746</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>23.037758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.548764</td>\n",
       "      <td>0.284216</td>\n",
       "      <td>16.100000</td>\n",
       "      <td>22.778465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RM     LSTAT      PRICE  PREDICTED\n",
       "0   0.504311  0.546082  11.000000  21.008846\n",
       "1   0.727534  0.082781  31.500000  26.716106\n",
       "2   0.442422  0.348786  22.000000  20.642385\n",
       "3   0.443380  0.197296  50.000000  21.224915\n",
       "4   0.519640  0.139349  24.100000  22.800194\n",
       "5   0.511401  0.309051  20.100000  22.019934\n",
       "6   0.425752  0.450607  22.500000  19.965260\n",
       "7   0.612569  0.049669  32.400002  24.790985\n",
       "8   0.623683  0.061258  31.600000  24.945768\n",
       "9   0.571757  0.533940  10.900000  22.256081\n",
       "10  0.498755  0.544426  21.700001  20.916006\n",
       "11  0.550297  0.214956  24.500000  23.064304\n",
       "12  0.489366  0.212472  20.500000  21.987755\n",
       "13  0.399885  0.341336  20.799999  19.912169\n",
       "14  0.110558  0.596302  11.900000  13.804527\n",
       "15  0.511209  0.381347  19.400000  21.746666\n",
       "16  0.183560  0.972682   7.000000  13.700577\n",
       "17  0.704158  0.143488  36.099998  26.072941\n",
       "18  0.625216  0.579746   8.400000  23.037758\n",
       "19  0.548764  0.284216  16.100000  22.778465"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_predicted_ts = trained_model(test_ftr_ts)\n",
    "print(test_predicted_ts.requires_grad, test_predicted_ts.shape)\n",
    "\n",
    "boston_test_df = pd.DataFrame({\n",
    "    'RM': test_features[:, 0],\n",
    "    'LSTAT': test_ftr_ts[:, 1],\n",
    "    'PRICE': test_tgt_ts,\n",
    "    'PREDICTED': test_predicted_ts.squeeze(-1).detach().numpy()\n",
    "})\n",
    "\n",
    "test_total_mse = mean_squared_error(boston_test_df['PRICE'], boston_test_df['PREDICTED'])\n",
    "print(\"test 데이터 세트의 MSE:\", test_total_mse)\n",
    "\n",
    "boston_test_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 활성화 함수(Activation Function)\n",
    "* function 기반으로 또는 Layer 기반으로 적용 가능\n",
    "* relu()/ReLU()는 inplace=False(default)시 원본 입력 tensor를 복제하여 relu 적용. inplace=True시 원본 입력 tensor에 바로 relu를 적용하므로 원본 입력 tensor가 변환됨(메모리가 절감됨)\n",
    "* softmax()/Softmax()의 경우는 dim=-1 을 보통 적용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_output: tensor([5.0000e-01, 9.9988e-01, 9.9995e-01, 1.2339e-04, 4.5398e-05])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.tensor([0, 9.0, 10.0, -9.0, -10.0], dtype=torch.float32)\n",
    "\n",
    "# sigmoid 적용\n",
    "sigmoid_output = F.sigmoid(input_tensor)\n",
    "# sigmoid_output = torch.sigmoid(input_tensor)\n",
    "\n",
    "print('sigmoid_output:', sigmoid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_output_02: tensor([5.0000e-01, 9.9988e-01, 9.9995e-01, 1.2339e-04, 4.5398e-05])\n"
     ]
    }
   ],
   "source": [
    "# Sigmoid Layer 적용\n",
    "sigmoid_layer = nn.Sigmoid()\n",
    "sigmoid_output = sigmoid_layer(input_tensor)\n",
    "print('sigmoid_output_02:', sigmoid_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu_output: tensor([ 0.,  9., 10.,  0.,  0.])\n",
      "input_tensor: tensor([  0.,   9.,  10.,  -9., -10.])\n"
     ]
    }
   ],
   "source": [
    "# relu() 함수 적용\n",
    "input_tensor = torch.tensor([0, 9.0, 10.0, -9.0, -10.0], dtype=torch.float32)\n",
    "# relu function 적용\n",
    "# inplace=False시 입력 원본 tensor는 그대로 유지한 채 입력 tensor를 복사하여 relu함수 적용하여 반환\n",
    "# inplace=True시 입력 원본 tensor에 바로 relu함수 적용하여 반환. 입력 tensor가 바로 relu 변환됨. 메모리 절감 \n",
    "relu_output = F.relu(input_tensor, inplace=False) # inplace=True\n",
    "print('relu_output:', relu_output)\n",
    "# inplace=False시 input_tensor 값은 변환 없음. inplace=True시 input_tensor 값이 변화됨. \n",
    "print('input_tensor:', input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relu_output: tensor([ 0.,  9., 10.,  0.,  0.])\n",
      "input_tensor: tensor([  0.,   9.,  10.,  -9., -10.])\n"
     ]
    }
   ],
   "source": [
    "# ReLU Layer 적용\n",
    "input_tensor = torch.tensor([0, 9.0, 10.0, -9.0, -10.0], dtype=torch.float32)\n",
    "relu_layer = nn.ReLU() #inplace=True\n",
    "relu_output = relu_layer(input_tensor)\n",
    "print('relu_output:', relu_output)\n",
    "print('input_tensor:', input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tensor shape: torch.Size([2, 3])\n",
      "softmax_output: tensor([[0.5741, 0.0777, 0.3482],\n",
      "        [0.2119, 0.2119, 0.5761]])\n"
     ]
    }
   ],
   "source": [
    "# softmax 함수 적용\n",
    "input_tensor = torch.tensor([[1.0, -1.0, 0.5], \n",
    "                             [0.5, 0.5, 1.5]], dtype=torch.float32)\n",
    "print('input_tensor shape:', input_tensor.shape)\n",
    "softmax_output = F.softmax(input_tensor, dim=-1)\n",
    "#softmax_output = torch.softmax(input_tensor, dim=-1)\n",
    "print('softmax_output:', softmax_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Model에서 Activation Function 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "#from torch import nn\n",
    "\n",
    "# Custom Model 생성. \n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 반드시 super()를 호출. \n",
    "        super().__init__()\n",
    "        #Linear Layer와 ReLU Layer 생성. \n",
    "        self.linear_01 = nn.Linear(in_features=10, out_features=5)\n",
    "        self.relu_01 = nn.ReLU()\n",
    "        self.linear_02 = nn.Linear(in_features=5, out_features=3)\n",
    "        \n",
    "    # 순방향 전파(Pass Forward) 기술.\n",
    "    def forward(self, x):\n",
    "        x = self.linear_01(x)\n",
    "        x = self.relu_01(x)\n",
    "        output = self.linear_02(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9990, -0.3729, -0.1594,  0.2836, -1.1234,  0.5575,  0.4875,  2.2787,\n",
      "          0.1437,  1.6156],\n",
      "        [ 0.1340,  0.2344,  1.3478, -0.8199, -1.3315,  0.7726,  0.9116,  0.4670,\n",
      "          1.1802,  0.3709],\n",
      "        [-2.0193, -1.1871, -0.2822, -0.2651,  2.1293,  1.7256,  0.1499,  1.2593,\n",
      "         -0.5026, -0.8897],\n",
      "        [-0.0430, -0.0234,  0.9300,  1.1032,  0.6404,  0.2513, -1.2607, -0.7816,\n",
      "         -0.0619, -0.6913]])\n",
      "tensor([[-0.4969,  0.4712,  0.1203],\n",
      "        [-0.3782,  0.3505,  0.1125],\n",
      "        [-0.1365,  0.5857, -0.2523],\n",
      "        [-0.2776,  0.3470, -0.0123]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#임의의 입력 tensor 생성. \n",
    "input_tensor = torch.randn(size=(4, 10))\n",
    "print(input_tensor)\n",
    "\n",
    "linear_model = LinearModel()\n",
    "\n",
    "# LinearModel 객체는 Callable Object이므로 LinearModel 객체에 함수 호출과 유사한 형태로 입력 인자 전달하여 forward()메소드 호출. \n",
    "output_tensor = linear_model(input_tensor)\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax_output: tensor([[0.1823, 0.4799, 0.3379],\n",
      "        [0.2125, 0.4404, 0.3471],\n",
      "        [0.2532, 0.5213, 0.2255],\n",
      "        [0.2397, 0.4477, 0.3126]], grad_fn=<SoftmaxBackward0>)\n",
      "predicted class: tensor([1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "softmax_output = F.softmax(output_tensor, dim=-1)\n",
    "#softmax_output = torch.softmax(input_tensor, dim=-1)\n",
    "print('softmax_output:', softmax_output)\n",
    "print('predicted class:', softmax_output.argmax(dim=-1))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
