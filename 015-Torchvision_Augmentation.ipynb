{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 테스트용 이미지 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!wget https://www.sciencenews.org/wp-content/uploads/2020/03/033120_HT_covid-cat_feat-1028x579.jpg\n",
    "!ls -lia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIL로 시각화 및 torch transforms 적용해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "org_img_pil = Image.open('033120_HT_covid-cat_feat-1028x579.jpg')\n",
    "\n",
    "def show_image(image):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "show_image(org_img_pil)\n",
    "print(type(org_img_pil))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "\n",
    "# 좌우 반전을 위한 transforms RandomHorizontalFlip 객체 생성. \n",
    "transform = T.RandomHorizontalFlip(p=1)\n",
    "\n",
    "# 좌우 반전 수행. 입력인자로 PIL 객체 또는 Tensor객체를 입력 받음.\n",
    "# 입력 PIL 시 좌우 반전 수행 완료된 PIL을 반환\n",
    "aug_img = transform(org_img_pil)\n",
    "\n",
    "print(type(aug_img))\n",
    "show_image(aug_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### opencv로 읽어들인 Numpy array는 transforms 에 적용할 수 없음\n",
    "* opencv의 imread(파일명)은 이미지 파일을 numpy 형태로 읽어들임. (높이, 너비, 채널수) 형태의 shape을 가지며 Channel Last임\n",
    "* imread()로 읽어들인 Channel 순서는 RGB가 아니라 BGR임. 이를 RGB로 변경하기 위해 cvtColor(array, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "org_img_numpy = cv2.cvtColor(cv2.imread('033120_HT_covid-cat_feat-1028x579.jpg'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def show_image(image):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    \n",
    "show_image(org_img_numpy)\n",
    "print(type(org_img_numpy), org_img_numpy.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms as T\n",
    "import cv2\n",
    "\n",
    "# opencv 는 image를 (h, w, c) 즉 channel last로 읽어 들임. 그리고 값은 unsigned int 형임.\n",
    "org_img_numpy = cv2.cvtColor(cv2.imread('033120_HT_covid-cat_feat-1028x579.jpg'), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(type(org_img_numpy), org_img_numpy.dtype)\n",
    "# 좌우 반전 객체 생성. \n",
    "transform = T.RandomHorizontalFlip(p=1)\n",
    "\n",
    "# transforms는 PIL 또는 pytorch tensor가 입력 되어야 만 함.\n",
    "aug_img = transform(org_img_numpy)\n",
    "\n",
    "print(type(aug_img))\n",
    "show_image(aug_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy array를 torchvision transforms에 부득이하게 사용해야 할 경우\n",
    "* numpy array를 PIL이나 Tensor로 변환\n",
    "* Tensor변환 시 Channel Last numpy array가 Channel First Tensor로 변환이 되어야 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Numpy array를 PIL로 변환 후 transforms 적용\n",
    "'''\n",
    "# opencv 는 image를 (h, w, c) 즉 channel last로 읽어 들임. 그리고 값은 unsigned int 형임.\n",
    "org_img_numpy = cv2.cvtColor(cv2.imread('033120_HT_covid-cat_feat-1028x579.jpg'), cv2.COLOR_BGR2RGB)\n",
    "# numpy array를 PIL 객체로 변환\n",
    "org_img_pil = Image.fromarray(org_img_numpy)\n",
    "\n",
    "# 좌우 반전 객체 생성. \n",
    "transform = T.RandomHorizontalFlip(p=1)\n",
    "aug_img = transform(org_img_pil)\n",
    "\n",
    "show_image(aug_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Numpy array를 Tensor로 변환 후 Transforms 적용. Channel Last -> Channel First 변경 및 float 32로 dtype 변경 \n",
    "'''\n",
    "import torch\n",
    "\n",
    "org_img_numpy = cv2.cvtColor(cv2.imread('033120_HT_covid-cat_feat-1028x579.jpg'), cv2.COLOR_BGR2RGB)\n",
    "# numpy를 tensor로 변환하되, channel first로 변환하고 uint8값을 float32로 변경해 줌. \n",
    "org_img_tensor = torch.tensor(org_img_numpy).permute(2, 0, 1).to(torch.float32)\n",
    "print(type(org_img_tensor), org_img_tensor.dtype, org_img_tensor.shape)\n",
    "\n",
    "# 좌우 반전 객체 생성. \n",
    "transform = T.RandomHorizontalFlip(p=1)\n",
    "aug_img_tensor = transform(org_img_tensor)\n",
    "print(type(aug_img_tensor), aug_img_tensor.dtype, aug_img_tensor.shape)\n",
    "\n",
    "# 시각화를 위해서 PIL로 다시 변환. ToPILImage() 적용시 tensor는 channel first이되 dtype은 uint8로 변경되어야 함. \n",
    "to_pil = T.ToPILImage()\n",
    "aug_pil_image = to_pil(aug_img_tensor.to(torch.uint8))\n",
    "\n",
    "print(type(aug_pil_image))\n",
    "# show_image(aug_img)\n",
    "plt.imshow(aug_pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "org_img_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation 적용 확률 적용 및 RandomHorizontalFlip과 RandomVerticalFlip\n",
    "* p 인자를 통해 해당 augmentation을 적용할지, 원본 image를 그대로 유지할 지의 확률 설정. \n",
    "* RandomHorizontalFlip은 좌우 반전, RandomVerticalFlip은 상하 반전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms as T\n",
    "#from torchvision.transforms import v2\n",
    "\n",
    "org_img_pil = Image.open('033120_HT_covid-cat_feat-1028x579.jpg')\n",
    "# 좌우 반전 적용\n",
    "transforms_h = T.RandomHorizontalFlip(p=0.5)\n",
    "aug_img = transforms_h(org_img_pil)\n",
    "\n",
    "show_image(aug_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 상하 반전 적용\n",
    "transform_v = T.RandomVerticalFlip(p=0.5)\n",
    "aug_img = transform_v(org_img_pil)\n",
    "show_image(aug_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 여러개의 이미지를 subplots로 시각화\n",
    "def show_images(images, labels, ncols=5, title=None):\n",
    "    figure, axs = plt.subplots(figsize=(28, 6), nrows=1, ncols=ncols)\n",
    "    for i in range(ncols):\n",
    "        axs[i].imshow(images[i])\n",
    "        axs[i].set_title(labels[i])\n",
    "\n",
    "# augmentor로 입력된 albumentations augmentation을 반복 수행\n",
    "def repeat_aug(count=4, org_image=None, label=None, transform=None):\n",
    "    image_list = [org_image]\n",
    "    label_list = ['original']\n",
    "\n",
    "    for i in range(count):\n",
    "        aug_image = transform(org_image)\n",
    "        image_list.append(aug_image)\n",
    "        label_list.append(label)\n",
    "        \n",
    "    show_images(image_list, label_list, ncols=count+1)\n",
    "    \n",
    "    \n",
    "transform = T.RandomHorizontalFlip(p=0.5)\n",
    "repeat_aug(count=4, org_image=org_img_pil, label='HorizontalFlip', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 상하 반전 연속 적용\n",
    "transform = T.RandomVerticalFlip(p=0.5)\n",
    "\n",
    "repeat_aug(count=4, org_image=org_img_pil, label='VerticalFlip', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compose() 를 이용하여 여러 Augmentation를 체인 형태로 연속 적용.\n",
    "* 인자로 연속으로 적용할 transforms 객체를 List로 입력함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomHorizontalFlip(p=0.5)\n",
    "    ])\n",
    "\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='Composite', transform=transform)\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='Composite', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CenterCrop, RandomCrop, RandomReSizedCrop\n",
    "* Crop은 입력된 height, width 값 만큼의 원본 이미지의 특정 영역을 잘라낸 후 원본 사이즈로 다시 Resize하지 않음.\n",
    "* CenterCrop은 원본 이미지의 중심을 기준으로 입력된 height, width 값 만큼의 영역을 Crop 함(원본 이미지 사이즈로 Resize하지 않음)\n",
    "* RandomCrop은 입력된 height, width 값 만큼의 영역을 Random하게 Crop.(원본 이미지 사이즈로 Resize하지 않음)\n",
    "* RandomResizedCrop은 원본 이미지의 Random한 영역을 잘라낸 후, 입력된 height, width값의 사이즈로 Resize함\n",
    "* RandomResizedCrop은 어떤 영역을 얼마만큼 잘라내는 지는 알수 없으며, 대략적으로 scale과 ratio를 조정하여 적용.\n",
    "* CenterCrop, RandomCrop, RandomResizedCrop 모두 Probability 인자를 가지지 않음. 이의 적용을 위하여 RandomApply 사용. \n",
    "* CenterCrop, RadomCrop은 적용 후 pytorch Model에 입력될 이미지 사이즈로 resize 적용 해야 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CenterCrop은 원본 이미지의 중심을 기반으로 size 만큼을 추출함.\n",
    "# probability를 인자로 가지고 있지 않음. RandomApply로 감싸서 호출이 필요. \n",
    "transform = T.CenterCrop(size=(224, 224)) #또는 size=224\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='CenterCrop', transform=transform)\n",
    "\n",
    "# probability를 인자로 가지고 있지 않음. RandomApply로 감싸서 호출이 필요. RandomApply()의 인자로 List필요하며 default p는 0.5임.\n",
    "transform = T.RandomApply([T.CenterCrop(size=(224, 224))], p=0.5)\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='CenterCrop', transform=transform)\n",
    "\n",
    "#최종 이미지가 만들어지므로 Resize를 이용하여 다른 이미지 크기로 재 변환이 필요할 수 있음.\n",
    "transform = T.RandomApply([T.CenterCrop(size=(224, 224)), T.Resize(size=(448, 448))], p=0.5)\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='CenterCrop', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compose 체인에 CenterCrop, RandomCrop, Crop등의 Crop계열 변환이 사용되었을 경우 고정된 이미지 크기를 위해서 맨 마지막에 Resize변환 필요\n",
    "transform = T.Compose([\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomApply([T.CenterCrop(size=(224, 224))], p=0.5),\n",
    "    T.Resize(size=(448, 448))\n",
    "    ])\n",
    "\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='CenterCrop_Comp', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# RandomCrop은 입력된 height, width 값 만큼의 영역을 Random하게 Crop.(원본 이미지 사이즈로 Resize하지 않음)\n",
    "transform = T.RandomCrop(size=(224, 224))\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomCrop', transform=transform)\n",
    "\n",
    "transform = T.RandomCrop(size=(1280, 1280), pad_if_needed=True) #  padding_mode는 'constant', 'edge', 'reflect', 'symmetric'\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomCrop', transform=transform)\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.RandomVerticalFlip(p=0.5),\n",
    "    T.RandomApply([T.RandomCrop(size=(224, 224))], p=0.5),\n",
    "    T.Resize(size=(448, 448))\n",
    "    ])\n",
    "\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomCrop_Comp', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# RandomResizedCrop은 원본 이미지의 Random한 영역을 잘라낸 후, 입력된 height, width값의 사이즈로 Resize함\n",
    "# RandomResizedCrop은 어떤 영역을 얼마만큼 잘라내는 지는 알수 없으며, 대략적으로 scale과 ratio를 조정하여 적용.\n",
    "# scale은 전체 image에서 잘라낼 비율영역(최소, 최대), ratio는 상하 비율 0.75=3:4(taller), 1.333=4:3(wider)\n",
    "transform = T.RandomResizedCrop(size=(224, 224)) # default scale은 (0.08, 1.0), ratio는 (0.75, 1.333)\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomResizedCrop', transform=transform)\n",
    "\n",
    "# scale인자를 적용하여 crop되는 위치 변동성을 줄일 수 있음.\n",
    "transform = T.RandomResizedCrop(size=(224, 224), scale=(0.3, 0.8))\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomResizedCrop', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomRotation\n",
    "* degrees에 정해진 범위내에서 회전. degrees=90 또는 (-90, 90) 이면 -90~90도 사이에 random 회전.\n",
    "* p 인자가 없으므로 RandomApply로 적용 해야 함. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = T.RandomRotation(degrees=90) # degrees=(-90, 90), expand=True이면 rotate 적용된 이미지가 모두 포함되도록 이미지 크기를 늘림\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='Rotate', transform=transform)\n",
    "\n",
    "transform = T.RandomApply([T.RandomRotation(degrees=90)], p=0.5)\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='Rotate', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomAffine\n",
    "* Rotation(회전), Shift(상하/좌우 이동), Scale(이미지 확장)을 한꺼번에 적용. 개별 적용은 Random 적용\n",
    "* p인자가 없으므로 RandomApply를 감싸서 적용. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = T.RandomAffine(degrees=30, # -30 ~ 30도 Random 회전\n",
    "                            translate=(0.1, 0.1), # horizontal과 vertical방향으로 Random shift를 이미지의 10%씩\n",
    "                            scale=(0.8, 1.2)) # 이미지를 80% ~ 120% 사이로 Random하게 영역 확장.  scale을 (0.1, 1.5)로 해볼것\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomAffine', transform=transform)\n",
    "\n",
    "# 개별 변환만 적용하려면, 해당 변환만 값을 입력하고, 나머지 변환은 None. 아래는 degrees만 적용. \n",
    "transform = T.RandomAffine(degrees=30, # -30 ~ 30도 Random 회전\n",
    "                            translate=None, # horizontal과 vertical방향으로 Random shift를 이미지의 10%씩\n",
    "                            scale=None) # 이미지를 80% ~ 120% 사이로 Random하게 영역 확장.  scale을 (0.1, 1.5)로 해볼것\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomAffine', transform=transform)\n",
    "\n",
    "transform = T.RandomApply([T.RandomAffine(degrees=30, \n",
    "                            translate=(0.1, 0.1),\n",
    "                            scale=(0.8, 1.2))], p=0.5) \n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomAffine', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColorJitter\n",
    "* 밝기(brightness), 대조(contrast), 채도(saturation), 색조(hue)를 함께 무작위로 변경.\n",
    "* 확률값 파라미터가 없으므로 RandomApply로 확률 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = T.ColorJitter(brightness=0.5, #밝기를 (1-0.5 ~ 1+0.5) 사이로, 최소값은 0임. [max(0, 1 - brightness), 1 + brightness] \n",
    "                           contrast=0.5, #대조를 (1-0.5 ~ 1+0.5) 사이로, 최소값은 0임.  \n",
    "                           saturation=0.5, #채도를 (1-0.5 ~ 1+0.5) 사이로, 최소값은 0임.\n",
    "                           hue=0.1 # 색조를 -0.1 ~ 0.1 사이로, 최소는 -0.5, 최대는 0.5\n",
    "                          )\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='ColorJitter', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianBlur\n",
    "* Blur는 전체적인 경계선을 흐리게 하거나 Noise를 제거하는 효과(Smoothing). blurring을 위한 filter 를 적용함. \n",
    "* 일반적인 Blur는 filter kernel size내의 픽셀값을 평균함. Gaussian Blur는 Gaussian 함수를 통해 kernel 값을 생성\n",
    "* kernel size가 클수록 이미지가 더 흐림. Gaussian blur의 경우는 kernel size가 홀수가 되어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = T.Compose(\n",
    "    [T.Resize(size=224), T.GaussianBlur(kernel_size=(7, 7))])\n",
    "repeat_aug(count=4, org_image=org_img_pil, label='Gaussian Blur', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomChoice를 이용하여 여러개의 변환 중 하나의 변환을 선택적으로 적용\n",
    "* 주로 강한 변환이 일어나는 transform들 중에 하나를 선택하기 위해서 사용됨.\n",
    "* RandomChoice는 인자로 주어진 transforms 중에 반드시 하나를 선택하므로 RandomChoice자체를 확률로 설정하기 위해서는 RandomApply를 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#CenterCrop, ColorJitter, Rotate 중에 하나를 0.5, 0.3, 0.2 의 확률로 선택(p가 list로 입력되어야 함. )\n",
    "transform = T.RandomChoice([T.CenterCrop(size=224),\n",
    "                             T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "                             T.RandomRotation(degrees=90)],\n",
    "                           p=[0.5, 0.3, 0.2])\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomChoice', transform=transform)\n",
    "\n",
    "transform = T.RandomApply([\n",
    "                           T.RandomChoice([T.CenterCrop(size=224),\n",
    "                                            T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1),\n",
    "                                            T.RandomRotation(degrees=90)],\n",
    "                                           p=[0.5, 0.3, 0.2])\n",
    "                           ], p=0.5)\n",
    "repeat_aug(count=6, org_image=org_img_pil, label='RandomChoice', transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToTensor와 Normalize 적용 시 유의사항\n",
    "* ToTensor() 입력 인자로 PIL 또는 Numpy array를 받아서 Tensor로 반환. Tensor가 입력 되어서는 안됨. 입력을 0 ~ 1 사이값으로 변환\n",
    "* Normalize()는 입력 인자로 반드시 Tensor가 입력되어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandomApply([T.RandomAffine(degrees=30, translate=(0.1, 0.1),scale=(0.8, 1.2))\n",
    "                   ], p=0.2),\n",
    "    T.ToTensor(),#Tensor 변환이 되지 않으면 Normalize는 오류 발생. \n",
    "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "aug_img = transform(org_img_pil)\n",
    "print(type(aug_img))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flowers 데이터 세트를 이용하여 이미지에 Augmentation 적용 및 모델 학습\n",
    "* 서로 다른 Augmentation을 적용하면서 모델 성능 영향이 어떻게 되는지 확인\n",
    "* 처음에는 Light한 Augmentation을, 이후에는 좀 더 Heavy한 Augmentation을 적용. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flowers 데이터 세트 다운로드 및 메타 데이터 생성\n",
    "* 총 3670장의 이미지로 구성되어 있으며 꽃 유형(daisy, dandelion, rose, sunflower, tulip)을 판별하기 위한 이미지 데이터 세트\n",
    "* 개별 디렉토리 명이 꽃이름으로 되어 있음\n",
    "* 전체의 70%를 학습, 30%를 테스트로 분할뒤, 다시 학습 데이터의 80%를 학습, 20%를 검증으로 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "def create_flowers_meta_df(file_dir):\n",
    "    paths = [] # 이미지 파일 경로 리스트\n",
    "    labels = [] # 꽃 종류\n",
    "    \n",
    "    # os.walk()를 이용하여 특정 디렉토리 밑에 있는 모든 하위 디렉토리를 모두 조사. \n",
    "    # kaggle/input/flowers-dataset 하위 디렉토리 밑에 jpg 확장자를 가진 파일이 모두 이미지 파일임\n",
    "    # kaggle/input/flowers-dataset 밑으로 하위 디렉토리 존재\n",
    "    for dirname, _, filenames in os.walk(file_dir):\n",
    "        for filename in filenames:\n",
    "            # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n",
    "            if '.jpg' in filename:\n",
    "                # 파일의 절대 경로를 file_path 변수에 할당. \n",
    "                file_path = dirname+'/'+ filename\n",
    "                paths.append(file_path)\n",
    "                \n",
    "                # 파일의 절대 경로에 daily, dandelion, roses, sunflowers, tulips에 따라 labels에 값 할당.\n",
    "                if 'daisy' in file_path:\n",
    "                    labels.append('daisy')\n",
    "                elif 'dandelion' in file_path:\n",
    "                    labels.append('dandelion')\n",
    "                elif 'roses' in file_path:\n",
    "                    labels.append('rose')\n",
    "                elif 'sunflowers' in file_path:\n",
    "                    labels.append('sunflowers')\n",
    "                elif 'tulips' in file_path:\n",
    "                    labels.append('tulips')\n",
    "    # DataFrame 메타 데이터 생성. \n",
    "    data_df = pd.DataFrame({'path':paths, \n",
    "                            'label':labels})\n",
    "    # Target값  변환\n",
    "    label_mapping = {'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflowers': 3, 'tulips': 4}\n",
    "    data_df['target'] = data_df['label'].map(label_mapping)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_df = create_flowers_meta_df('/kaggle/input/') # /kaggle/input/flowers-dataset\n",
    "\n",
    "# 전체 데이터 세트에서 학습(전체의 70%)과 테스트용(전체의 30%) 메타 정보 DataFrame 생성.\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.3, stratify=data_df['target'], random_state=2025)\n",
    "# 기존 학습 DataFrame을 다시 학습과 검증 DataFrame으로 분할. 80%가 학습, 20%가 검증\n",
    "tr_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['target'], random_state=2025)\n",
    "\n",
    "print(data_df.shape, train_df.shape, tr_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(data_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# 이미지를 시각화\n",
    "def show_grid_images(image_paths, labels, ncols=6):\n",
    "    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n",
    "    for i in range(ncols):\n",
    "        image = Image.open(image_paths[i])\n",
    "        axs[i].imshow(image)\n",
    "        axs[i].set_title(labels[i])\n",
    "\n",
    "daisy_image_paths = data_df[data_df['label']=='daisy']['path'].iloc[:6].tolist()\n",
    "daisy_labels = data_df[data_df['label']=='daisy']['label'].iloc[:6].tolist()\n",
    "show_grid_images(daisy_image_paths, daisy_labels, ncols=6)\n",
    "\n",
    "rose_image_paths = data_df[data_df['label']=='rose']['path'].iloc[:6].tolist()\n",
    "rose_labels = data_df[data_df['label']=='rose']['label'].iloc[:6].tolist()\n",
    "show_grid_images(rose_image_paths, rose_labels, ncols=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flower Custom Dataset 및 DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "class FlowerDataset(Dataset):\n",
    "    # 이미지 파일리스트, 타겟 파일리스트, transforms 등 이미지와 타겟 데이터 가공에 필요한 인자들을 입력 받음\n",
    "    def __init__(self, image_paths, targets=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "    \n",
    "    # 전체 건수를 반환\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    # idx로 지정된 하나의 image, label을 tensor 형태로 반환\n",
    "    def __getitem__(self, idx):\n",
    "        # PIL을 이용하여 이미지 로딩하고 PIL Image 객체 반환.\n",
    "        pil_image = Image.open(self.image_paths[idx])\n",
    "        # 보통은 transform이 None이 되는 경우는 거의 없음(Tensor 변환이라도 있음)\n",
    "        image = self.transform(pil_image)\n",
    "        \n",
    "        if self.targets is not None:\n",
    "            # 개별 target값을 tensor로 변환.\n",
    "            target = torch.tensor(self.targets[idx])\n",
    "            return image, target\n",
    "        # 테스트 데이터의 경우 targets가 입력 되지 않을 수 있으므로 이를 대비. \n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#학습과 검증용 DataLoader 생성 함수\n",
    "def create_tr_val_loader(tr_df, val_df, tr_transform, val_transform):\n",
    "    tr_dataset = FlowerDataset(image_paths=tr_df['path'].to_list(),\n",
    "                               targets=tr_df['target'].to_list(), transform=tr_transform)\n",
    "    val_dataset = FlowerDataset(image_paths=val_df['path'].to_list(),\n",
    "                               targets=val_df['target'].to_list(), transform=val_transform)\n",
    "    tr_loader= DataLoader(tr_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2*BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    \n",
    "    return tr_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "IMG_MEANS = [0.485, 0.456, 0.406] # ImageNet 데이터세트의 이미지 채널별 평균값\n",
    "IMG_STD = [0.229, 0.224, 0.225] # ImageNet 데이터세트의 이미지 채널별 표준편차값\n",
    "\n",
    "tr_transform = T.Compose([\n",
    "            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "])\n",
    "\n",
    "tr_loader, val_loader = create_tr_val_loader(tr_df=tr_df, val_df=val_df,\n",
    "                                             tr_transform=tr_transform, val_transform=val_transform)\n",
    "images, labels = next(iter(tr_loader))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 생성 후 학습 및 평가 수행\n",
    "* torchvision pretrained resnet 50 모델 생성\n",
    "* 서로 다른 유형의 Augmentation을 적용하면서 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "def create_resnet_model(model_name, num_classes=10, weights='DEFAULT'):\n",
    "    model = None\n",
    "    if model_name == 'resnet50':\n",
    "        model = models.resnet50(weights=weights)\n",
    "    elif model_name == 'resnet101':\n",
    "        model = models.resnet101(weights=weights)\n",
    "    \n",
    "    num_in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features=num_in_features, out_features=num_classes)\n",
    "\n",
    "    return model\n",
    "    \n",
    "resnet_model = create_resnet_model('resnet50', num_classes=5, weights='DEFAULT') #resnet50, resnet101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer 클래스 생성 및 적용\n",
    "* Trainer 클래스는 https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/main/modular/v1/utils.py?raw=true 로 download 후 import 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# /kaggle/working/modular/v1 디렉토리에 utils.py 파일 다운로드\n",
    "!rm -rf ./modular/v1\n",
    "!mkdir -p ./modular/v1\n",
    "!wget -O ./modular/v1/utils.py https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/main/modular/v1/utils.py?raw=true\n",
    "!ls ./modular/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# 반드시 system path를 아래와 같이 잡아줘야 함. \n",
    "sys.path.append('/kaggle/working')\n",
    "\n",
    "#아래가 수행되는지 반드시 확인\n",
    "from modular.v1.utils import Trainer, Predictor, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "\n",
    "# 5개의 꽃 종류\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "def train_flowers_with_aug(tr_df, val_df, tr_transform, val_transform, epochs=20):\n",
    "    tr_loader, val_loader = create_tr_val_loader(tr_df=tr_df, val_df=val_df, \n",
    "                                                 tr_transform=tr_transform, val_transform=val_transform)\n",
    "    model = create_resnet_model('resnet50', num_classes=NUM_CLASSES, weights='DEFAULT')\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    optimizer = Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # 여러 augmentation 비교를 위해서 고정된 패턴의 학습율 적용을 위해 StepLR 선택.\n",
    "    scheduler = StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "    # callbacks는 None\n",
    "    trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                      train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler, callbacks=None, \n",
    "                      device=device)\n",
    "    # 학습 및 평가.\n",
    "    history = trainer.fit(epochs)\n",
    "    #학습된 trainer와 history 반환. \n",
    "    return trainer, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Augmentation을 적용하지 않은 transform을 이용하여 학습 및 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "IMG_MEANS = [0.485, 0.456, 0.406] # ImageNet 데이터세트의 이미지 채널별 평균값\n",
    "IMG_STD = [0.229, 0.224, 0.225] # ImageNet 데이터세트의 이미지 채널별 표준편차값\n",
    "\n",
    "tr_transform = T.Compose([\n",
    "            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "])\n",
    "\n",
    "trainer, history = train_flowers_with_aug(tr_df, val_df, \n",
    "                                          tr_transform=tr_transform, val_transform=val_transform,\n",
    "                                          epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from modular.v1.utils import Predictor\n",
    "\n",
    "test_image_paths = test_df['path'].to_list()\n",
    "test_targets = test_df['target'].to_list()\n",
    "\n",
    "IMG_SIZE=224\n",
    "test_transform = T.Compose([\n",
    "                        T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "                        T.ToTensor(), \n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = FlowerDataset(image_paths=test_image_paths, \n",
    "                            targets=test_targets, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "trained_model = trainer.get_trained_model()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "predictor = Predictor(model=trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation을 적용하여 학습 및 평가수행 - 01\n",
    "* Flip 계열만 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tr_transform_aug_01 = T.Compose([\n",
    "            T.RandomHorizontalFlip(0.5),\n",
    "            T.RandomVerticalFlip(0.5),\n",
    "            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "])\n",
    "\n",
    "trainer, history = train_flowers_with_aug(tr_df=tr_df, val_df=val_df, \n",
    "                                          tr_transform=tr_transform_aug_01, val_transform=val_transform,\n",
    "                                          epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_model = trainer.get_trained_model()\n",
    "\n",
    "predictor = Predictor(model=trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation을 적용하여 학습 및 평가수행 - 02\n",
    "* Flip, Affine, ColorJitter 함께 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tr_transform_aug_02 = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.3),\n",
    "    T.RandomVerticalFlip(p=0.3),\n",
    "    T.RandomApply([T.RandomAffine(degrees=30, translate=(0.1, 0.1),scale=(0.8, 1.2))\n",
    "                   ], p=0.2),\n",
    "    T.RandomApply([T.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)], p=0.3),\n",
    "    T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "    \n",
    "])\n",
    "\n",
    "trainer, history = train_flowers_with_aug(tr_df=tr_df, val_df=val_df, \n",
    "                                          tr_transform=tr_transform_aug_02, val_transform=val_transform,\n",
    "                                          epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_model = trainer.get_trained_model()\n",
    "\n",
    "predictor = Predictor(model=trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation을 적용하여 학습 및 평가수행 - 03\n",
    "* CenterCrop(RandomResizedCrop)을 적용. Crop 계열은 강한 Augmentation. 좋은 결과를 얻을 수 있지만, 이미지 특성에 따라 성능이 떨어질 수도 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "tr_transform_aug_03 = T.Compose([\n",
    "    T.RandomHorizontalFlip(p=0.3),\n",
    "    T.RandomVerticalFlip(p=0.3),\n",
    "    T.RandomApply([T.CenterCrop(size=(200,200))], p=0.4),\n",
    "    T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "    T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "    \n",
    "])\n",
    "\n",
    "trainer, history = train_flowers_with_aug(tr_df=tr_df, val_df=val_df, \n",
    "                                          tr_transform=tr_transform_aug_03, val_transform=val_transform,\n",
    "                                          epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_model = trainer.get_trained_model()\n",
    "\n",
    "predictor = Predictor(model=trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5795628,
     "sourceId": 9519275,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
