{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST 데이터 기반으로 모델 생성 및 학습 수행\n",
    "* FashionMNIST Dataset 생성, 변환, DataLoader 구성 \n",
    "* Layer, 함수 그리고 Sequential을 활용한 네트웍 모델 생성하기\n",
    "* Loss와 Optimizer 이해\n",
    "* Training 로직(루프) 만들기\n",
    "    * Pytorch는 신경망 모델 생성과 학습(Training) 로직이 Loosely Coupled 되어 있음.\n",
    "    * 모델은 batch 단위로 입력된 학습 데이터 입력 tensor를 받아서 모델 구조에 맞춰서 출력 tensor로 변환하고 이를 반환하는데 촛점(Computational Graph 생성)\n",
    "    * Training 로직은 모델의 출력 tensor값을 Loss 함수 및 Optimizer를 이용하여 backpropagation 기반으로 Weight(가중치) 파라미터를 Update 수행. 학습 데이터의 batch 단위로 반복 수행 하며, 전체 학습 데이터를 epochs 지정된 횟수만큼 수행.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fashion MNIST Dataset 생성\n",
    "* torchvision.datasets에서 FashionMNIST Dataset 다운로드\n",
    "* FashionMNIST Dataset은 train=True이면 학습 Dataset, train=False이면 테스트 Dataset으로 만들어짐\n",
    "* 이미지 데이터가 PIL 형태로 되어 있음. torchvision.transforms의 ToTensor()를 이용하여 PIL을 tensor로 변환 필요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "train_data = datasets.FashionMNIST(root='data', train=True, download=True)#,transform=ToTensor()\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True)#,transform=ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.Image.Image image mode=L size=28x28 at 0x104208D10>, 9)\n",
      "<class 'tuple'>\n",
      "<class 'PIL.Image.Image'>\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "# 개별 Fashion MNIST 데이터는 PIL Image와 Label/Target 값 Tuple로 구성됨. \n",
    "print(train_data[0])\n",
    "print(type(train_data[0]))\n",
    "print(type(train_data[0][0]))\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tbw1oNx4m8QWmkWx2yXD4LkZCADJJ+gFbviL4a63oc7COE3MW4hdn38duD976jNc9daDqllIsc9lKrMu4YGeMkdR7gj8KzcV7H8BtEvV16+1iWCeG1Wz8mOV02pIzupwCeuAp6Z98cZ90aIzLIlw0c0ZJ4KgjHoeOa+evjS9n/wnMcNxBPCYLKONFhA2FNzMpGenDcgd816V4K03wefC+m3NlpVhP+5QSXBiR5fMx825iMg5zwce3FdbOzTwgW90lu6uCm8eYrL02soIyCPQgggEdMGQ3cluiPNK0rJwrRQBNueuMkt+teNfGKxsdY8WWdxNqcNo66eieXMwVsb5DnH415Hp2rajpE5n02/urOUjBe3laMkehIPIrVm8eeLrhNknibVivoLtx/I1UPinxC3XXtUP1vJP8ay5JZJpGkldnduSzHJP41//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACS0lEQVR4AWKgA2BkYOD1ZGBgZAHZxcjIAKZBbBBm+quS8v3rj1N/GBiZGP8wMKNIMv91cnnCzuU65+X/vww8/76hSP5iMFVgZtpp2HXm8nUz02PHGUHGQTHjf9cugd//GE7f+cUo8ft0yDSEJCMDw/8TCgyMf34x/Ph3/vYfT0VphLH/GRgY3kt+Z2fl+cH5z8aSSWwHqmsZuJiZvn18p/CPkYnr7z9ZBiaofQwMjMwMPFI/frH++sr/j537K9sldhOE5H9mhnBJJg4Gbtlf7L//cQhvusaCkGT5xXDlBxsXl6rSD2Yunr9PoraeYAGZx8T4+x/DHwaGbV+/s/1/zczxm+H3P2a9jwxMDMz///z6+Y+BwW7ime9v//z78/XrXw6GbwxsX4NAYc3AICSlJhmk/oPpN+czVjbhX1zHeOz+fWR9qcnIYNkkKvCX+cMfrl+M36+HneEVVGC4x/v5GycPHxcj83GpP3+/MTB/Z2DgF0lwy3z24/49VeFfrLxsf+UBY0xqv8vDw87Ayv/4mSiTRACHIrexMdMvJjYGRlYLlpeP+X485mHje/eQ5/uPP+svKwj9+vD77y/Wf4xsaixP/z/mFvnw5jULOysHL9Mbza+P37O/+f3nN6fERwOWC+sTn937wcPGwcb88+//by/+/WX5wfPrw4fffxRfMjIweBWLv/7wl5mNhZnxPysrGysjA+NLBrZ/EpfCGJn+MTA4tYnxMzGz/GV8+f/pvy/MDP9/f2Paff0YJBAYGBg0RN/LPPx1Fx5HFDIAaCTYdiCc4RIAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([60000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data.data), train_data.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST 이미지 시각화 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, '9')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_image = train_data[0][0]\n",
    "train_label = train_data[0][1]\n",
    "\n",
    "#PIL Image를 시각화 하기\n",
    "plt.imshow(train_image, cmap='gray')\n",
    "plt.title(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_images = [image for image, _ in train_data]\n",
    "train_labels = [label for _, label in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2200x600 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2200x600 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "def show_images(images, labels, ncols=8):\n",
    "    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n",
    "    for i in range(ncols):\n",
    "        axs[i].imshow(images[i], cmap='gray')\n",
    "        axs[i].set_title(class_names[labels[i]])\n",
    "        \n",
    "show_images(train_images[:8], train_labels[:8], ncols=8)\n",
    "show_images(train_images[8:16], train_labels[8:16], ncols=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset를 Batch 단위로 Fetch하는 DataLoader 생성.\n",
    "* DataLoader는 Dataset을 Batch 단위로 효과적이고 빠르게 모델에 전달하기 위한 역할을 수행\n",
    "    * dataset: 읽어들일 dataset 객체\n",
    "    * batch_size: 한번에 읽어들인 batch 크기\n",
    "    * shuffle: Training loop에서 epoch 완료 시마다 데이터를 섞을 지 여부\n",
    "    * num_workers: 로딩 병렬 작업 workers\n",
    "* DataLoader는 출력은 tensor 형태로 고정됨이 바람직. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# DataLoader 생성\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(dataset=test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 아래는 다음과 같은 오류 발생. DataLoader는 PIL이미지를 Batch화 시킬 수 없음.\n",
    "# TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape, images.dtype, labels.shape, labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data image, label type: <class 'torch.Tensor'> <class 'int'>\n",
      "torch.Size([32, 1, 28, 28]) torch.float32 torch.Size([32]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader의 반환값이 Tensor가 되도록 datasets.FashionMNIST 다운로드시 transform인자로 torchvision.transforms의 ToTensor() 적용\n",
    "train_data = datasets.FashionMNIST(root='data',train= True, download=True, transform=ToTensor())\n",
    "val_data = datasets.FashionMNIST(root='data',train= False, download=True, transform=ToTensor())\n",
    "\n",
    "# Dataset을 인덱싱으로 접근해서 가져갈 때마다 ToTensor()가 호출됨.  \n",
    "print('train_data image, label type:', type(train_data[0][0]), type(train_data[0][1]))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4 )\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape, images.dtype, labels.shape, labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0588, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.3961, 1.0000, 0.9020,  ..., 0.0000, 0.0078, 0.0000],\n",
       "          [0.0000, 0.0000, 0.2863,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          ...,\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images#images[0], images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model을 생성\n",
    "* Linear Model은 4차원으로 되어 있는 image batch tensor를 입력 받아서, 10개의 classification logit output을 만들어냄.\n",
    "* Linear Layer Model은 배치를 포함한 2차원 형태를 받아들이므로 개별 3차원 이미지(Grayscale image이므로 원래는 2차원 이미지 이지만, 이미지 자체를 3차원으로 표현)를 1차원 형태로 변환해야함. 이를 위해 nn.Flatten()을 적용.\n",
    "* Linear, Activation function 들을 모두 Layer로 선언한 뒤 이를 forward에서 이어 붙이는 방식, Linear 등 weight를 가지는 Layer들만 Layer로 선언하고 activation function, pooling 등은 functional 함수로 이어 붙이는 방식, Sequential을 활용하는 방식등으로 모델 구성. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer 기반으로 모델 생성\n",
    "* Linear Layer 뿐만 아니라 Activation도 Layer로 생성\n",
    "* Image Input Flatten -> Linear(input_size*input_size, 200)->ReLU->Linear(200, 100)->ReLU->Linear(100, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28]) torch.Size([32, 784])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Flatten 적용해보기\n",
    "flatten = nn.Flatten()\n",
    "flatten_output = flatten(images)\n",
    "print(images.shape, flatten_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleLinearModel_01(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_01 = nn.Linear(in_features=input_size*input_size, out_features=200)\n",
    "        self.act_01 = nn.ReLU()\n",
    "        self.linear_02 =nn.Linear(in_features=200, out_features=100)\n",
    "        self.act_02 = nn.ReLU()\n",
    "        self.linear_03 = nn.Linear(100, num_classes)\n",
    "        \n",
    "    # 입력 tensor -> Layer 적용된 출력 tensor -> 다음 Layer에 입력 tensor로 적용 -> ... -> 최종 출력 tensor 반환\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear_01(x)\n",
    "        x = self.act_01(x)\n",
    "        x = self.linear_02(x)\n",
    "        x = self.act_02(x)\n",
    "        output = self.linear_03(x)\n",
    "        # 반드시 tensor가 layer를 통과하면서 적용된 최종 tensor를 반환한다. \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleLinearModel_01(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_01): Linear(in_features=784, out_features=200, bias=True)\n",
      "  (act_01): ReLU()\n",
      "  (linear_02): Linear(in_features=200, out_features=100, bias=True)\n",
      "  (act_02): ReLU()\n",
      "  (linear_03): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 28\n",
    "NUM_CLASSES = 10 \n",
    "\n",
    "model_01 = SimpleLinearModel_01(input_size=INPUT_SIZE, num_classes=NUM_CLASSES)\n",
    "print(model_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n"
     ]
    }
   ],
   "source": [
    "#임의의 tensor를 입력\n",
    "input_tensor = torch.randn(4, 1, 28, 28)\n",
    "output = model_01(input_tensor)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# batch size만큼의 입력 Tensor를 모델에 입력 후 출력 Tensor의 shape확인\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4 )\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(images.shape)\n",
    "output = model_01(images)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name):depth-idx)             Input Shape               Output Shape              Param #\n",
       "========================================================================================================================\n",
       "SimpleLinearModel_01 (SimpleLinearModel_01)   [1, 1, 28, 28]            [1, 10]                   --\n",
       "├─Flatten (flatten): 1-1                      [1, 1, 28, 28]            [1, 784]                  --\n",
       "├─Linear (linear_01): 1-2                     [1, 784]                  [1, 200]                  157,000\n",
       "├─ReLU (act_01): 1-3                          [1, 200]                  [1, 200]                  --\n",
       "├─Linear (linear_02): 1-4                     [1, 200]                  [1, 100]                  20,100\n",
       "├─ReLU (act_02): 1-5                          [1, 100]                  [1, 100]                  --\n",
       "├─Linear (linear_03): 1-6                     [1, 100]                  [1, 10]                   1,010\n",
       "========================================================================================================================\n",
       "Total params: 178,110\n",
       "Trainable params: 178,110\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.18\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.71\n",
       "Estimated Total Size (MB): 0.72\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=model_01, input_size=(1, 1, 28, 28), \n",
    "        col_names=['input_size', 'output_size', 'num_params'],\n",
    "        row_settings=['var_names', 'depth'],\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear만 Layer로, activation은 relu()함수를 적용하여 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleLinearModel_02(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=10):\n",
    "        super().__init__()\n",
    "        # nn.Linear는 Layer를 그대로 사용. Linear Layer는 내부적으로 weight, bias등의 learnable parameter 값을 가지고 있기 때문\n",
    "        # 또한 모델 구성이 좀 더 가득성있고 편리하게 적용 가능. \n",
    "        self.linear_01 = nn.Linear(input_size * input_size, 200)\n",
    "        self.linear_02 = nn.Linear(200, 100)\n",
    "        self.linear_03 = nn.Linear(100, num_classes)\n",
    "\n",
    "     # 입력 tensor -> Layer 적용된 출력 tensor -> 다음 Layer에 입력 tensor로 적용 -> ... -> 최종 출력 tensor 반환\n",
    "    def forward(self, x):\n",
    "        # nn.Flatten()과 달리 torch.flatten()의 start_dim의 default 값은 0임. batch를 고려하여 flatten되도록 start_dim=1로 설정\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        x = F.relu(self.linear_01(x))\n",
    "        x = F.relu(self.linear_02(x))\n",
    "        output = self.linear_03(x)\n",
    "\n",
    "        #또는 아래와 같이 축약해서 사용할 수도 있음. 가독성이 떨어지므로 사용 자제\n",
    "        #x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        #output = self.linear_03(F.relu(self.linear_02(F.relu(self.linear_01(x))))) \n",
    "\n",
    "        # layer들을 통과하면서 적용된 최종 tensor를 반환한다. \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "print(images.shape)\n",
    "model_02 = SimpleLinearModel_02(input_size=INPUT_SIZE, num_classes=NUM_CLASSES)\n",
    "\n",
    "output = model_02(images)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name):depth-idx)             Input Shape               Output Shape              Param #\n",
       "========================================================================================================================\n",
       "SimpleLinearModel_02 (SimpleLinearModel_02)   [1, 1, 28, 28]            [1, 10]                   --\n",
       "├─Linear (linear_01): 1-1                     [1, 784]                  [1, 200]                  157,000\n",
       "├─Linear (linear_02): 1-2                     [1, 200]                  [1, 100]                  20,100\n",
       "├─Linear (linear_03): 1-3                     [1, 100]                  [1, 10]                   1,010\n",
       "========================================================================================================================\n",
       "Total params: 178,110\n",
       "Trainable params: 178,110\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.18\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.71\n",
       "Estimated Total Size (MB): 0.72\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=model_02, input_size=(1, 1, 28, 28), \n",
    "        col_names=['input_size', 'output_size', 'num_params'],\n",
    "        row_settings=['var_names', 'depth'],\n",
    "        #verbose=2\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential을 이용하여 모델 만들기\n",
    "* Sequential은 인자로 들어온 Layer나 Module들을 Sub Module하여 연속적으로 연결 시켜서 모듈화 시킬 수 있음\n",
    "* 반복되는 Layer들의 연결을 forward() 메소드에서 기술하지 않고, 블록 단위로 모듈을 효율적으로 생성할 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Sequential을 이용하여 모델 구성.\n",
    "class SimpleLinearModel_03(nn.Module):\n",
    "    def __init__(self, input_size, num_classes=10):\n",
    "        super().__init__()\n",
    "        # Sequential()의 인자로 Layer또는 Sub module 입력. \n",
    "        self.linear_block_01 = nn.Sequential(nn.Linear(in_features=input_size*input_size, out_features=200),\n",
    "                                              nn.ReLU(), \n",
    "                                              nn.Linear(200, 100),\n",
    "                                              nn.ReLU(),\n",
    "                                              nn.Linear(100, num_classes)\n",
    "                                            )\n",
    "\n",
    "    # 입력 tensor -> Layer 적용된 출력 tensor -> 다음 Layer에 입력 tensor로 적용 -> ... -> 최종 출력 tensor 반환\n",
    "    def forward(self, x):\n",
    "        # nn.Flatten()과 달리 torch.flatten()의 start_dim의 default 값은 0임. batch를 고려하여 flatten되도록 start_dim=1로 설정\n",
    "        x = torch.flatten(x, start_dim=1, end_dim=-1)\n",
    "        output = self.linear_block_01(x)\n",
    "\n",
    "        # layer들을 통과하면서 적용된 최종 tensor를 반환한다. \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 28\n",
    "NUM_CLASSES = 10 \n",
    "\n",
    "model_03 = SimpleLinearModel_03(input_size=INPUT_SIZE, num_classes=NUM_CLASSES)\n",
    "output = model_03(images)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name):depth-idx)             Input Shape               Output Shape              Param #\n",
       "========================================================================================================================\n",
       "SimpleLinearModel_03 (SimpleLinearModel_03)   [1, 1, 28, 28]            [1, 10]                   --\n",
       "├─Sequential (linear_block_01): 1-1           [1, 784]                  [1, 10]                   --\n",
       "│    └─Linear (0): 2-1                        [1, 784]                  [1, 200]                  157,000\n",
       "│    └─ReLU (1): 2-2                          [1, 200]                  [1, 200]                  --\n",
       "│    └─Linear (2): 2-3                        [1, 200]                  [1, 100]                  20,100\n",
       "│    └─ReLU (3): 2-4                          [1, 100]                  [1, 100]                  --\n",
       "│    └─Linear (4): 2-5                        [1, 100]                  [1, 10]                   1,010\n",
       "========================================================================================================================\n",
       "Total params: 178,110\n",
       "Trainable params: 178,110\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.18\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.71\n",
       "Estimated Total Size (MB): 0.72\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=model_03, input_size=(1, 1, 28, 28), \n",
    "        col_names=['input_size', 'output_size', 'num_params'],\n",
    "        row_settings=['var_names', 'depth'],\n",
    "        #verbose=2\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SimpleLinearModel 을 생성하는 함수 선언\n",
    "* create_simple_linear_model() 함수로 모델 생성 후 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 28\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "def create_simple_linear_model(input_size, num_classes=10):\n",
    "    model = SimpleLinearModel_02(input_size=input_size, num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 28, 28])\n",
      "tensor([3, 6, 0, 7, 7, 8, 3, 2, 8, 3, 5, 4, 6, 8, 8, 6, 6, 2, 4, 8, 1, 7, 9, 6,\n",
      "        7, 0, 5, 6, 0, 4, 6, 1])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# DataLoader의 반환값이 Tensor가 되도록 datasets.FashionMNIST 다운로드시 transform인자로 torchvision.transforms의 ToTensor() 적용\n",
    "train_data = datasets.FashionMNIST(root='data',train= True, download=True, transform=ToTensor())\n",
    "val_data = datasets.FashionMNIST(root='data',train= False, download=True, transform=ToTensor())\n",
    "\n",
    "# batch size만큼의 입력 Tensor를 모델에 입력 후 출력 Tensor의 shape확인\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4 )\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "print(images.shape)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss 함수(클래스)\n",
    "* Multiclass의 예측 모델의 경우 Loss는 Cross entropy Loss 적용. pytorch는 이를 위해 CrossEntropyLoss 클래스를 제공\n",
    "* CrossEntropyLoss는 입력으로 softmax가 적용되지 않은 logit 예측값과 Class Index 형태의 정수형 label(target)값을 받음(one-hot encoding 적용해서 입력하면 안됨)\n",
    "* 클래스 내부에서 입력 logit값에 대해서 softmax를 적용하므로 반드시 softmax가 적용되지 않은 logit 예측 값을 입력해 줘야함.\n",
    "* label(target)값의 경우 class 값(class index값 0 ~ C)으로 입력함. 클래스 내부에서 필요한 경우 one-hot encoding을 적용하므로, 입력 시에는 one-hot encoding을 적용해 주면 안됨.\n",
    "* 따라서 입력 인자로 softmax를 적용하지 않은 모델 출력값, one-hot encoding을 적용하지 않은 label(target) 값을 입력해 줌.\n",
    "* CrossEntropyLoss 클래스는 LogSoftmax와 NLLLoss가 결합된 형태임 LogSoftmax는 softmax에 log를 순차적으로 적용시 발생 할 수 있는 수치 불안정한 현상을 개선하여 Cross Entropy Loss 계산을 보다 안정적으로 수행 할 수 있게함. \n",
    "* CrossEntropyLoss의 출력은 계산된 Loss값을 가지고 있는 tensor임. loss tensor의 backward()메소드를 호출하면 Backpropagation을 수행하여 Gradient 값을 계산함. 학습 파라미터들은 적용될 Gradient값을 grad 속성에 가지고 있지만, 학습파라미터를 Update하지는 않음. 실제 Update는 Optimizer 에서 수행.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3118, grad_fn=<NllLossBackward0>) 2.311757802963257\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_linear_model(input_size=28, num_classes=10)\n",
    "# 모델 출력(logit 값 출력)\n",
    "output = model(images)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "# CrossEntropyLoss 객체 생성. \n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "# 모델의 출력값을 첫번째 인자로, labels 값을 두번째 인자로 입력.\n",
    "loss = loss_fn(output, labels)\n",
    "print(loss, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward():\n",
      "linear_01.weight.grad: None\n",
      "linear_01.bias.grad: None\n",
      "linear_02.weight.grad: None\n",
      "linear_02.bias.grad: None\n",
      "linear_03.weight.grad: None\n",
      "linear_03.bias.grad: None\n"
     ]
    }
   ],
   "source": [
    "print(\"Before backward():\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}.grad: {param.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3034, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After backward():\n",
      "linear_01.weight.grad: tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.5130e-04,\n",
      "         -1.0854e-04,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  3.2047e-06,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.4372e-04,\n",
      "         -1.0506e-04,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.8056e-05,\n",
      "          3.4475e-05,  0.0000e+00]])\n",
      "linear_01.bias.grad: tensor([-1.7917e-03,  1.8554e-03, -1.1963e-03,  9.7032e-05, -4.8209e-03,\n",
      "        -1.4740e-03, -1.6421e-03, -6.1733e-04, -1.6547e-04, -3.3368e-04,\n",
      "        -4.0641e-03,  2.4103e-03,  3.5170e-03, -2.0568e-03, -4.3742e-03,\n",
      "        -1.3403e-04,  1.3280e-03,  4.0586e-03,  1.3014e-03,  6.5254e-04,\n",
      "        -8.1962e-04,  1.3342e-03,  2.6678e-03, -1.4129e-03, -2.6994e-04,\n",
      "         2.5431e-04,  1.0406e-03, -7.4998e-05, -1.2936e-03,  0.0000e+00,\n",
      "        -1.0153e-03, -6.2193e-04, -4.9898e-04,  7.9181e-04,  1.7858e-04,\n",
      "         7.7113e-05, -8.5460e-04, -1.1669e-03, -5.8320e-04,  3.0432e-03,\n",
      "         7.6907e-06, -2.5712e-03,  6.1807e-04, -2.4782e-04,  2.2306e-03,\n",
      "        -2.3171e-03,  3.4762e-03,  1.4439e-03,  1.0690e-03, -1.0202e-04,\n",
      "         1.9401e-03, -1.2983e-03, -7.3130e-04,  5.8048e-04,  3.3459e-03,\n",
      "        -3.7270e-04,  1.1768e-03, -1.1800e-03, -1.8392e-03, -2.6462e-03,\n",
      "        -4.8762e-04, -9.3555e-04, -5.9326e-05,  0.0000e+00, -6.6147e-03,\n",
      "        -6.1405e-03,  0.0000e+00,  4.2713e-04, -5.2786e-03,  6.1618e-04,\n",
      "         3.4085e-04, -3.9348e-04, -1.2105e-03, -1.8100e-03, -3.3603e-04,\n",
      "        -6.0208e-05, -6.9176e-04,  1.8781e-03,  4.1470e-04,  2.0362e-03,\n",
      "         0.0000e+00,  5.6389e-04, -4.2393e-04, -4.5917e-03,  9.1300e-04,\n",
      "         1.7555e-03,  2.9808e-03,  4.3153e-04, -1.0198e-04,  2.4847e-04,\n",
      "        -1.2911e-04, -4.6153e-03,  7.6375e-04,  4.2254e-04,  6.6796e-04,\n",
      "         3.7747e-04,  2.9057e-04,  5.7569e-04, -2.3242e-03,  8.5501e-04,\n",
      "         5.5617e-04,  5.9898e-04, -4.6699e-03, -2.0639e-03,  7.8615e-04,\n",
      "         2.1030e-04, -2.5823e-03, -2.0336e-03, -5.2953e-03,  4.7473e-03,\n",
      "         1.9658e-04,  1.7372e-03, -1.0300e-03, -9.6125e-04,  0.0000e+00,\n",
      "        -2.1747e-04, -1.4870e-03,  1.7765e-03,  2.9203e-04, -9.4949e-05,\n",
      "         0.0000e+00, -9.6715e-04, -2.6440e-04,  2.9774e-03,  7.8499e-04,\n",
      "        -1.6211e-03, -7.3100e-04, -2.7986e-03,  1.4525e-03,  8.1137e-04,\n",
      "         0.0000e+00, -7.7659e-04,  1.5358e-03, -1.6272e-03, -2.3266e-03,\n",
      "         1.1347e-03,  1.2592e-03,  2.6302e-03,  9.6552e-05, -3.0340e-03,\n",
      "        -3.1832e-03,  2.3356e-03,  2.0230e-03, -1.8691e-03,  1.3621e-03,\n",
      "        -4.5617e-04, -5.0789e-04,  2.4725e-03, -3.3579e-04, -2.1032e-05,\n",
      "         1.6050e-03, -1.3112e-03,  2.4516e-03, -1.0350e-03, -1.9786e-03,\n",
      "        -1.2700e-04, -9.5327e-05,  1.7466e-04,  0.0000e+00,  3.2210e-03,\n",
      "        -2.1733e-03, -1.8571e-03, -1.8024e-03, -3.1163e-03,  2.5252e-03,\n",
      "        -1.7573e-03,  1.8675e-03, -2.5634e-04,  9.2747e-04,  6.9287e-04,\n",
      "        -1.0594e-04, -4.7902e-03,  2.8651e-04,  4.7944e-04,  0.0000e+00,\n",
      "         1.9335e-03, -1.9048e-03,  5.7760e-04,  0.0000e+00,  4.0166e-04,\n",
      "         3.3221e-03, -4.5017e-04, -3.7264e-03, -2.3417e-05, -3.0181e-04,\n",
      "         4.5756e-04, -9.6271e-04, -1.5771e-03, -2.3551e-04,  0.0000e+00,\n",
      "        -1.6145e-03,  6.1197e-03, -4.5046e-04,  1.5020e-03, -1.7854e-03,\n",
      "         1.5282e-03,  1.0320e-04, -1.9680e-03,  7.1901e-04,  6.0928e-04])\n",
      "linear_02.weight.grad: tensor([[ 7.6136e-04,  5.5419e-04, -7.0284e-04,  ...,  7.3241e-03,\n",
      "         -1.6228e-04, -3.5347e-05],\n",
      "        [ 5.8039e-04, -1.6059e-04, -1.3405e-04,  ..., -2.2911e-03,\n",
      "          6.9044e-05,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 1.0455e-03,  2.3957e-04,  9.9109e-05,  ...,  5.3546e-03,\n",
      "          0.0000e+00,  1.1926e-04],\n",
      "        [-2.3967e-03, -1.8185e-04,  0.0000e+00,  ..., -4.0860e-03,\n",
      "         -2.4165e-04, -4.1241e-04],\n",
      "        [-1.1742e-03,  1.4006e-04, -1.9063e-04,  ...,  1.6932e-03,\n",
      "         -5.4313e-04,  0.0000e+00]])\n",
      "linear_02.bias.grad: tensor([ 2.3027e-03,  5.4293e-03,  0.0000e+00,  9.7882e-03,  9.1276e-03,\n",
      "        -2.9980e-03,  3.4764e-03,  5.2837e-03,  3.4032e-03, -6.0379e-03,\n",
      "        -9.4388e-03,  1.8630e-04, -5.0727e-04,  9.2658e-03, -2.7724e-04,\n",
      "        -1.0013e-03,  1.1519e-03, -7.2734e-03,  0.0000e+00, -4.3188e-03,\n",
      "        -4.7270e-03, -5.7899e-03, -5.2453e-03, -4.4193e-03,  5.6659e-04,\n",
      "        -5.3706e-03,  1.4361e-03,  0.0000e+00, -4.9307e-03, -6.2851e-03,\n",
      "        -1.7592e-03,  7.6870e-03,  1.2953e-04, -1.2427e-02,  6.6859e-03,\n",
      "        -1.7117e-03, -4.0385e-03,  6.0454e-03,  6.8325e-03,  0.0000e+00,\n",
      "         2.8848e-03, -2.9798e-03,  0.0000e+00,  3.1339e-04,  1.2741e-03,\n",
      "        -2.1973e-03,  1.6125e-03, -5.9643e-03,  1.8371e-03, -6.5115e-03,\n",
      "         0.0000e+00,  6.2431e-03,  0.0000e+00, -6.2935e-04, -2.3296e-02,\n",
      "         1.3596e-03,  3.3434e-03,  4.3962e-03,  4.7529e-05,  8.4273e-03,\n",
      "         8.8169e-03,  2.2640e-03, -5.3095e-03, -1.2709e-02, -3.8990e-03,\n",
      "         1.8462e-02, -3.1244e-03, -5.3212e-03,  1.1891e-03,  1.8247e-02,\n",
      "        -2.7549e-03, -5.8078e-03,  4.4212e-04,  5.5316e-04, -7.7662e-03,\n",
      "         2.5161e-03, -3.4817e-03,  0.0000e+00,  1.4054e-03,  0.0000e+00,\n",
      "        -6.0243e-03, -1.3668e-02, -9.3140e-03,  2.5373e-03, -1.1129e-02,\n",
      "         0.0000e+00, -3.9661e-03, -1.0943e-03,  2.6219e-03,  0.0000e+00,\n",
      "         0.0000e+00,  6.8848e-04,  6.1763e-04,  8.2100e-04, -1.0982e-02,\n",
      "         1.2264e-03,  8.5307e-03,  1.4670e-02, -7.5249e-03,  5.4425e-03])\n",
      "linear_03.weight.grad: tensor([[ 7.3279e-03, -1.2382e-05,  0.0000e+00, -1.9364e-03,  2.4091e-03,\n",
      "          2.7652e-03,  3.5474e-04,  5.2998e-04,  2.4045e-03,  3.4906e-03,\n",
      "          3.1055e-03,  6.4751e-03,  4.3494e-05,  2.8160e-03,  2.3564e-05,\n",
      "          3.0759e-05,  1.2624e-03,  2.9537e-03,  0.0000e+00,  5.0623e-03,\n",
      "          2.0600e-03,  1.0285e-03,  7.7736e-03,  9.1921e-04,  2.8951e-03,\n",
      "          3.9709e-03,  8.7126e-03,  0.0000e+00,  1.6846e-03,  1.8927e-03,\n",
      "          1.2931e-02,  1.3148e-03,  1.1069e-02,  1.8112e-03,  6.8078e-03,\n",
      "          6.8578e-05,  5.9323e-03, -1.6000e-03,  1.9819e-03,  0.0000e+00,\n",
      "          5.5174e-03,  4.2893e-03,  0.0000e+00,  4.2676e-03,  5.6090e-05,\n",
      "          1.1105e-03,  1.5940e-03,  2.0314e-03,  3.9854e-05,  1.9934e-03,\n",
      "          0.0000e+00,  6.1184e-04,  0.0000e+00,  1.0758e-02,  1.4657e-03,\n",
      "          2.8530e-05,  8.0308e-03,  1.2928e-03,  2.3584e-05,  7.9772e-03,\n",
      "          4.6924e-03,  3.6199e-03,  3.3812e-05,  3.4632e-03,  3.9470e-03,\n",
      "          3.8608e-03,  1.2564e-02,  1.0741e-02,  7.6091e-04,  5.2211e-03,\n",
      "          1.7582e-04,  2.5863e-04,  5.9770e-05,  5.7929e-03,  4.8851e-03,\n",
      "          1.5698e-04,  8.4298e-04,  0.0000e+00,  4.4422e-05,  0.0000e+00,\n",
      "          2.7827e-03,  1.5960e-03,  4.5766e-03,  1.8084e-03,  1.2763e-03,\n",
      "          0.0000e+00,  4.2199e-03,  3.0922e-03,  3.2695e-03,  0.0000e+00,\n",
      "          0.0000e+00,  3.8273e-04,  3.4367e-03,  1.0701e-03,  1.0155e-03,\n",
      "          1.7970e-04,  1.5366e-03,  1.4473e-03,  5.7548e-03,  1.0570e-03],\n",
      "        [-3.4410e-03,  1.2274e-03,  0.0000e+00, -4.1481e-03, -5.9024e-03,\n",
      "         -2.0003e-04, -1.2693e-04, -3.0109e-04, -7.6706e-03, -2.2903e-03,\n",
      "         -5.4295e-03,  1.0845e-03,  4.2217e-05, -2.2777e-03,  2.2681e-05,\n",
      "          3.0304e-05,  6.6082e-04, -6.7692e-03,  0.0000e+00, -5.0595e-03,\n",
      "          1.7545e-04,  6.9450e-04,  7.9775e-03,  6.5590e-04, -6.4204e-03,\n",
      "         -1.3355e-02,  4.6383e-04,  0.0000e+00,  1.6122e-03,  4.0360e-03,\n",
      "         -7.2703e-03,  8.2519e-05, -1.2076e-02,  1.3551e-03, -1.0167e-02,\n",
      "         -3.6719e-06, -3.9806e-03,  1.9168e-03,  2.3468e-03,  0.0000e+00,\n",
      "         -8.5472e-03, -8.1352e-03,  0.0000e+00,  1.0112e-02,  5.5173e-05,\n",
      "          1.0751e-03, -7.9581e-04, -2.1588e-03,  3.8684e-05,  1.8922e-03,\n",
      "          0.0000e+00,  5.9307e-04,  0.0000e+00, -5.8568e-03,  2.2351e-04,\n",
      "          2.7461e-05, -1.0306e-02, -6.4145e-03,  2.3454e-05, -8.4096e-03,\n",
      "         -1.8821e-03, -2.9424e-03,  3.2862e-05, -1.6110e-02,  5.8498e-04,\n",
      "         -7.2066e-03, -2.7572e-04, -4.6858e-03,  3.4206e-03,  4.7104e-04,\n",
      "          1.7180e-04, -1.2771e-03,  5.8211e-05, -2.3998e-03, -1.6523e-03,\n",
      "          1.5361e-04,  4.2838e-04,  0.0000e+00,  4.2757e-05,  0.0000e+00,\n",
      "         -1.0049e-03, -1.5727e-03, -3.0135e-03, -4.1011e-03, -8.8883e-03,\n",
      "          0.0000e+00, -2.9863e-03,  2.7359e-04,  4.8630e-03,  0.0000e+00,\n",
      "          0.0000e+00,  3.1665e-03, -6.0476e-04,  3.1982e-03, -3.3917e-03,\n",
      "         -3.9964e-04,  2.2536e-04, -2.2564e-03, -9.0208e-03,  1.0305e-03],\n",
      "        [-4.5573e-03,  1.0507e-03,  0.0000e+00,  2.1603e-03,  8.5035e-04,\n",
      "          1.4953e-03,  3.5729e-04, -3.9454e-03, -3.0570e-03, -5.4061e-03,\n",
      "         -3.8448e-03, -9.0627e-03,  4.5236e-05, -2.3617e-04,  2.3251e-05,\n",
      "          3.1312e-05, -1.3037e-02,  9.9729e-04,  0.0000e+00, -1.0889e-02,\n",
      "         -2.8759e-03, -6.3357e-03, -6.9961e-03,  3.8449e-04,  2.2196e-03,\n",
      "          4.6747e-04, -1.4075e-02,  0.0000e+00,  1.6186e-03, -9.1059e-04,\n",
      "         -9.3131e-03, -1.0527e-04, -1.2031e-02, -1.1386e-03, -9.2205e-04,\n",
      "         -5.3219e-04, -9.6086e-03, -3.6621e-03, -3.7936e-03,  0.0000e+00,\n",
      "         -1.1520e-02, -1.1006e-02,  0.0000e+00, -1.8612e-02,  5.6030e-05,\n",
      "          8.0151e-04, -3.3623e-03,  1.7669e-03,  4.1451e-05, -7.7577e-04,\n",
      "          0.0000e+00, -4.2708e-03,  0.0000e+00, -9.9488e-03, -4.5095e-03,\n",
      "          2.8151e-05, -8.5721e-03,  1.3229e-03,  2.3645e-05, -6.8025e-03,\n",
      "         -4.5041e-03, -2.1088e-02, -1.1180e-04,  9.0005e-04, -3.8475e-03,\n",
      "         -4.3708e-03, -1.4398e-02, -1.7097e-02,  3.8043e-03, -7.8940e-03,\n",
      "          1.7568e-04,  2.6908e-04,  5.9314e-05, -6.1275e-03, -3.4120e-03,\n",
      "         -8.3487e-04, -1.7882e-05,  0.0000e+00,  4.3832e-05,  0.0000e+00,\n",
      "         -3.1707e-03,  1.5600e-03, -1.5246e-02, -3.1997e-03,  1.0328e-03,\n",
      "          0.0000e+00, -7.8448e-04, -4.4672e-03, -6.0328e-04,  0.0000e+00,\n",
      "          0.0000e+00, -7.8886e-04,  1.4323e-03, -3.2330e-03,  2.3063e-04,\n",
      "         -5.1086e-04, -1.2882e-02, -1.0591e-03, -1.1688e-02,  1.1117e-05],\n",
      "        [-4.1950e-03,  5.4995e-04,  0.0000e+00, -7.1615e-03,  1.2210e-03,\n",
      "          7.3236e-04,  3.5787e-04,  5.2424e-04, -1.0756e-02, -1.9201e-03,\n",
      "         -3.7536e-03, -4.7301e-03,  4.4663e-05, -8.1089e-04,  2.3351e-05,\n",
      "          3.1548e-05, -7.7042e-03, -1.7104e-03,  0.0000e+00, -7.1596e-03,\n",
      "         -1.6925e-03, -9.9721e-04,  5.2559e-03,  9.2591e-04,  5.2512e-04,\n",
      "         -6.5835e-03, -1.9450e-03,  0.0000e+00,  1.6160e-03,  3.0428e-03,\n",
      "         -8.5593e-03, -1.9308e-03, -3.5986e-03, -2.0545e-03, -1.0705e-02,\n",
      "          7.0325e-05,  4.8928e-04, -2.5636e-03, -1.6979e-03,  0.0000e+00,\n",
      "         -3.0584e-03, -6.3903e-03,  0.0000e+00, -4.5623e-03,  5.6525e-05,\n",
      "          1.1084e-03,  6.7234e-04,  1.9868e-03,  4.0925e-05,  1.9237e-03,\n",
      "          0.0000e+00, -3.0279e-05,  0.0000e+00,  1.9279e-03,  2.8262e-03,\n",
      "          2.8272e-05, -7.3338e-03, -1.2244e-04,  2.3931e-05, -5.7837e-04,\n",
      "         -2.6758e-03, -3.9212e-04,  3.4649e-05, -4.3240e-04, -7.2992e-04,\n",
      "          2.2772e-03, -1.4145e-03, -5.4860e-03,  3.3878e-03, -2.0752e-03,\n",
      "          1.7616e-04,  2.6354e-04,  5.9735e-05, -7.8921e-03,  2.8754e-03,\n",
      "          1.5521e-04, -3.4773e-03,  0.0000e+00,  4.4019e-05,  0.0000e+00,\n",
      "          5.6373e-04, -1.8887e-03, -5.5681e-03, -3.4738e-03,  4.3322e-04,\n",
      "          0.0000e+00, -6.7608e-03,  1.3084e-03,  6.0546e-03,  0.0000e+00,\n",
      "          0.0000e+00,  1.6140e-03,  2.9936e-04, -3.8675e-03, -6.4353e-04,\n",
      "          1.7834e-04, -3.5421e-03, -1.6536e-04, -8.2913e-04,  1.0475e-03],\n",
      "        [ 3.8102e-03,  1.2758e-03,  0.0000e+00,  1.7800e-05,  2.5135e-03,\n",
      "         -3.6512e-04,  3.6241e-04,  5.3541e-04, -2.1760e-03,  3.1112e-03,\n",
      "         -2.6520e-03,  4.4341e-03,  4.5832e-05,  2.5894e-03,  2.3494e-05,\n",
      "          3.1492e-05, -9.4640e-03,  3.5790e-03,  0.0000e+00, -1.0437e-03,\n",
      "          4.6442e-04, -4.8894e-04,  2.2146e-03,  9.4430e-04,  2.8616e-03,\n",
      "          2.6794e-03,  5.4240e-03,  0.0000e+00,  1.6751e-03,  4.8960e-03,\n",
      "         -7.4479e-04, -6.1666e-04,  3.5768e-03, -1.8338e-03,  3.0183e-03,\n",
      "          6.7991e-05,  3.6388e-03,  6.7925e-04, -1.4028e-03,  0.0000e+00,\n",
      "          1.3074e-03, -2.0320e-03,  0.0000e+00, -1.1556e-03,  5.9056e-05,\n",
      "          1.1350e-03, -3.4867e-03,  1.9893e-03,  4.1997e-05,  1.9178e-03,\n",
      "          0.0000e+00,  5.9585e-04,  0.0000e+00,  2.6010e-05,  2.0357e-04,\n",
      "          2.8445e-05, -1.8554e-03,  7.6680e-04,  2.5038e-05,  3.1721e-03,\n",
      "          3.7754e-03,  2.6449e-03,  3.4591e-05,  9.7907e-04, -1.9719e-03,\n",
      "          1.0571e-03,  9.5274e-05,  2.7426e-03,  4.2063e-03, -1.8831e-03,\n",
      "          1.8093e-04,  2.6838e-04,  6.0970e-05, -8.1615e-04, -7.8464e-04,\n",
      "         -4.1396e-04, -5.2387e-04,  0.0000e+00,  4.4289e-05,  0.0000e+00,\n",
      "          1.2256e-03,  2.4131e-03,  1.9930e-03,  1.8441e-03,  1.3385e-03,\n",
      "          0.0000e+00, -7.1461e-04,  1.2658e-03,  8.0431e-03,  0.0000e+00,\n",
      "          0.0000e+00,  6.5964e-04,  2.7371e-03, -1.4571e-03, -1.1390e-03,\n",
      "         -3.3321e-04, -1.7846e-03,  9.4722e-04, -2.0424e-03,  6.0974e-04],\n",
      "        [ 7.5529e-03, -1.9172e-03,  0.0000e+00,  2.3400e-03, -1.0682e-03,\n",
      "          1.8122e-03, -4.7749e-04,  4.7069e-04,  4.7039e-03,  4.6282e-03,\n",
      "          4.7864e-03,  6.6091e-03,  3.9236e-05,  7.9546e-04,  2.0324e-05,\n",
      "          2.7395e-05,  8.4089e-03,  9.8522e-04,  0.0000e+00,  9.7451e-03,\n",
      "          3.3236e-04,  2.5541e-03,  2.8435e-03, -2.2262e-03, -1.9037e-03,\n",
      "          2.4913e-03,  6.6267e-03,  0.0000e+00, -1.0387e-03,  1.3140e-03,\n",
      "          8.5973e-03,  1.1533e-03,  1.1725e-02,  1.6004e-03,  7.8635e-03,\n",
      "          6.0196e-05,  5.2794e-03,  1.6318e-03,  4.3793e-05,  0.0000e+00,\n",
      "          9.4370e-03,  8.4064e-03,  0.0000e+00,  1.2190e-02, -5.0630e-04,\n",
      "         -2.6158e-03,  1.3890e-03,  1.7116e-03,  3.5952e-05,  5.3280e-04,\n",
      "          0.0000e+00,  5.2950e-04,  0.0000e+00,  5.5462e-03,  3.4691e-03,\n",
      "          2.4608e-05,  5.0806e-03,  4.5881e-04,  2.1226e-05,  7.5088e-03,\n",
      "          5.0771e-03,  8.1941e-03,  3.0066e-05,  2.5348e-03,  1.4218e-03,\n",
      "          6.1765e-04,  1.0137e-02,  1.2488e-02,  3.1146e-03,  6.6845e-03,\n",
      "         -3.0899e-05, -8.2694e-04,  5.2353e-05,  8.1886e-03,  3.9337e-04,\n",
      "          1.3180e-04,  7.5112e-04,  0.0000e+00,  3.8315e-05,  0.0000e+00,\n",
      "         -2.8157e-03,  6.5687e-05,  9.1217e-03,  1.6417e-03,  1.1775e-03,\n",
      "          0.0000e+00,  4.3428e-03,  2.8543e-03, -4.8587e-04,  0.0000e+00,\n",
      "          0.0000e+00,  1.5121e-03,  2.2897e-04,  2.5718e-03,  1.2218e-03,\n",
      "          1.5809e-04,  8.2678e-03, -2.5243e-04,  6.8893e-03, -6.2566e-04],\n",
      "        [ 4.0953e-03,  1.1622e-03,  0.0000e+00,  8.7117e-04, -4.6587e-04,\n",
      "          7.0733e-04, -5.4233e-04,  4.9783e-04,  4.7057e-03,  1.3595e-03,\n",
      "          2.1680e-03,  2.6423e-05, -3.9327e-04, -2.7080e-03,  2.1120e-05,\n",
      "          2.8132e-05, -4.5960e-04, -3.7398e-04,  0.0000e+00,  3.2393e-04,\n",
      "         -2.6386e-03, -3.6640e-04,  1.9882e-03, -6.5166e-04,  9.5458e-04,\n",
      "          6.1601e-04,  1.7147e-03,  0.0000e+00,  1.5410e-03,  1.1733e-03,\n",
      "          2.8949e-03,  9.6141e-04, -8.0846e-04, -2.3032e-03,  4.7191e-03,\n",
      "          6.2955e-05,  1.2344e-03,  2.3451e-03,  8.1457e-04,  0.0000e+00,\n",
      "          1.1240e-03, -1.4292e-03,  0.0000e+00,  2.6582e-03,  5.1657e-05,\n",
      "          1.0140e-03,  1.5000e-03,  1.8663e-03, -3.6036e-04,  1.8267e-03,\n",
      "          0.0000e+00,  5.6543e-04,  0.0000e+00, -2.5752e-04,  2.5031e-03,\n",
      "          2.5571e-05, -1.6586e-03, -4.0387e-04,  2.1576e-05,  1.7354e-03,\n",
      "         -6.6600e-04, -1.9479e-03, -1.5832e-04, -1.7533e-03, -4.3347e-03,\n",
      "         -5.7086e-03,  2.1951e-03,  2.6284e-03,  3.9599e-03,  3.9453e-04,\n",
      "          1.6111e-04,  2.3873e-04,  5.3967e-05,  1.0562e-03, -3.5356e-04,\n",
      "          1.4882e-04,  7.9213e-04,  0.0000e+00,  3.9815e-05,  0.0000e+00,\n",
      "         -3.1783e-03,  1.6308e-03, -1.0931e-03, -2.4096e-04, -4.8836e-04,\n",
      "          0.0000e+00, -9.3477e-04,  3.7699e-03, -6.3198e-04,  0.0000e+00,\n",
      "          0.0000e+00,  2.8791e-03, -9.4948e-04, -1.3338e-03,  1.3494e-04,\n",
      "          1.5951e-04,  3.7014e-03, -1.0561e-03, -8.4811e-03,  3.8632e-04],\n",
      "        [ 5.3476e-03, -3.0056e-03,  0.0000e+00,  2.8591e-03, -2.9607e-03,\n",
      "         -6.9139e-03, -3.2340e-07,  5.3605e-04,  5.0261e-03,  3.0787e-03,\n",
      "          5.4871e-03,  4.6021e-03,  4.2502e-05, -4.4854e-03, -2.0566e-04,\n",
      "         -2.7428e-04,  9.5505e-03, -6.4661e-03,  0.0000e+00,  8.8447e-03,\n",
      "          3.3741e-04,  2.1930e-03, -4.4458e-03, -1.0283e-03, -1.1123e-04,\n",
      "          3.2621e-03,  1.3139e-03,  0.0000e+00, -1.9077e-03, -7.1075e-03,\n",
      "          4.4230e-03,  9.5204e-04,  6.2933e-03,  1.8014e-03,  4.4986e-03,\n",
      "          5.8759e-05, -1.4331e-03,  2.2258e-03,  1.7684e-03,  0.0000e+00,\n",
      "          6.4315e-03,  1.0144e-02,  0.0000e+00,  2.5738e-03,  5.5649e-05,\n",
      "         -3.1682e-03,  1.5962e-03,  5.8657e-05,  3.8945e-05, -6.0163e-04,\n",
      "          0.0000e+00,  6.0498e-04,  0.0000e+00, -4.8972e-05,  3.1488e-03,\n",
      "         -2.4900e-04,  7.8455e-03,  3.6593e-04, -2.1122e-04,  8.9435e-04,\n",
      "         -1.0592e-03,  9.0951e-03,  3.3155e-05,  2.8941e-03,  9.4912e-04,\n",
      "          3.3427e-03,  3.3505e-03,  8.6568e-03, -7.7464e-03,  6.6850e-03,\n",
      "         -1.0282e-03,  2.5482e-04, -5.2652e-04,  4.5077e-03, -7.6131e-04,\n",
      "          1.6029e-04,  8.2831e-04,  0.0000e+00, -3.8769e-04,  0.0000e+00,\n",
      "         -1.4566e-03, -5.7181e-03,  9.8002e-03,  1.8079e-03,  1.2982e-03,\n",
      "          0.0000e+00, -2.4486e-03,  3.3319e-03, -1.0942e-02,  0.0000e+00,\n",
      "          0.0000e+00, -5.5745e-06, -4.1808e-03,  8.7605e-04,  8.8189e-04,\n",
      "          1.7931e-04,  7.6664e-03, -5.2468e-04,  8.6330e-03, -2.4053e-03],\n",
      "        [ 3.3014e-03, -6.0843e-04,  0.0000e+00,  3.0987e-03,  9.1851e-04,\n",
      "          8.3330e-04,  3.7655e-04,  5.7394e-04,  3.3039e-03,  4.3212e-03,\n",
      "         -2.9815e-03,  3.4634e-03,  4.6681e-05,  3.0192e-03,  2.3851e-05,\n",
      "          3.2480e-05,  8.8628e-04,  3.1530e-03,  0.0000e+00,  5.4677e-03,\n",
      "          2.0802e-03,  2.7264e-03,  8.3645e-04, -2.6200e-04,  2.9974e-03,\n",
      "          4.3096e-03,  5.1262e-03,  0.0000e+00,  1.7115e-03,  3.4435e-03,\n",
      "          3.3058e-03, -1.6988e-03,  8.9594e-03,  1.4866e-03,  2.5284e-03,\n",
      "          7.2885e-05,  5.8660e-03, -7.3928e-04,  1.2201e-03,  0.0000e+00,\n",
      "          7.3390e-03,  5.2256e-03,  0.0000e+00,  2.7020e-03,  5.9659e-05,\n",
      "          1.1601e-03, -8.3720e-04,  2.0854e-03,  4.2775e-05,  2.0268e-03,\n",
      "          0.0000e+00,  1.3987e-04,  0.0000e+00,  3.1670e-03,  1.6833e-03,\n",
      "          2.8877e-05,  2.5348e-03,  1.4174e-03,  2.5086e-05,  6.5524e-03,\n",
      "          3.9851e-03,  3.9435e-04,  3.6030e-05,  3.8031e-03,  3.1485e-05,\n",
      "          3.3105e-03,  5.2148e-03,  8.9503e-03,  3.3503e-03,  1.2828e-03,\n",
      "         -1.6077e-04,  2.8058e-04,  6.1595e-05,  2.5027e-03,  2.1088e-04,\n",
      "          1.6924e-04, -5.1668e-04,  0.0000e+00,  4.4962e-05,  0.0000e+00,\n",
      "          3.5840e-03,  2.4851e-03,  2.2185e-04,  1.9730e-03,  1.4225e-03,\n",
      "          0.0000e+00, -1.3868e-03,  2.6448e-03,  6.3095e-03,  0.0000e+00,\n",
      "          0.0000e+00,  1.7615e-03,  3.4350e-03,  2.8941e-03,  1.9162e-04,\n",
      "          1.9495e-04, -4.4197e-04,  1.4190e-03,  3.8682e-03,  1.0892e-03],\n",
      "        [-1.9242e-02,  2.7750e-04,  0.0000e+00,  1.8989e-03,  2.4847e-03,\n",
      "         -8.6656e-04, -6.6179e-04,  5.7832e-04,  3.5150e-03, -1.0373e-02,\n",
      "          3.1143e-03, -1.2902e-02,  4.3412e-05,  1.2982e-03,  2.4020e-05,\n",
      "          3.0853e-05,  9.8959e-03,  3.6515e-03,  0.0000e+00, -5.2924e-03,\n",
      "          1.7570e-03, -1.0083e-03, -1.7448e-02,  3.3832e-04, -4.0182e-03,\n",
      "          2.1412e-03, -1.3362e-02,  0.0000e+00, -8.5125e-03, -1.1780e-02,\n",
      "         -6.2641e-03, -1.1254e-04, -1.3110e-02, -7.2461e-04, -7.6418e-03,\n",
      "          7.4172e-05, -7.4179e-03, -2.3387e-04, -1.2811e-03,  0.0000e+00,\n",
      "         -8.0303e-03,  9.2678e-04,  0.0000e+00, -1.0174e-02,  5.6459e-05,\n",
      "         -1.6205e-03,  1.7304e-03, -1.1338e-02,  3.9779e-05, -1.0736e-02,\n",
      "          0.0000e+00,  6.6049e-04,  0.0000e+00, -5.3126e-03, -1.1014e-02,\n",
      "          2.9082e-05,  6.2344e-03,  1.3162e-03,  2.3681e-05, -1.2050e-02,\n",
      "         -6.7428e-03,  2.4217e-03,  3.4955e-05,  3.7210e-03,  3.9496e-03,\n",
      "          2.8200e-03, -1.7468e-02, -1.8938e-02, -1.8258e-02, -8.8866e-03,\n",
      "          1.7836e-04,  2.7030e-04,  6.0601e-05, -4.8125e-03, -1.4009e-03,\n",
      "          1.7289e-04,  8.9286e-04,  0.0000e+00,  4.5281e-05,  0.0000e+00,\n",
      "          3.4702e-03, -5.7107e-04, -7.9297e-04,  1.9403e-03,  1.3977e-03,\n",
      "          0.0000e+00,  7.4536e-03, -1.4074e-02, -1.5877e-02,  0.0000e+00,\n",
      "          0.0000e+00, -1.1181e-02, -5.8343e-03, -7.1874e-04,  1.4979e-03,\n",
      "          1.9381e-04, -2.7470e-03,  1.5006e-03,  6.9158e-03, -2.2003e-03]])\n",
      "linear_03.bias.grad: tensor([ 0.0694, -0.0582, -0.0542,  0.0074,  0.0391,  0.0264, -0.0311, -0.0247,\n",
      "         0.0450, -0.0191])\n"
     ]
    }
   ],
   "source": [
    "print(\"After backward():\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}.grad: {param.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer 생성\n",
    "* pytorch의 torch.optim의 여러 클래스들은 다양한 Optimizer 기능을 제공\n",
    "* optimizer 클래스의 생성 인자로 model의 parameter들과 학습률(learning rate)을 입력\n",
    "* zero_grad() 메소드는 학습 파라미터의 grad 값을 초기화\n",
    "* step() 메소드는 학습 파라미터를 업데이트 시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adagrad, RMSprop, Adam\n",
    "\n",
    "sgd_optim = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "ada_grad_optim = Adagrad(model.parameters(), lr=0.001, eps=1e-9)\n",
    "rmsprop_optim = RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=1e-9)\n",
    "adam_optim = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# adam_optim.zero_grad() # gradient 값을 초기화. 수행하지 않으면 gradient값이 계속 누적 계산됨.\n",
    "# loss.backward() # backpropagation으로 학습 파라미터에 적용될 gradient 값 계산\n",
    "# adam_optim.step() # gradient 값을 학습 파라미터에 적용. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### gradients in initial state: None\n",
      "### gradient after loss.backward(): tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  6.9895e-07,  ...,  8.0349e-04,\n",
      "          1.9376e-04,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.7244e-04,\n",
      "         -8.9835e-05,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "### gradient after optimizer.zero_grad(): None\n"
     ]
    }
   ],
   "source": [
    "model = create_simple_linear_model(input_size=28, num_classes=10)\n",
    "# 모델 출력(logit 값 출력)\n",
    "output = model(images)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(output, labels)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))\n",
    "\n",
    "params = list(model.parameters())\n",
    "print('### gradients in initial state:', params[0].grad)\n",
    "# print('### params[0] in initial state:', params[0])\n",
    "\n",
    "loss.backward()\n",
    "print('### gradient after loss.backward():', params[0].grad)\n",
    "# print('### params[0] after loss.backward():', params[0])\n",
    "\n",
    "# optimizer.step()\n",
    "# print(\"### params[0] after optimizer.step():\", params[0])\n",
    "\n",
    "optimizer.zero_grad()\n",
    "print('### gradient after optimizer.zero_grad():', params[0].grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### device 설정하기\n",
    "* pytorch는 사용자가 GPU 또는 CPU를 사용할 지 명확하게 기술해 줘야함. 기술되지 않으면 기본적으로 CPU를 사용하며 모델 학습 및 예측을 수행.\n",
    "* 이를 위해 pytorch는 torch.device 객체를 모델과 모델과 Loss에 입력하는 tensor에 device를 기술해 줘야함(이외에 torchemetric을 사용하는 경우라면 metric에 device를 지정해 줘야하는등의 경우가 있지만, 기본적으로는 모델 자체와 모델에 입력하는 tensor에 device를 지정해 줘야함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tensor의 device는 기본적으로 cpu임. \n",
    "images, labels = next(iter(train_loader))\n",
    "print(type(images), images.device, type(labels), labels.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu <class 'torch.device'>\n",
      "<class 'torch.Tensor'> cpu <class 'torch.Tensor'> cpu\n",
      "<class 'torch.Tensor'> cpu <class 'torch.Tensor'> cpu\n"
     ]
    }
   ],
   "source": [
    "# 아래를 수행하기 전에 kaggle notebook의 Accelerate를 GPU로 설정. \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device, type(device))\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "print(type(images), images.device, type(labels), labels.device)\n",
    "\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "print(type(images), images.device, type(labels), labels.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model과 입력 tensor 모두 동일한 device에 있는 것이 중요. 서로 다른 device에 있을 경우 오류 발생. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 학습 데이터에서 image와 labels을 batch size 만큼 추출 \n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = create_simple_linear_model(input_size=28, num_classes=10)\n",
    "model = model.to(device)\n",
    "# model은 device가 cuda:0 이지만, images는 cpu임. \n",
    "print('image tensor device:', images.device) # model의 device는 model내 parameter의 device로 호출. next(model.parameters()).device)\n",
    "\n",
    "# images를 to('cuda') 하여야 오류 발생하지 않음. \n",
    "#images = images.to(device)\n",
    "#print(images.device)\n",
    "\n",
    "# 아래 코드는 오류를 발생. model의 device가 cuda인데, model에 입력하는 tensor의 device가 cpu가 될 수 없음.  \n",
    "pred = model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training 로직(루프) 만들기\n",
    "* Pytorch는 신경망 모델 생성과 학습(Training) 로직이 Loosely Coupled 되어 있음.\n",
    "* 모델은 batch 단위로 입력된 학습 데이터 입력 tensor를 받아서 모델 구조에 맞춰서 출력 tensor로 변환하고 이를 반환하는데 촛점(Computational Graph 생성)\n",
    "* Training 로직은 모델의 출력 tensor값을 Loss 함수 및 Optimizer를 이용하여 backpropagation 기반으로 Weight(가중치) 파라미터를 Update 수행. 학습 데이터의 batch 단위로 반복 수행 하며, 전체 학습 데이터를 epochs 지정된 횟수만큼 수행.\n",
    "* model은 training 모드임을 설정한 뒤(model.train()) training 시작. Batch Normalization이나 Drop Out 설정 시 검증/예측 모드와는 다르게 모델이 동작해야 하기 때문임.\n",
    "  \n",
    "![](https://github.com/chulminkw/CNN_PG_Torch/blob/main/image/Training_Loop.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1, 28, 28]), torch.float32, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습 데이터에서 image와 labels을 BATCH_SIZE 만큼 추출 \n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=4 )\n",
    "\n",
    "images, labels = next(iter(train_loader))\n",
    "images.shape, images.dtype, labels.shape, labels.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 번째 gradient update 수행중, loss는 2.2976982593536377\n",
      "101 번째 gradient update 수행중, loss는 0.7576976418495178\n",
      "201 번째 gradient update 수행중, loss는 0.5119157433509827\n",
      "301 번째 gradient update 수행중, loss는 0.8123482465744019\n",
      "401 번째 gradient update 수행중, loss는 0.6440306305885315\n",
      "501 번째 gradient update 수행중, loss는 0.6796685457229614\n",
      "601 번째 gradient update 수행중, loss는 0.5798084735870361\n",
      "701 번째 gradient update 수행중, loss는 0.6170673966407776\n",
      "801 번째 gradient update 수행중, loss는 0.5279263854026794\n",
      "901 번째 gradient update 수행중, loss는 0.35698163509368896\n",
      "1001 번째 gradient update 수행중, loss는 0.44524189829826355\n",
      "1101 번째 gradient update 수행중, loss는 0.4812728464603424\n",
      "1201 번째 gradient update 수행중, loss는 0.4569271504878998\n",
      "1301 번째 gradient update 수행중, loss는 0.6402689218521118\n",
      "1401 번째 gradient update 수행중, loss는 0.42868468165397644\n",
      "1501 번째 gradient update 수행중, loss는 0.5299511551856995\n",
      "1601 번째 gradient update 수행중, loss는 0.5428129434585571\n",
      "1701 번째 gradient update 수행중, loss는 0.26848939061164856\n",
      "1801 번째 gradient update 수행중, loss는 0.40846315026283264\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# training loop에 들어가기 전에 model에 device 설정하는 것이 좋음. \n",
    "model.to(device)\n",
    "\n",
    "# train 모드 설정 \n",
    "model.train()\n",
    "\n",
    "# 미니 배치 형태로 train loop 수행. train dataset은 60,000개, \n",
    "for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "    # 모델 입력 데이터 device 설정.\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # 이미지 tensor를 모델에 입력하여 출력 tensor(logit tensor) 출력  \n",
    "    pred = model(images)\n",
    "   \n",
    "    # loss 계산. CrossEntropyLoss는 내부에서 소프트맥스를 적용하므로 모델의 출력 결과에 소프트맥스를 적용하지 않아야 함. \n",
    "    loss = loss_fn(pred, labels)\n",
    "\n",
    "    # optimizer 초기화\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # backpropagation으로 Gradient 계산\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimizer가 학습 파라미터 Update\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_idx % 100 == 0:\n",
    "        print(f'{batch_idx+1} 번째 gradient update 수행중, loss는 {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수로 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 번째 batch gradient 적용, loss는 0.6938162446022034\n",
      "101 번째 batch gradient 적용, loss는 0.348589688539505\n",
      "201 번째 batch gradient 적용, loss는 0.16002213954925537\n",
      "301 번째 batch gradient 적용, loss는 0.4838063716888428\n",
      "401 번째 batch gradient 적용, loss는 0.3785673677921295\n",
      "501 번째 batch gradient 적용, loss는 0.41460978984832764\n",
      "601 번째 batch gradient 적용, loss는 0.3954102694988251\n",
      "701 번째 batch gradient 적용, loss는 0.2778903543949127\n",
      "801 번째 batch gradient 적용, loss는 0.49124544858932495\n",
      "901 번째 batch gradient 적용, loss는 0.2008000612258911\n",
      "1001 번째 batch gradient 적용, loss는 0.2410466969013214\n",
      "1101 번째 batch gradient 적용, loss는 0.47854527831077576\n",
      "1201 번째 batch gradient 적용, loss는 0.444202721118927\n",
      "1301 번째 batch gradient 적용, loss는 0.5956094264984131\n",
      "1401 번째 batch gradient 적용, loss는 0.5749620795249939\n",
      "1501 번째 batch gradient 적용, loss는 0.6672629117965698\n",
      "1601 번째 batch gradient 적용, loss는 0.24317024648189545\n",
      "1701 번째 batch gradient 적용, loss는 0.43577325344085693\n",
      "1801 번째 batch gradient 적용, loss는 0.27122125029563904\n"
     ]
    }
   ],
   "source": [
    "# 1 epoch 동안 train dataset을 미니 배치 단위로 모델을 학습 시킴. \n",
    "def train_step():\n",
    "    # 미니 배치 형태로 train loop 수행. train dataset은 60,000개\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        # 모델 입력 데이터 device 설정. \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 이미지 tensor를 모델에 입력하여 출력 tensor(logit tensor) 출력  \n",
    "        pred = model(images)\n",
    "        \n",
    "        # loss 계산. CrossEntropyLoss는 내부에서 소프트맥스를 적용하므로 모델의 출력 결과에 소프트맥스를 적용하지 않아야 함. \n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        #optimizer 초기화\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #backpropagation으로 Gradient 계산\n",
    "        loss.backward()\n",
    "        \n",
    "        #optimizer가 학습 파라미터 Update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 100 loop시 마다 아래 출력\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'{batch_idx+1} 번째 batch gradient 적용, loss는 {loss.item()}')\n",
    "\n",
    "train_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여러 epochs 동안 training loop 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## 1 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.4039299488067627\n",
      "101 번째 batch gradient 적용, loss는 0.19796788692474365\n",
      "201 번째 batch gradient 적용, loss는 0.23422081768512726\n",
      "301 번째 batch gradient 적용, loss는 0.5775916576385498\n",
      "401 번째 batch gradient 적용, loss는 0.17856915295124054\n",
      "501 번째 batch gradient 적용, loss는 0.2524421811103821\n",
      "601 번째 batch gradient 적용, loss는 0.2781696915626526\n",
      "701 번째 batch gradient 적용, loss는 0.44753801822662354\n",
      "801 번째 batch gradient 적용, loss는 0.3502524495124817\n",
      "901 번째 batch gradient 적용, loss는 0.32259055972099304\n",
      "1001 번째 batch gradient 적용, loss는 0.26233986020088196\n",
      "1101 번째 batch gradient 적용, loss는 0.3680242598056793\n",
      "1201 번째 batch gradient 적용, loss는 0.3525160849094391\n",
      "1301 번째 batch gradient 적용, loss는 0.22979041934013367\n",
      "1401 번째 batch gradient 적용, loss는 0.0710383951663971\n",
      "1501 번째 batch gradient 적용, loss는 0.30304840207099915\n",
      "1601 번째 batch gradient 적용, loss는 0.2980533242225647\n",
      "1701 번째 batch gradient 적용, loss는 0.41852349042892456\n",
      "1801 번째 batch gradient 적용, loss는 0.31595897674560547\n",
      "######## 2 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.36490631103515625\n",
      "101 번째 batch gradient 적용, loss는 0.19227425754070282\n",
      "201 번째 batch gradient 적용, loss는 0.5099907517433167\n",
      "301 번째 batch gradient 적용, loss는 0.23816023766994476\n",
      "401 번째 batch gradient 적용, loss는 0.2168572098016739\n",
      "501 번째 batch gradient 적용, loss는 0.2526734471321106\n",
      "601 번째 batch gradient 적용, loss는 0.29833024740219116\n",
      "701 번째 batch gradient 적용, loss는 0.22997775673866272\n",
      "801 번째 batch gradient 적용, loss는 0.34611138701438904\n",
      "901 번째 batch gradient 적용, loss는 0.2860875129699707\n",
      "1001 번째 batch gradient 적용, loss는 0.2873469591140747\n",
      "1101 번째 batch gradient 적용, loss는 0.424015611410141\n",
      "1201 번째 batch gradient 적용, loss는 0.1334020048379898\n",
      "1301 번째 batch gradient 적용, loss는 0.40609210729599\n",
      "1401 번째 batch gradient 적용, loss는 0.1205689013004303\n",
      "1501 번째 batch gradient 적용, loss는 0.505409836769104\n",
      "1601 번째 batch gradient 적용, loss는 0.4785616397857666\n",
      "1701 번째 batch gradient 적용, loss는 0.23090283572673798\n",
      "1801 번째 batch gradient 적용, loss는 0.17244432866573334\n",
      "######## 3 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.125837504863739\n",
      "101 번째 batch gradient 적용, loss는 0.22656933963298798\n",
      "201 번째 batch gradient 적용, loss는 0.35357293486595154\n",
      "301 번째 batch gradient 적용, loss는 0.20063908398151398\n",
      "401 번째 batch gradient 적용, loss는 0.33132505416870117\n",
      "501 번째 batch gradient 적용, loss는 0.19229064881801605\n",
      "601 번째 batch gradient 적용, loss는 0.468763530254364\n",
      "701 번째 batch gradient 적용, loss는 0.24771039187908173\n",
      "801 번째 batch gradient 적용, loss는 0.35586628317832947\n",
      "901 번째 batch gradient 적용, loss는 0.312820166349411\n",
      "1001 번째 batch gradient 적용, loss는 0.3349778354167938\n",
      "1101 번째 batch gradient 적용, loss는 0.32340532541275024\n",
      "1201 번째 batch gradient 적용, loss는 0.3278157114982605\n",
      "1301 번째 batch gradient 적용, loss는 0.24036812782287598\n",
      "1401 번째 batch gradient 적용, loss는 0.24994201958179474\n",
      "1501 번째 batch gradient 적용, loss는 0.48486778140068054\n",
      "1601 번째 batch gradient 적용, loss는 0.3821420669555664\n",
      "1701 번째 batch gradient 적용, loss는 0.19187398254871368\n",
      "1801 번째 batch gradient 적용, loss는 0.37636733055114746\n",
      "######## 4 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.15034182369709015\n",
      "101 번째 batch gradient 적용, loss는 0.24060145020484924\n",
      "201 번째 batch gradient 적용, loss는 0.19088131189346313\n",
      "301 번째 batch gradient 적용, loss는 0.4133903980255127\n",
      "401 번째 batch gradient 적용, loss는 0.5173112750053406\n",
      "501 번째 batch gradient 적용, loss는 0.14849647879600525\n",
      "601 번째 batch gradient 적용, loss는 0.4540599286556244\n",
      "701 번째 batch gradient 적용, loss는 0.22282947599887848\n",
      "801 번째 batch gradient 적용, loss는 0.2402770072221756\n",
      "901 번째 batch gradient 적용, loss는 0.27302223443984985\n",
      "1001 번째 batch gradient 적용, loss는 0.29359570145606995\n",
      "1101 번째 batch gradient 적용, loss는 0.17819492518901825\n",
      "1201 번째 batch gradient 적용, loss는 0.39807915687561035\n",
      "1301 번째 batch gradient 적용, loss는 0.3341413736343384\n",
      "1401 번째 batch gradient 적용, loss는 0.19552922248840332\n",
      "1501 번째 batch gradient 적용, loss는 0.3523732125759125\n",
      "1601 번째 batch gradient 적용, loss는 0.16932672262191772\n",
      "1701 번째 batch gradient 적용, loss는 0.34355878829956055\n",
      "1801 번째 batch gradient 적용, loss는 0.3857150375843048\n",
      "######## 5 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.6564304828643799\n",
      "101 번째 batch gradient 적용, loss는 0.2977694869041443\n",
      "201 번째 batch gradient 적용, loss는 0.47376060485839844\n",
      "301 번째 batch gradient 적용, loss는 0.31161922216415405\n",
      "401 번째 batch gradient 적용, loss는 0.40779584646224976\n",
      "501 번째 batch gradient 적용, loss는 0.3034712076187134\n",
      "601 번째 batch gradient 적용, loss는 0.24011127650737762\n",
      "701 번째 batch gradient 적용, loss는 0.28098589181900024\n",
      "801 번째 batch gradient 적용, loss는 0.304882287979126\n",
      "901 번째 batch gradient 적용, loss는 0.1775187849998474\n",
      "1001 번째 batch gradient 적용, loss는 0.39931720495224\n",
      "1101 번째 batch gradient 적용, loss는 0.24157702922821045\n",
      "1201 번째 batch gradient 적용, loss는 0.28516873717308044\n",
      "1301 번째 batch gradient 적용, loss는 0.3268846869468689\n",
      "1401 번째 batch gradient 적용, loss는 0.20896369218826294\n",
      "1501 번째 batch gradient 적용, loss는 0.16915932297706604\n",
      "1601 번째 batch gradient 적용, loss는 0.6766839027404785\n",
      "1701 번째 batch gradient 적용, loss는 0.17600026726722717\n",
      "1801 번째 batch gradient 적용, loss는 0.4220241606235504\n",
      "######## 6 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.29692888259887695\n",
      "101 번째 batch gradient 적용, loss는 0.17381320893764496\n",
      "201 번째 batch gradient 적용, loss는 0.14315028488636017\n",
      "301 번째 batch gradient 적용, loss는 0.28698089718818665\n",
      "401 번째 batch gradient 적용, loss는 0.24696360528469086\n",
      "501 번째 batch gradient 적용, loss는 0.25094443559646606\n",
      "601 번째 batch gradient 적용, loss는 0.2744806408882141\n",
      "701 번째 batch gradient 적용, loss는 0.4127253592014313\n",
      "801 번째 batch gradient 적용, loss는 0.3130183219909668\n",
      "901 번째 batch gradient 적용, loss는 0.12022977322340012\n",
      "1001 번째 batch gradient 적용, loss는 0.2892412543296814\n",
      "1101 번째 batch gradient 적용, loss는 0.31228742003440857\n",
      "1201 번째 batch gradient 적용, loss는 0.12349429726600647\n",
      "1301 번째 batch gradient 적용, loss는 0.2145930677652359\n",
      "1401 번째 batch gradient 적용, loss는 0.5760102272033691\n",
      "1501 번째 batch gradient 적용, loss는 0.43187350034713745\n",
      "1601 번째 batch gradient 적용, loss는 0.35563671588897705\n",
      "1701 번째 batch gradient 적용, loss는 0.085396409034729\n",
      "1801 번째 batch gradient 적용, loss는 0.10621265321969986\n",
      "######## 7 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.23170460760593414\n",
      "101 번째 batch gradient 적용, loss는 0.44459623098373413\n",
      "201 번째 batch gradient 적용, loss는 0.22643010318279266\n",
      "301 번째 batch gradient 적용, loss는 0.09584508091211319\n",
      "401 번째 batch gradient 적용, loss는 0.39000245928764343\n",
      "501 번째 batch gradient 적용, loss는 0.13911600410938263\n",
      "601 번째 batch gradient 적용, loss는 0.17439690232276917\n",
      "701 번째 batch gradient 적용, loss는 0.1739434450864792\n",
      "801 번째 batch gradient 적용, loss는 0.31738534569740295\n",
      "901 번째 batch gradient 적용, loss는 0.3901907801628113\n",
      "1001 번째 batch gradient 적용, loss는 0.33370545506477356\n",
      "1101 번째 batch gradient 적용, loss는 0.29981353878974915\n",
      "1201 번째 batch gradient 적용, loss는 0.48318102955818176\n",
      "1301 번째 batch gradient 적용, loss는 0.14477865397930145\n",
      "1401 번째 batch gradient 적용, loss는 0.13849420845508575\n",
      "1501 번째 batch gradient 적용, loss는 0.11899929493665695\n",
      "1601 번째 batch gradient 적용, loss는 0.15917296707630157\n",
      "1701 번째 batch gradient 적용, loss는 0.22560063004493713\n",
      "1801 번째 batch gradient 적용, loss는 0.6751691699028015\n",
      "######## 8 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.20582272112369537\n",
      "101 번째 batch gradient 적용, loss는 0.35066160559654236\n",
      "201 번째 batch gradient 적용, loss는 0.1648637056350708\n",
      "301 번째 batch gradient 적용, loss는 0.20061199367046356\n",
      "401 번째 batch gradient 적용, loss는 0.34644779562950134\n",
      "501 번째 batch gradient 적용, loss는 0.19784964621067047\n",
      "601 번째 batch gradient 적용, loss는 0.4164178967475891\n",
      "701 번째 batch gradient 적용, loss는 0.3410380184650421\n",
      "801 번째 batch gradient 적용, loss는 0.18288184702396393\n",
      "901 번째 batch gradient 적용, loss는 0.32058200240135193\n",
      "1001 번째 batch gradient 적용, loss는 0.14685998857021332\n",
      "1101 번째 batch gradient 적용, loss는 0.24262316524982452\n",
      "1201 번째 batch gradient 적용, loss는 0.07549162209033966\n",
      "1301 번째 batch gradient 적용, loss는 0.23253226280212402\n",
      "1401 번째 batch gradient 적용, loss는 0.15817193686962128\n",
      "1501 번째 batch gradient 적용, loss는 0.08362727612257004\n",
      "1601 번째 batch gradient 적용, loss는 0.22757552564144135\n",
      "1701 번째 batch gradient 적용, loss는 0.19026827812194824\n",
      "1801 번째 batch gradient 적용, loss는 0.3687448799610138\n",
      "######## 9 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.2074972689151764\n",
      "101 번째 batch gradient 적용, loss는 0.07552159577608109\n",
      "201 번째 batch gradient 적용, loss는 0.24877819418907166\n",
      "301 번째 batch gradient 적용, loss는 0.18823650479316711\n",
      "401 번째 batch gradient 적용, loss는 0.20776747167110443\n",
      "501 번째 batch gradient 적용, loss는 0.13658030331134796\n",
      "601 번째 batch gradient 적용, loss는 0.2269584983587265\n",
      "701 번째 batch gradient 적용, loss는 0.16719625890254974\n",
      "801 번째 batch gradient 적용, loss는 0.04022917523980141\n",
      "901 번째 batch gradient 적용, loss는 0.3876086175441742\n",
      "1001 번째 batch gradient 적용, loss는 0.26069873571395874\n",
      "1101 번째 batch gradient 적용, loss는 0.14904826879501343\n",
      "1201 번째 batch gradient 적용, loss는 0.25248444080352783\n",
      "1301 번째 batch gradient 적용, loss는 0.17949865758419037\n",
      "1401 번째 batch gradient 적용, loss는 0.07079122215509415\n",
      "1501 번째 batch gradient 적용, loss는 0.22178873419761658\n",
      "1601 번째 batch gradient 적용, loss는 0.17473194003105164\n",
      "1701 번째 batch gradient 적용, loss는 0.24790649116039276\n",
      "1801 번째 batch gradient 적용, loss는 0.31693553924560547\n",
      "######## 10 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.3628687262535095\n",
      "101 번째 batch gradient 적용, loss는 0.21434424817562103\n",
      "201 번째 batch gradient 적용, loss는 0.18534904718399048\n",
      "301 번째 batch gradient 적용, loss는 0.290518581867218\n",
      "401 번째 batch gradient 적용, loss는 0.25684964656829834\n",
      "501 번째 batch gradient 적용, loss는 0.1747659146785736\n",
      "601 번째 batch gradient 적용, loss는 0.1515459418296814\n",
      "701 번째 batch gradient 적용, loss는 0.2374461442232132\n",
      "801 번째 batch gradient 적용, loss는 0.07348094880580902\n",
      "901 번째 batch gradient 적용, loss는 0.15453290939331055\n",
      "1001 번째 batch gradient 적용, loss는 0.22103270888328552\n",
      "1101 번째 batch gradient 적용, loss는 0.15801458060741425\n",
      "1201 번째 batch gradient 적용, loss는 0.15525509417057037\n",
      "1301 번째 batch gradient 적용, loss는 0.20495975017547607\n",
      "1401 번째 batch gradient 적용, loss는 0.32762259244918823\n",
      "1501 번째 batch gradient 적용, loss는 0.3072241246700287\n",
      "1601 번째 batch gradient 적용, loss는 0.4806956350803375\n",
      "1701 번째 batch gradient 적용, loss는 0.4160027503967285\n",
      "1801 번째 batch gradient 적용, loss는 0.23698313534259796\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# model에 device 설정\n",
    "model = model.to(device)\n",
    "\n",
    "# train 모드 설정 \n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"######## {epoch+1} 번째 train epoch 시작\")\n",
    "    train_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증 데이터 기반 evaluation 수행. \n",
    "* Training Loop 내에서 Batch 단위로 학습된 model에 대해서 Batch 단위 검증 데이터로 모델의 Loss 및 성능 메트릭을 출력하여 확인\n",
    "* 학습 데이터의 성능 메트릭과 검증 데이터의 성능 메트릭을 비교하여 오버피팅이 어느 정도 진행되고 있는 확인\n",
    "* model은 evaluation 모드임을 설정한 뒤(model.eval()) evaluation 시작. Batch Normalization이나 Drop Out 설정 시 학습 모드와는 다르게 모델이 동작해야 하기 때문임.\n",
    "* evaluation은 모델의 출력값을 기반으로 하므로 별도의 Backpropagation을 Gradient Tracking이 필요하지 않음. 기본적으로 pytorch 모델은 Forward pass 시 Gradient Tracking을 requires_grad=True인 Tensor에 수행하므로 evaluation 시에는 수행 속도 향상을 위해서 이의 적용을 Disable 시킴. 이를 위해 with torch.no_grad() 컨텍스트를 적용.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "# validation은 shuffle을 할 필요 없음. 일반적으로 속도 향상을 위해 BATCH_SIZE도 학습 시 보다 크게 설정. \n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 valid loss는 0.30266273021698\n",
      "100 번째 valid loss는 0.26760828495025635\n",
      "200 번째 valid loss는 0.27434566617012024\n",
      "300 번째 valid loss는 0.17560608685016632\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# model에 device 설정\n",
    "model = model.to(device)\n",
    "\n",
    "# evaluation 모드 설정\n",
    "model.eval()\n",
    "\n",
    "# 모든 tensor가 Gradient Tracking을 하지 않도록 torch.no_grad() 컨텍스트를 적용. \n",
    "with torch.no_grad():\n",
    "    # 미니 배치 형태로 검증 loop 수행.\n",
    "    for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "        # 모델 입력 데이터 device 설정. \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # 모델 출력\n",
    "        pred = model(images)\n",
    "        # 학습 loss와 비교하기 위해 loss 계산. loss만 계산하며 backprogation은 수행하지 않음. \n",
    "        loss = loss_fn(pred, labels)\n",
    "\n",
    "        # 100회 마다 아래 출력\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'{batch_idx} 번째 valid loss는 {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def val_step():\n",
    "    with torch.no_grad():\n",
    "        # 미니 배치 형태로 검증 loop 수행.\n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):\n",
    "            # 모델 입력 데이터 device 설정. \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # 모델 출력\n",
    "            pred = model(images)\n",
    "            # loss 계산\n",
    "            loss = loss_fn(pred, labels)\n",
    "    \n",
    "            # 100회 마다 아래 출력\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'{batch_idx} 번째 valid loss는 {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검증 데이터 evaluation을 적용한 Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## 1 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.26603245735168457\n",
      "101 번째 batch gradient 적용, loss는 0.07069289684295654\n",
      "201 번째 batch gradient 적용, loss는 0.3076517879962921\n",
      "301 번째 batch gradient 적용, loss는 0.12542521953582764\n",
      "401 번째 batch gradient 적용, loss는 0.2429988980293274\n",
      "501 번째 batch gradient 적용, loss는 0.23334600031375885\n",
      "601 번째 batch gradient 적용, loss는 0.11451195925474167\n",
      "701 번째 batch gradient 적용, loss는 0.24849656224250793\n",
      "801 번째 batch gradient 적용, loss는 0.15787442028522491\n",
      "901 번째 batch gradient 적용, loss는 0.42368853092193604\n",
      "1001 번째 batch gradient 적용, loss는 0.2428923100233078\n",
      "1101 번째 batch gradient 적용, loss는 0.15192337334156036\n",
      "1201 번째 batch gradient 적용, loss는 0.17789509892463684\n",
      "1301 번째 batch gradient 적용, loss는 0.15823759138584137\n",
      "1401 번째 batch gradient 적용, loss는 0.22722084820270538\n",
      "1501 번째 batch gradient 적용, loss는 0.3701518177986145\n",
      "1601 번째 batch gradient 적용, loss는 0.16208244860172272\n",
      "1701 번째 batch gradient 적용, loss는 0.16665120422840118\n",
      "1801 번째 batch gradient 적용, loss는 0.08984823524951935\n",
      "######## 1 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.4059549570083618\n",
      "100 번째 valid loss는 0.3128625154495239\n",
      "200 번째 valid loss는 0.24073441326618195\n",
      "300 번째 valid loss는 0.22839407622814178\n",
      "######## 2 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.15577127039432526\n",
      "101 번째 batch gradient 적용, loss는 0.17969265580177307\n",
      "201 번째 batch gradient 적용, loss는 0.2909700572490692\n",
      "301 번째 batch gradient 적용, loss는 0.26190873980522156\n",
      "401 번째 batch gradient 적용, loss는 0.21993911266326904\n",
      "501 번째 batch gradient 적용, loss는 0.2451024353504181\n",
      "601 번째 batch gradient 적용, loss는 0.10634174197912216\n",
      "701 번째 batch gradient 적용, loss는 0.23132218420505524\n",
      "801 번째 batch gradient 적용, loss는 0.17308349907398224\n",
      "901 번째 batch gradient 적용, loss는 0.18042626976966858\n",
      "1001 번째 batch gradient 적용, loss는 0.2668481767177582\n",
      "1101 번째 batch gradient 적용, loss는 0.19253025949001312\n",
      "1201 번째 batch gradient 적용, loss는 0.11669588088989258\n",
      "1301 번째 batch gradient 적용, loss는 0.301452100276947\n",
      "1401 번째 batch gradient 적용, loss는 0.24627132713794708\n",
      "1501 번째 batch gradient 적용, loss는 0.2757898271083832\n",
      "1601 번째 batch gradient 적용, loss는 0.18986068665981293\n",
      "1701 번째 batch gradient 적용, loss는 0.015707194805145264\n",
      "1801 번째 batch gradient 적용, loss는 0.20258337259292603\n",
      "######## 2 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.41873759031295776\n",
      "100 번째 valid loss는 0.32061225175857544\n",
      "200 번째 valid loss는 0.26851990818977356\n",
      "300 번째 valid loss는 0.19849759340286255\n",
      "######## 3 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.2329133152961731\n",
      "101 번째 batch gradient 적용, loss는 0.21214956045150757\n",
      "201 번째 batch gradient 적용, loss는 0.3402547240257263\n",
      "301 번째 batch gradient 적용, loss는 0.2194177359342575\n",
      "401 번째 batch gradient 적용, loss는 0.21327190101146698\n",
      "501 번째 batch gradient 적용, loss는 0.21490183472633362\n",
      "601 번째 batch gradient 적용, loss는 0.050063882023096085\n",
      "701 번째 batch gradient 적용, loss는 0.46298152208328247\n",
      "801 번째 batch gradient 적용, loss는 0.08506665378808975\n",
      "901 번째 batch gradient 적용, loss는 0.04428831487894058\n",
      "1001 번째 batch gradient 적용, loss는 0.2681852877140045\n",
      "1101 번째 batch gradient 적용, loss는 0.19520746171474457\n",
      "1201 번째 batch gradient 적용, loss는 0.3302430510520935\n",
      "1301 번째 batch gradient 적용, loss는 0.24673376977443695\n",
      "1401 번째 batch gradient 적용, loss는 0.45911651849746704\n",
      "1501 번째 batch gradient 적용, loss는 0.14980295300483704\n",
      "1601 번째 batch gradient 적용, loss는 0.12405160069465637\n",
      "1701 번째 batch gradient 적용, loss는 0.11081833392381668\n",
      "1801 번째 batch gradient 적용, loss는 0.20994290709495544\n",
      "######## 3 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.5181536674499512\n",
      "100 번째 valid loss는 0.4303722083568573\n",
      "200 번째 valid loss는 0.3459470868110657\n",
      "300 번째 valid loss는 0.19954915344715118\n",
      "######## 4 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.18517929315567017\n",
      "101 번째 batch gradient 적용, loss는 0.2097429484128952\n",
      "201 번째 batch gradient 적용, loss는 0.14530521631240845\n",
      "301 번째 batch gradient 적용, loss는 0.17479291558265686\n",
      "401 번째 batch gradient 적용, loss는 0.2654676139354706\n",
      "501 번째 batch gradient 적용, loss는 0.09581044316291809\n",
      "601 번째 batch gradient 적용, loss는 0.1568204164505005\n",
      "701 번째 batch gradient 적용, loss는 0.1739446222782135\n",
      "801 번째 batch gradient 적용, loss는 0.21690604090690613\n",
      "901 번째 batch gradient 적용, loss는 0.2652242183685303\n",
      "1001 번째 batch gradient 적용, loss는 0.06356482207775116\n",
      "1101 번째 batch gradient 적용, loss는 0.1401132047176361\n",
      "1201 번째 batch gradient 적용, loss는 0.2107946127653122\n",
      "1301 번째 batch gradient 적용, loss는 0.23096029460430145\n",
      "1401 번째 batch gradient 적용, loss는 0.23082946240901947\n",
      "1501 번째 batch gradient 적용, loss는 0.0826934427022934\n",
      "1601 번째 batch gradient 적용, loss는 0.24823400378227234\n",
      "1701 번째 batch gradient 적용, loss는 0.09806111454963684\n",
      "1801 번째 batch gradient 적용, loss는 0.0675828605890274\n",
      "######## 4 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.524138331413269\n",
      "100 번째 valid loss는 0.4407211244106293\n",
      "200 번째 valid loss는 0.32148346304893494\n",
      "300 번째 valid loss는 0.2924382984638214\n",
      "######## 5 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.15287645161151886\n",
      "101 번째 batch gradient 적용, loss는 0.1840572953224182\n",
      "201 번째 batch gradient 적용, loss는 0.3173258900642395\n",
      "301 번째 batch gradient 적용, loss는 0.3278454840183258\n",
      "401 번째 batch gradient 적용, loss는 0.21767398715019226\n",
      "501 번째 batch gradient 적용, loss는 0.3146466314792633\n",
      "601 번째 batch gradient 적용, loss는 0.16523727774620056\n",
      "701 번째 batch gradient 적용, loss는 0.1369543820619583\n",
      "801 번째 batch gradient 적용, loss는 0.4371480643749237\n",
      "901 번째 batch gradient 적용, loss는 0.13000018894672394\n",
      "1001 번째 batch gradient 적용, loss는 0.16101334989070892\n",
      "1101 번째 batch gradient 적용, loss는 0.050165098160505295\n",
      "1201 번째 batch gradient 적용, loss는 0.08376873284578323\n",
      "1301 번째 batch gradient 적용, loss는 0.15410825610160828\n",
      "1401 번째 batch gradient 적용, loss는 0.3034074306488037\n",
      "1501 번째 batch gradient 적용, loss는 0.16151070594787598\n",
      "1601 번째 batch gradient 적용, loss는 0.26901015639305115\n",
      "1701 번째 batch gradient 적용, loss는 0.12905141711235046\n",
      "1801 번째 batch gradient 적용, loss는 0.27207639813423157\n",
      "######## 5 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.5806660056114197\n",
      "100 번째 valid loss는 0.5228930711746216\n",
      "200 번째 valid loss는 0.3346077501773834\n",
      "300 번째 valid loss는 0.1666545867919922\n",
      "######## 6 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.1664593517780304\n",
      "101 번째 batch gradient 적용, loss는 0.17621685564517975\n",
      "201 번째 batch gradient 적용, loss는 0.08113257586956024\n",
      "301 번째 batch gradient 적용, loss는 0.16826066374778748\n",
      "401 번째 batch gradient 적용, loss는 0.09849964082241058\n",
      "501 번째 batch gradient 적용, loss는 0.16529135406017303\n",
      "601 번째 batch gradient 적용, loss는 0.0746207982301712\n",
      "701 번째 batch gradient 적용, loss는 0.11991193890571594\n",
      "801 번째 batch gradient 적용, loss는 0.406518816947937\n",
      "901 번째 batch gradient 적용, loss는 0.27758052945137024\n",
      "1001 번째 batch gradient 적용, loss는 0.22308768332004547\n",
      "1101 번째 batch gradient 적용, loss는 0.20072366297245026\n",
      "1201 번째 batch gradient 적용, loss는 0.10365623235702515\n",
      "1301 번째 batch gradient 적용, loss는 0.10127193480730057\n",
      "1401 번째 batch gradient 적용, loss는 0.16886961460113525\n",
      "1501 번째 batch gradient 적용, loss는 0.17003121972084045\n",
      "1601 번째 batch gradient 적용, loss는 0.1446467638015747\n",
      "1701 번째 batch gradient 적용, loss는 0.29303044080734253\n",
      "1801 번째 batch gradient 적용, loss는 0.13037562370300293\n",
      "######## 6 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.22585463523864746\n",
      "100 번째 valid loss는 0.5067050457000732\n",
      "200 번째 valid loss는 0.46230053901672363\n",
      "300 번째 valid loss는 0.18300119042396545\n",
      "######## 7 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.26535293459892273\n",
      "101 번째 batch gradient 적용, loss는 0.09286224097013474\n",
      "201 번째 batch gradient 적용, loss는 0.06617560982704163\n",
      "301 번째 batch gradient 적용, loss는 0.19975274801254272\n",
      "401 번째 batch gradient 적용, loss는 0.15291206538677216\n",
      "501 번째 batch gradient 적용, loss는 0.06532502919435501\n",
      "601 번째 batch gradient 적용, loss는 0.17968657612800598\n",
      "701 번째 batch gradient 적용, loss는 0.2886731028556824\n",
      "801 번째 batch gradient 적용, loss는 0.04399421438574791\n",
      "901 번째 batch gradient 적용, loss는 0.12869171798229218\n",
      "1001 번째 batch gradient 적용, loss는 0.3399493992328644\n",
      "1101 번째 batch gradient 적용, loss는 0.10430058091878891\n",
      "1201 번째 batch gradient 적용, loss는 0.27549460530281067\n",
      "1301 번째 batch gradient 적용, loss는 0.03179379180073738\n",
      "1401 번째 batch gradient 적용, loss는 0.21013624966144562\n",
      "1501 번째 batch gradient 적용, loss는 0.3175816535949707\n",
      "1601 번째 batch gradient 적용, loss는 0.15353168547153473\n",
      "1701 번째 batch gradient 적용, loss는 0.2978689670562744\n",
      "1801 번째 batch gradient 적용, loss는 0.08780170232057571\n",
      "######## 7 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.3777484893798828\n",
      "100 번째 valid loss는 0.5134339332580566\n",
      "200 번째 valid loss는 0.34076789021492004\n",
      "300 번째 valid loss는 0.1844463348388672\n",
      "######## 8 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.04087284579873085\n",
      "101 번째 batch gradient 적용, loss는 0.20080314576625824\n",
      "201 번째 batch gradient 적용, loss는 0.1027713268995285\n",
      "301 번째 batch gradient 적용, loss는 0.08067114651203156\n",
      "401 번째 batch gradient 적용, loss는 0.18848402798175812\n",
      "501 번째 batch gradient 적용, loss는 0.026516908779740334\n",
      "601 번째 batch gradient 적용, loss는 0.17374871671199799\n",
      "701 번째 batch gradient 적용, loss는 0.27188268303871155\n",
      "801 번째 batch gradient 적용, loss는 0.10877548158168793\n",
      "901 번째 batch gradient 적용, loss는 0.13988035917282104\n",
      "1001 번째 batch gradient 적용, loss는 0.08137315511703491\n",
      "1101 번째 batch gradient 적용, loss는 0.21923665702342987\n",
      "1201 번째 batch gradient 적용, loss는 0.2221006602048874\n",
      "1301 번째 batch gradient 적용, loss는 0.12163427472114563\n",
      "1401 번째 batch gradient 적용, loss는 0.18331557512283325\n",
      "1501 번째 batch gradient 적용, loss는 0.09329268336296082\n",
      "1601 번째 batch gradient 적용, loss는 0.13508881628513336\n",
      "1701 번째 batch gradient 적용, loss는 0.0978316068649292\n",
      "1801 번째 batch gradient 적용, loss는 0.20079953968524933\n",
      "######## 8 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.4421136677265167\n",
      "100 번째 valid loss는 0.42652279138565063\n",
      "200 번째 valid loss는 0.1824326068162918\n",
      "300 번째 valid loss는 0.1528381109237671\n",
      "######## 9 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.09433155506849289\n",
      "101 번째 batch gradient 적용, loss는 0.1063811406493187\n",
      "201 번째 batch gradient 적용, loss는 0.24358057975769043\n",
      "301 번째 batch gradient 적용, loss는 0.1289166957139969\n",
      "401 번째 batch gradient 적용, loss는 0.06983783096075058\n",
      "501 번째 batch gradient 적용, loss는 0.09165763854980469\n",
      "601 번째 batch gradient 적용, loss는 0.07961589843034744\n",
      "701 번째 batch gradient 적용, loss는 0.2258649468421936\n",
      "801 번째 batch gradient 적용, loss는 0.1532217264175415\n",
      "901 번째 batch gradient 적용, loss는 0.09685370326042175\n",
      "1001 번째 batch gradient 적용, loss는 0.11514990031719208\n",
      "1101 번째 batch gradient 적용, loss는 0.24564877152442932\n",
      "1201 번째 batch gradient 적용, loss는 0.20603874325752258\n",
      "1301 번째 batch gradient 적용, loss는 0.10997708141803741\n",
      "1401 번째 batch gradient 적용, loss는 0.17521008849143982\n",
      "1501 번째 batch gradient 적용, loss는 0.08861124515533447\n",
      "1601 번째 batch gradient 적용, loss는 0.2101477086544037\n",
      "1701 번째 batch gradient 적용, loss는 0.33959582448005676\n",
      "1801 번째 batch gradient 적용, loss는 0.09549900144338608\n",
      "######## 9 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.6461150646209717\n",
      "100 번째 valid loss는 0.7046481370925903\n",
      "200 번째 valid loss는 0.38369250297546387\n",
      "300 번째 valid loss는 0.222299262881279\n",
      "######## 10 번째 train epoch 시작\n",
      "1 번째 batch gradient 적용, loss는 0.18223287165164948\n",
      "101 번째 batch gradient 적용, loss는 0.08936986327171326\n",
      "201 번째 batch gradient 적용, loss는 0.20335829257965088\n",
      "301 번째 batch gradient 적용, loss는 0.034688811749219894\n",
      "401 번째 batch gradient 적용, loss는 0.320587694644928\n",
      "501 번째 batch gradient 적용, loss는 0.21590764820575714\n",
      "601 번째 batch gradient 적용, loss는 0.3655400276184082\n",
      "701 번째 batch gradient 적용, loss는 0.06200920045375824\n",
      "801 번째 batch gradient 적용, loss는 0.13780754804611206\n",
      "901 번째 batch gradient 적용, loss는 0.11407522112131119\n",
      "1001 번째 batch gradient 적용, loss는 0.08725257217884064\n",
      "1101 번째 batch gradient 적용, loss는 0.015842938795685768\n",
      "1201 번째 batch gradient 적용, loss는 0.42193329334259033\n",
      "1301 번째 batch gradient 적용, loss는 0.12116555869579315\n",
      "1401 번째 batch gradient 적용, loss는 0.3589818775653839\n",
      "1501 번째 batch gradient 적용, loss는 0.4180046617984772\n",
      "1601 번째 batch gradient 적용, loss는 0.18496322631835938\n",
      "1701 번째 batch gradient 적용, loss는 0.14908897876739502\n",
      "1801 번째 batch gradient 적용, loss는 0.12627246975898743\n",
      "######## 10 번째 validation epoch 시작\n",
      "0 번째 valid loss는 0.26635316014289856\n",
      "100 번째 valid loss는 0.4782847762107849\n",
      "200 번째 valid loss는 0.3231213390827179\n",
      "300 번째 valid loss는 0.17108581960201263\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# model에 device 설정\n",
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # train 모드 설정 \n",
    "    model.train()\n",
    "    print(f\"######## {epoch+1} 번째 train epoch 시작\")\n",
    "    train_step()\n",
    "\n",
    "    # evaluation 모드 설정\n",
    "    model.eval()\n",
    "    print(f\"######## {epoch+1} 번째 validation epoch 시작\")\n",
    "    val_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습이 완료된 모델로 이미지 예측하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "# 테스트 데이터는 검증 데이터를 재 사용\n",
    "test_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "images, labels = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output shape: torch.Size([32, 10])\n",
      "predicted class: tensor([9, 2, 1, 1, 0, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 5,\n",
      "        1, 2, 6, 0, 9, 0, 8, 8]) torch.Size([32])\n",
      "targets: tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
      "        1, 4, 6, 0, 9, 3, 8, 8]) torch.Size([32])\n",
      "32개 데이터 중 28개 예측 정확\n"
     ]
    }
   ],
   "source": [
    "# 여기서 model은 학습이 완료된 모델이어야 함. model device는 'cuda:0'. \n",
    "# model을 evaluation mode로 설정. \n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    pred_logit = model(images)\n",
    "    pred_class = F.softmax(pred_logit, dim=-1).argmax(-1)\n",
    "    \n",
    "print('model output shape:', pred_logit.shape)\n",
    "print('predicted class:', pred_class, pred_class.shape)\n",
    "print('targets:', labels, labels.shape)\n",
    "\n",
    "#print('pred_logit[0]:', pred_logit[0])\n",
    "#print('after softmax:', F.softmax(pred_logit[0], -1))\n",
    "#print('after tensor argmax:', F.softmax(pred_logit[0], dim=-1).argmax(-1))\n",
    "\n",
    "num_correct = (pred_class == labels).sum().item()\n",
    "# print(pred_class == targets)\n",
    "print(f'{images.size()[0]}개 데이터 중 {num_correct}개 예측 정확')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28]) torch.Size([])\n",
      "target class:9, predicted class: tensor([9])\n"
     ]
    }
   ],
   "source": [
    "# 단일 이미지 예측. \n",
    "s_image = images[0] #.unsqueeze(0)\n",
    "s_label = labels[0] #.unsqueeze(0)\n",
    "print(s_image.shape, s_label.shape)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    s_image = s_image.to(device)\n",
    "    s_label = s_label.to(device)\n",
    "\n",
    "    pred_logit = model(s_image)\n",
    "    pred_class = F.softmax(pred_logit, dim=-1).argmax(-1)\n",
    "\n",
    "print(f'target class:{s_label}, predicted class: {pred_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Ankle boot')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat','Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "#tensor를 PIL로 변환 \n",
    "to_pil = ToPILImage()\n",
    "pil_image = to_pil(images[0])\n",
    "\n",
    "plt.imshow(pil_image, cmap='gray')\n",
    "plt.title(class_names[pred_class.item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
