{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet 34 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, last_channels, stride=1, downsample=None):\n",
    "        '''\n",
    "        입력 채널수, 중간 채널 수, 최종 채널수\n",
    "        stride는 기본 1. stage 별로 feature map의 크기를 줄일 경우 2\n",
    "        downsample은 stride가 2일 경우 BasicBlock 입력 전 값도 1x1 conv, stride 2를 적용하여 사이즈를 줄이는 Conv block\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            # 첫번째 3x3 Conv. stage별로 Feature Map의 크기를 줄일 시 첫번째 Conv에서 줄임(3x3 kernel에 stride=2로)\n",
    "            nn.Conv2d(in_channels, last_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(last_channels),\n",
    "            nn.ReLU(),\n",
    "            # 두번째 3x3 Conv\n",
    "            nn.Conv2d(last_channels, last_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(last_channels),\n",
    "        )\n",
    "        self.stride = stride\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv_block(x)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_block_01 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        # 3, 4, 6, 3개의 BasicBlock들로 이루어진 stage들. \n",
    "        self.stage_01 = self.make_basic_stage(in_channels=64, last_channels=64, stride=1, blocks=3)\n",
    "        self.stage_02 = self.make_basic_stage(in_channels=64, last_channels=128, stride=2, blocks=4)\n",
    "        self.stage_03 = self.make_basic_stage(in_channels=128, last_channels=256, stride=2, blocks=6)\n",
    "        self.stage_04 = self.make_basic_stage(in_channels=256, last_channels=512, stride=2, blocks=3)\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def make_basic_stage(self, in_channels, last_channels, stride, blocks):\n",
    "        # 모든 BasicBlock들을 담을 List\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        # 함수의 인자로 stride가 1이 아닌 2가 들어올 경우 downsample 생성. \n",
    "        if stride != 1:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=last_channels, \n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(num_features=last_channels)\n",
    "            )\n",
    "        # 각 stage의 첫번째 Block. 함수의 stride 인자가 1 또는 2인지에 따라 생성된 downsample을 BasicBlock의 인자로 입력. \n",
    "        layers.append(BasicBlock(in_channels=in_channels, last_channels=last_channels,\n",
    "                                     stride=stride, downsample=downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            # 각 stage의 첫번째 Block을 제외하고는 모두 stride=1, downsample=None \n",
    "            layers.append(BasicBlock(in_channels=last_channels, last_channels=last_channels, \n",
    "                                    stride=1, downsample=None))\n",
    "        \n",
    "        #layer list에 있는 모든 BasicBlock들을 Sequential로 연결하여 반환 \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_01(x)\n",
    "        x = self.stage_01(x)\n",
    "        x = self.stage_02(x)\n",
    "        x = self.stage_03(x)\n",
    "        x = self.stage_04(x)\n",
    "        \n",
    "        # GAP 및 최종 Classifier Layer forward\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 50 모델 살펴 보기\n",
    "* Bottleneck Block이 개별 stage 별로 3, 4, 6, 3 개로 구성.\n",
    "* Bottleneck Block은 1x1 Conv block, 3x3 Conv Block, 1x1 Conv Block으로 구성됨.\n",
    "* Stage의 첫번째 Block에서 Feature Map의 크기를 절반으로 줄일 경우 3x3 Conv에 stride 2를 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "\n",
    "resnet50_model = models.resnet50(weights=None)\n",
    "print(resnet50_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=resnet50_model, input_size=(1, 3, 224, 224),\n",
    "        col_names=['input_size', 'output_size', 'num_params'], \n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 50용 Residual(Identity) Block 생성(BottleneckBlock)\n",
    "* 1x1 Conv block -> 3x3 Conv Block -> 1x1 Conv Block으로 구성된 BottleneckBlock 생성\n",
    "* 첫번째 Conv block에서 입력 채널의 갯수를 (주로)감소 시키며, 마지막 1x1 Conv Block에서는 채널 수를 증가 시킴\n",
    "* stride가 2로 Feature map의 크기를 줄여야 할 경우 중간 3x3 Conv Block에서 stride 2를 적용\n",
    "* downsample은 stage별 첫 BottleneckBlock에 적용됨.\n",
    "![Residual Block Bottleneck](https://github.com/chulminkw/CNN_PG_Torch/blob/main/image/bottleneck_block_real.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, last_channels, stride=1, downsample=None):\n",
    "        super().__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            # 첫번째 1x1 Conv, 채널의 갯수 감소\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=mid_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(),\n",
    "            # 중간 3x3 Conv, stage별로 Feature Map의 크기를 줄일 시 두번째 Conv에서 줄임(3x3 kernel에 stride=2로)\n",
    "            nn.Conv2d(mid_channels, mid_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(),\n",
    "            # 마지막 1x1 Conv. 채널의 갯수를 다시 증가.\n",
    "            nn.Conv2d(mid_channels, last_channels, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(last_channels)\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv_block(x)\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "downsample = nn.Sequential(\n",
    "    nn.Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "    nn.BatchNorm2d(256)\n",
    ")\n",
    "bottleneck = BottleneckBlock(in_channels=64, mid_channels=64, last_channels=256, stride=1, downsample=downsample)\n",
    "print(bottleneck)\n",
    "print(resnet50_model.layer1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "bottleneck = BottleneckBlock(in_channels=64, mid_channels=64, last_channels=256, \n",
    "                             stride=1, downsample=downsample)\n",
    "\n",
    "summary(model=bottleneck, input_size=(1, 64, 56, 56),\n",
    "        col_names=['input_size', 'output_size', 'num_params'], \n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BottleneckBlock을 연속으로 연결하여 Stage를 만드는 함수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def make_bottleneck_stage(in_channels, mid_channels, last_channels, stride, blocks):\n",
    "    '''\n",
    "    in_channels는 downsample의 입력 채널수, last_channels는 downsample의 출력 채널수\n",
    "    stage별 첫번째 Bottleneck Block이 아닌 경우는 나머지 Bottleneck Block의 생성 인자로\n",
    "    in_channels에 함수 인자 last_channels를 입력. \n",
    "    '''\n",
    "    # 모든 BasicBlock들을 담을 List\n",
    "    layers = []\n",
    "    downsample = None\n",
    "    # stride 여부와 관계없이 downsample이 적용됨. downsample의 stride는 함수의 인자로 전달됨. \n",
    "    downsample = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=in_channels, out_channels=last_channels, \n",
    "                  kernel_size=1, stride=stride, bias=False),\n",
    "        nn.BatchNorm2d(num_features=last_channels)\n",
    "    )\n",
    "    # 각 stage의 첫번째 Block. stride는 함수의 인자로 전달됨. \n",
    "    layers.append(BottleneckBlock(in_channels=in_channels, mid_channels=mid_channels, last_channels=last_channels,\n",
    "                                 stride=stride, downsample=downsample))\n",
    "    for _ in range(1, blocks):\n",
    "        # 각 stage의 첫번째 Block을 제외하고는 모두 stride=1, downsample은 None\n",
    "        layers.append(BottleneckBlock(in_channels=last_channels, mid_channels=mid_channels, last_channels=last_channels, \n",
    "                                stride=1, downsample=None))\n",
    "\n",
    "    #layer list에 있는 모든 BottleBlock들을 Sequential로 연결하여 반환 \n",
    "    return nn.Sequential(*layers)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stage_01 = make_bottleneck_stage(in_channels=64, mid_channels=64, last_channels=256, stride=1, blocks=3)\n",
    "print(stage_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_mid_channels = 128\n",
    "\n",
    "stage_02 = make_bottleneck_stage(in_channels=base_mid_channels * 2, \n",
    "                                 mid_channels=base_mid_channels, \n",
    "                                 last_channels=base_mid_channels * 4, stride=2, blocks=4)\n",
    "print(stage_02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 50 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_block_01 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "        )\n",
    "        # 3, 4, 6, 3개의 Bottleneck Block들로 이루어진 stage들. \n",
    "        self.stage_01 = self.make_bottleneck_stage(in_channels=64, mid_channels=64,\n",
    "                                                   last_channels=256, stride=1, blocks=3)\n",
    "        self.stage_02 = self.make_bottleneck_stage(in_channels=256, mid_channels=128,\n",
    "                                                   last_channels=512, stride=2, blocks=4)\n",
    "        self.stage_03 = self.make_bottleneck_stage(in_channels=512, mid_channels=256, \n",
    "                                                   last_channels=1024, stride=2, blocks=6)\n",
    "        self.stage_04 = self.make_bottleneck_stage(in_channels=1024, mid_channels=512,\n",
    "                                                   last_channels=2048, stride=2, blocks=3)\n",
    "        # print(self.modules)\n",
    "        # for m in self.modules():\n",
    "        #     if isinstance(m, nn.Conv2d):\n",
    "        #         nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        #     elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "        #         nn.init.constant_(m.weight, 1)\n",
    "        #         nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def make_bottleneck_stage(self, in_channels, mid_channels, last_channels, stride, blocks):\n",
    "        '''\n",
    "        in_channels는 downsample의 입력 채널수, last_channels는 downsample의 출력 채널수\n",
    "        stage별 첫번째 Bottleneck Block이 아닌 경우는 나머지 Bottleneck Block의 생성 인자로\n",
    "        in_channels에 함수 인자 last_channels를 입력. \n",
    "        '''\n",
    "        # 모든 BasicBlock들을 담을 List\n",
    "        layers = []\n",
    "        downsample = None\n",
    "        # stride 여부와 관계없이 downsample이 적용됨. downsample의 stride는 함수의 인자로 전달됨. \n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=last_channels, \n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(num_features=last_channels)\n",
    "        )\n",
    "        # 각 stage의 첫번째 Block. stride는 함수의 인자로 전달됨. \n",
    "        layers.append(BottleneckBlock(in_channels=in_channels, mid_channels=mid_channels, last_channels=last_channels,\n",
    "                                     stride=stride, downsample=downsample))\n",
    "        for _ in range(1, blocks):\n",
    "            # 각 stage의 첫번째 Block을 제외하고는 모두 stride=1, downsample은 None\n",
    "            layers.append(BottleneckBlock(in_channels=last_channels, mid_channels=mid_channels, last_channels=last_channels, \n",
    "                                    stride=1, downsample=None))\n",
    "    \n",
    "        #layer list에 있는 모든 BottleBlock들을 Sequential로 연결하여 반환 \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_block_01(x)\n",
    "        x = self.stage_01(x)\n",
    "        x = self.stage_02(x)\n",
    "        x = self.stage_03(x)\n",
    "        x = self.stage_04(x)\n",
    "        \n",
    "        # GAP 및 최종 Classifier Layer forward\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "my_resnet50_model = ResNet50(num_classes=1000)\n",
    "summary(model=my_resnet50_model, input_size=(1, 3, 224, 224),\n",
    "        col_names=['input_size', 'output_size', 'num_params'],\n",
    "        depth=4,\n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch_resnet50_model = models.resnet50(weights='DEFAULT')\n",
    "summary(model=torch_resnet50_model, input_size=(1, 3, 224, 224),\n",
    "        col_names=['input_size', 'output_size', 'num_params'], \n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet 50 모델 학습 및 평가\n",
    "* Flowers 데이터 세트로 학습 및 평가\n",
    "* Trainer 클래스는 Modular 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "\n",
    "def create_flowers_meta_df(file_dir):\n",
    "    paths = [] # 이미지 파일 경로 리스트\n",
    "    \n",
    "    labels = [] # 꽃 종류\n",
    "    \n",
    "    # os.walk()를 이용하여 특정 디렉토리 밑에 있는 모든 하위 디렉토리를 모두 조사. \n",
    "    # kaggle/input/flowers-dataset 하위 디렉토리 밑에 jpg 확장자를 가진 파일이 모두 이미지 파일임\n",
    "    # kaggle/input/flowers-dataset 밑으로 하위 디렉토리 존재\n",
    "    for dirname, _, filenames in os.walk(file_dir):\n",
    "        for filename in filenames:\n",
    "            # 이미지 파일이 아닌 파일도 해당 디렉토리에 있음.\n",
    "            if '.jpg' in filename:\n",
    "                # 파일의 절대 경로를 file_path 변수에 할당. \n",
    "                file_path = dirname+'/'+ filename\n",
    "                paths.append(file_path)\n",
    "                \n",
    "                # 파일의 절대 경로에 daily, dandelion, roses, sunflowers, tulips에 따라 labels에 값 할당.               label_gubuns.append('daisy')\n",
    "                if 'daisy' in file_path:\n",
    "                    labels.append('daisy')\n",
    "                elif 'dandelion' in file_path:\n",
    "                    labels.append('dandelion')\n",
    "                elif 'roses' in file_path:\n",
    "                    labels.append('rose')\n",
    "                elif 'sunflowers' in file_path:\n",
    "                    labels.append('sunflowers')\n",
    "                elif 'tulips' in file_path:\n",
    "                    labels.append('tulips')\n",
    "    # DataFrame 메타 데이터 생성. \n",
    "    data_df = pd.DataFrame({'path':paths, \n",
    "                            'label':labels})\n",
    "    # Target값  변환\n",
    "    label_mapping = {'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflowers': 3, 'tulips': 4}\n",
    "    data_df['target'] = data_df['label'].map(label_mapping)\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_df = create_flowers_meta_df('/kaggle/input/flowers-dataset') # /kaggle/input\n",
    "\n",
    "# 전체 데이터 세트에서 학습(전체의 70%)과 테스트용(전체의 30%) 메타 정보 DataFrame 생성.\n",
    "train_df, test_df = train_test_split(data_df, test_size=0.3, stratify=data_df['target'], random_state=2025)\n",
    "# 기존 학습 DataFrame을 다시 학습과 검증 DataFrame으로 분할. 80%가 학습, 20%가 검증\n",
    "tr_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['target'], random_state=2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "class FlowerDataset(Dataset):\n",
    "    # 이미지 파일리스트, 타겟 파일리스트, transforms 등 이미지와 타겟 데이터 가공에 필요한 인자들을 입력 받음\n",
    "    def __init__(self, image_paths, targets=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "    \n",
    "    # 전체 건수를 반환\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    # idx로 지정된 하나의 image, label을 tensor 형태로 반환\n",
    "    def __getitem__(self, idx):    \n",
    "        # PIL을 이용하여 이미지 로딩하고 PIL Image 객체 반환.\n",
    "        pil_image = Image.open(self.image_paths[idx])\n",
    "        # 보통은 transform이 None이 되는 경우는 거의 없음(Tensor 변환이라도 있음)\n",
    "        image = self.transform(pil_image)\n",
    "\n",
    "        if self.targets is not None:\n",
    "            # 개별 target값을 tensor로 변환.\n",
    "            target = torch.tensor(self.targets[idx])\n",
    "            return image, target\n",
    "        # 테스트 데이터의 경우 targets가 입력 되지 않을 수 있으므로 이를 대비. \n",
    "        else:\n",
    "            return image\n",
    "\n",
    "def create_tr_val_loader(tr_df, val_df, tr_transform, val_transform):\n",
    "    tr_dataset = FlowerDataset(image_paths=tr_df['path'].to_list(), \n",
    "                            targets=tr_df['target'].to_list(), transform=tr_transform)\n",
    "    val_dataset = FlowerDataset(image_paths=val_df['path'].to_list(), \n",
    "                            targets=val_df['target'].to_list(), transform=val_transform)\n",
    "    \n",
    "    tr_loader = DataLoader(tr_dataset, batch_size = BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2*BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return tr_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "IMG_MEANS = [0.485, 0.456, 0.406] # ImageNet 데이터세트의 이미지 채널별 평균값\n",
    "IMG_STD = [0.229, 0.224, 0.225] # ImageNet 데이터세트의 이미지 채널별 표준편차값\n",
    "\n",
    "tr_transform = T.Compose([\n",
    "            T.RandomHorizontalFlip(p=0.3),\n",
    "            T.RandomVerticalFlip(p=0.3),\n",
    "            T.RandomApply([T.CenterCrop(size=(200,200))], p=0.4),\n",
    "            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "])\n",
    "    \n",
    "val_transform = T.Compose([\n",
    "            T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "            T.ToTensor(), T.Normalize(mean=IMG_MEANS, std=IMG_STD)\n",
    "])\n",
    "\n",
    "tr_loader, val_loader = create_tr_val_loader(tr_df=tr_df, val_df=val_df, \n",
    "                                             tr_transform=tr_transform, val_transform=val_transform)\n",
    "images, labels = next(iter(tr_loader))\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer 클래스 생성 및 적용\n",
    "* Trainer 클래스는 https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/main/modular/v1/utils.py?raw=true 로 download 후 import 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# /kaggle/working/modular/v1 디렉토리에 utils.py 파일 다운로드\n",
    "!rm -rf ./modular/v1\n",
    "!mkdir -p ./modular/v1\n",
    "!wget -O ./modular/v1/utils.py https://raw.githubusercontent.com/chulminkw/CNN_PG_Torch/main/modular/v1/utils.py?raw=true\n",
    "!ls ./modular/v1\n",
    "\n",
    "import sys\n",
    "\n",
    "# 반드시 system path를 아래와 같이 잡아줘야 함. \n",
    "sys.path.append('/kaggle/working')\n",
    "\n",
    "#아래가 수행되는지 반드시 확인\n",
    "from modular.v1.utils import Trainer, Predictor, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "# 5개의 꽃 종류\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "# 직접 구현한 ResNet50 모델을 이용\n",
    "model = ResNet50(num_classes=NUM_CLASSES)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = Adam(model.parameters(), lr=4e-4) #lr = 1e-4\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "scheduler = ReduceLROnPlateau(\n",
    "            optimizer=optimizer, mode='min', factor=0.5, patience=5, threshold=0.01, min_lr=1e-6)\n",
    "\n",
    "# 검증 데이터 정확도 기준 checkpoint 파일 생성. \n",
    "model_checkpoint = ModelCheckpoint('/kaggle/working/checkpoints', monitor='val_acc', mode='max')\n",
    "\n",
    "trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n",
    "                  train_loader=tr_loader, val_loader=val_loader, scheduler=scheduler,\n",
    "                  callbacks=[model_checkpoint], device=device)\n",
    "# 학습 및 평가.\n",
    "history = trainer.fit(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from modular.v1.utils import Predictor\n",
    "\n",
    "test_image_paths = test_df['path'].to_list()\n",
    "test_targets = test_df['target'].to_list()\n",
    "\n",
    "IMG_SIZE=224\n",
    "test_transform = T.Compose([\n",
    "                        T.Resize(size=(IMG_SIZE, IMG_SIZE)),\n",
    "                        T.ToTensor(), \n",
    "                        T.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_dataset = FlowerDataset(image_paths=test_image_paths, \n",
    "                            targets=test_targets, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 가장 검증 성능이 좋은 weight 파일을 모델로 로딩. \n",
    "# state_dict = torch.load('/kaggle/working/checkpoints/checkpoint_epoch_37_val_acc_0.8444.pt', weights_only=True)\n",
    "# best_trained_model = ResNet50(num_classes=NUM_CLASSES)\n",
    "# best_trained_model.load_state_dict(state_dict)\n",
    "\n",
    "#또는 맨 마지막 epoch로 학습된 model weight\n",
    "best_trained_model = trainer.get_trained_model()\n",
    "\n",
    "predictor = Predictor(model=best_trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5795628,
     "sourceId": 9519275,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
