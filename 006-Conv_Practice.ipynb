{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv2d 적용하기\n",
    "* Conv2d Layer의 주요 생성 인자\n",
    "    * in_channels: 입력 tensor의 channel 수(차원)\n",
    "    * out_channels: conv 연산 적용 후 생성되는 출력 tensor(output feature map)의 차원 수\n",
    "    * kernel_size: conv kernel size. (5, 5)와 같은 튜플 형태(이 경우 5x5 kernel size) 또는 5와 같이 정수값\n",
    "    * stride: conv연산 stride\n",
    "    * padding: conv 연산 전 입력 데이터의 상하좌우로 채우는 빈 값 size(output feature map의 사이즈 크기 조정을 위해 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "input_tensor = torch.randn(3, 28, 28)\n",
    "print('input tensor shape:', input_tensor.shape)\n",
    "\n",
    "conv_layer_01 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, stride=1)\n",
    "output_tensor = conv_layer_01(input_tensor)\n",
    "print('output tensor shape:', output_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "conv_layer_01 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, stride=1, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#conv2d layer weight shape\n",
    "conv_layer_01.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(3, 28, 28)\n",
    "\n",
    "# 2개의 convolution을 적용하여 최종 output의 shape가 (16, 22, 22)가 나옴. \n",
    "conv_layer_01 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, stride=1) #padding='same'\n",
    "conv_layer_02 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1) #padding='same'\n",
    "output_01 = conv_layer_01(input_tensor)\n",
    "output_02 = conv_layer_02(output_01)\n",
    "\n",
    "print('output_01 shape:', output_01.shape, 'output_02 shape:', output_02.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "conv_layer_01.weight.shape, conv_layer_02.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaxPool2d(AvgPool2d) 적용\n",
    "* MaxPool2d은 kernel_size 만큼 window 이동으로 가장 큰 값을 추출해 가면 output을 생성.\n",
    "* Pooling은 학습 파라미터를 가지고 있지 않은 Layer\n",
    "* 주요 기능\n",
    "  * 입력 feature map의 사이즈를 줄여서 computational cost 감소\n",
    "  * 차원 축소의 역할로서 특정 영역별로(kernel_size) 입력 feature map을 주요한 feature값으로 요약\n",
    "  * 입력값이 작은 변화에 너무 민감하게 반응하지 않도록 변동성 감소(overfitting 감소)\n",
    "* kernel_size, stride, padding을 생성 파라미터로 가짐. stride의 default값이 kernel_size 값임에 유의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(3, 28, 28)\n",
    "\n",
    "conv_layer_01 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3, stride=1, padding=1)\n",
    "pool_layer_01 = nn.MaxPool2d(kernel_size=2)# stride는 기재하지 않으면 kernel_size와 동일. \n",
    "output_01 = conv_layer_01(input_tensor)\n",
    "output_02 = pool_layer_01(output_01)\n",
    "\n",
    "print('output_01 shape:', output_01.shape, 'output_02 shape:', output_02.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 기반 모델 생성 - 01\n",
    "* Conv2d 기반으로 모델 생성. Conv2d -> ReLU -> Conv2d -> ReLU -> MaxPool2d 로 CNN 모델 생성\n",
    "* 마지막 classification layer는 Linear Layer가 되어야 하므로 Conv2d 수행 결과인 3차원 Feature Map을 flatten하여 Linear Layer 연결 필요. 이를 위해 Flatten을 Feature map에 적용한 뒤 Linear Layer로 연결\n",
    "* 마지막 Feature Map -> Flatten -> Linear 적용을 하게 되면 Linear에 매우 많은 Learnable Parameter 를 가지게 됨(Overfitting의 이슈 발생하기 쉬움. Drop out등의 설정 필요 할 수 있음)\n",
    "* Linear Layer의 입력 in_features는 Flatten의 결과로 만들어진 입력의 갯수를 설정해줘야 하지만, 이미지 크기나 Conv 설정에 따라 변하게 됨.이를 위해 최종 feature map의 크기를 식으로 계산하거나, summary등을 통해 미리 파악 후 적용 필요. 하지만 이미지 크기나 Conv 설정이 변경되면 다시 계산해야 하는 불편함이 있음.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "\n",
    "# 3x3 kernel, 32개의 filter들을 가지는 Conv Layer, 3x3 kernel, 64개의 filter들을 가지는 Conv Layer, 이후 MaxPooling \n",
    "class SimpleCNN_01(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.classifier = nn.Linear(in_features=12544, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "input = torch.randn(1, 3, 32, 32) # 이미지 사이즈를 64, 64로 변경하면 classfication layer 오류 발생.\n",
    "simple_cnn_01 = SimpleCNN_01(num_classes=10)\n",
    "output = simple_cnn_01(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(model=simple_cnn_01, input_size=(1, 3, 32, 32),\n",
    "        col_names=['input_size', 'output_size', 'num_params'],\n",
    "        row_settings=['var_names'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaptiveAvgPool2d를 이용한 Global Average Pooling\n",
    "* Pytorch에서는 GAP를 위해서 AdaptiveAvgPool2d()를 적용\n",
    "* AdaptiveAvgPool2d는 인자로 output_size를 받으며, channel별로 자동으로 지정된 output_size가 되도록 subsampling 수행. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 64, 8, 9)\n",
    "\n",
    "# 만들어지는 (채널별)출력 size는 5x5\n",
    "m = nn.AdaptiveAvgPool2d(output_size=(5, 5))\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "\n",
    "m = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "output = m(input)\n",
    "print(output.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 기반 모델 생성 - 02\n",
    "* Feature map을 바로 Flatten하지 않고 Adaptive Global Pooling을 적용한 뒤 Flatten 적용\n",
    "* Global Pooling은 MaxPool2d와 다르게 채널별로 하나의 값으로 Pooling을 적용할 수 있음. 보통은 AdaptiveAvgPool2d가 많이 활용됨.\n",
    "* 마지막 Feature Map에 AdaptiveAvgPool2d(output_size=(1, 1))을 적용하면 feature map의 채널수는 동일하지만 면적(가로와 세로)은 1인 feature map으로 Pooling됨. 때문에 마지막 Conv2d의 out_channels 수만 알면 Flatten을 생기는 차원을 알 수 있으며, 이를 Linear Layer의 in_features의 값으로 입력하면 됨.\n",
    "* Global Pooling은 Feature Map 압축 효과로 인하여 classification Layer의 파라미터를 크게 줄일 수 있음.\n",
    "* Global Pooling은 보통 CNN의 Layer가 어느정도 깊이가 있어야 성능 저하가 발생하지 않음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "\n",
    "class SimpleCNN_02(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv_1 = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=32, kernel_size=3, stride=1)\n",
    "        self.conv_2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.adapt_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "                \n",
    "        # in_features는 마지막 Conv2d의 out_channels임.\n",
    "        self.classifier = nn.Linear(in_features=64, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Global Pooling 적용. \n",
    "        x = self.adapt_pool(x)\n",
    "        x = x.view(x.size(0), -1) #x = torch.flatten(x, start_dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "        \n",
    "input = torch.randn(1, 3, 64, 64) # 이미지 사이즈를 64, 64로 변경해도 classfication layer 오류 없음.\n",
    "simple_cnn_02 = SimpleCNN_02(num_classes=10)\n",
    "output = simple_cnn_02(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "simple_cnn_02 = SimpleCNN_02(num_classes=10)\n",
    "summary(model=simple_cnn_02, input_size=(1, 3, 32, 32), # 이미지 사이즈를 64, 64로 변경해도 오류 발생하지 않음.  \n",
    "        col_names=['input_size', 'output_size', 'num_params'], \n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# torchvision의 pretrained 모델 구조에서 Global Average Pooling 적용 살펴 보기\n",
    "model = models.vgg19() # models.resnet50()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 Dataset 생성\n",
    "* torchvision.datasets의 CIFAR10으로 dataset 생성. transform=ToTensor() 수행 시 PIL image를 tensor로 변환하면서 0 ~ 1사이 값으로 Normalization 적용.\n",
    "* CIFAR10 Dataset의 data 속성은 numpy 형태로 이미지값을 가짐. targets 속성은 np.uint8 형태로 target값을 가짐. classes 속성은 개별 target에 매핑되는 class의 이름을 가짐.  \n",
    "* CIFAR10 Dataset 기반으로 DataLoader 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "#전체 6만개 데이터 중, 5만개는 학습 데이터용. 이를 다시 학습과 검증용으로 split , 1만개는 테스트 데이터용\n",
    "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "tr_size = int(0.85 * len(train_dataset))\n",
    "val_size = len(train_dataset) - tr_size\n",
    "tr_dataset, val_dataset = random_split(train_dataset, [tr_size, val_size])\n",
    "print('tr:', len(tr_dataset), 'valid:', len(val_dataset))\n",
    "\n",
    "tr_loader = DataLoader(tr_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(tr_loader))\n",
    "print(images.shape, labels.shape)\n",
    "print(images[0].max(), images[0].min(), labels.min(), labels.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# tr_dataset은 Subset\n",
    "print(type(tr_dataset), '\\n', type(train_dataset))\n",
    "print(tr_dataset, '\\n', train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# targets는 10개 target classes의 0 ~ 9까지의 값. clsses는 0~9까지의 target값에 매핑되는 label명\n",
    "# tr_dataset은 subset으로 .classes 속성이 없음. train_dataset.classes로 확인\n",
    "print(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#train_dataset[0]은 호출시 마다 ToTensor()로 변환됨. 이미지 표현을 위해 PIL이나 Numpy array 필요. \n",
    "#train_dataset.data 는 image를 numpy 배열 형태로 가짐(channel last)\n",
    "print(type(train_dataset.data), train_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def show_images(images, labels, ncols=8):\n",
    "    figure, axs = plt.subplots(figsize=(22, 6), nrows=1, ncols=ncols)\n",
    "    for i in range(ncols):\n",
    "        # imshow()는 numpy array를 그대로 이미지화 시킬 수 있음. \n",
    "        axs[i].imshow(images[i])\n",
    "        axs[i].set_title(class_names[labels[i]])\n",
    "        \n",
    "show_images(train_dataset.data[:8], train_dataset.targets[:8], ncols=8)\n",
    "show_images(train_dataset.data[8:16], train_dataset.targets[8:16], ncols=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 기반 모델 생성 - 03\n",
    "* Conv의 필터수 및 네트웍의 깊이를 좀 더 증가 시켜 모델 구성.\n",
    "* conv->relu->conv->relu->pooling을 연속적으로 수행. kernel 크기는 3, filter 수는 블럭별로 32 -> 64 -> 128로 증가. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        #padding 1로 conv 적용 후 출력 면적 사이즈를 입력 면적 사이즈와 동일하게 유지.\n",
    "        #kernel 크기 3, filter 개수 32 연속 적용.\n",
    "        self.conv_11 = nn.Conv2d(in_channels=NUM_INPUT_CHANNELS, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv_12 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool_01 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #out_channels이 64인 2개의 Conv2d 연속 적용. stride=1이 기본값, padding='same'은 version 1.8에서 소개됨.  \n",
    "        self.conv_21 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same')\n",
    "        self.conv_22 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding='same')\n",
    "        self.pool_02 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # Sequential Module을 이용하여 Conv Layer들을 생성. 이 경우 relu activation위해 ReLU Layer 연결 생성 필요.\n",
    "        # filter갯수 128개인 Conv Layer 2개 적용 후 Max Pooling 적용.\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        # GAP 및 최종 Classifier Layer\n",
    "        self.adapt_pool = nn.AdaptiveAvgPool2d(output_size=(1,1))\n",
    "        self.classifier = nn.Linear(in_features=128, out_features=num_classes)\n",
    "        # self.classifier_block = nn.Sequential(\n",
    "        #     nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(in_features=128, out_features=num_classes)\n",
    "        # )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_11(x))\n",
    "        x = F.relu(self.conv_12(x))\n",
    "        x = self.pool_01(x)\n",
    "        # x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = F.relu(self.conv_21(x))\n",
    "        x = F.relu(self.conv_22(x))\n",
    "        x = self.pool_02(x)\n",
    "\n",
    "        x = self.conv_block(x)\n",
    "        \n",
    "        # global pooling\n",
    "        x = self.adapt_pool(x)\n",
    "        x = torch.flatten(x, start_dim=1) #또는 x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # final classification\n",
    "        x = self.classifier(x)\n",
    "        # 또는 아래와 같이 classifier_block을 forward\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "simple_cnn = SimpleCNN(num_classes=10)\n",
    "\n",
    "summary(model=simple_cnn, input_size=(1, 3, 32, 32), \n",
    "        col_names=['input_size', 'output_size', 'num_params'], \n",
    "        row_settings=['var_names'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer 클래스를 이용하여 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, loss_fn, optimizer, train_loader, val_loader, device=None):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.device = device\n",
    "    \n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        \n",
    "        # running 평균 loss 계산. \n",
    "        accu_loss = 0.0\n",
    "        running_avg_loss = 0.0\n",
    "        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        accuracy = 0.0\n",
    "        \n",
    "        # tqdm으로 실시간 training loop 진행 상황 시각화\n",
    "        with tqdm(total=len(self.train_loader), desc=f\"Epoch {epoch+1} [Training..]\", leave=True) as progress_bar:\n",
    "            for batch_idx, (inputs, targets) in enumerate(self.train_loader):\n",
    "                # 반드시 to(self.device). to(device) 아님. \n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss_fn(outputs, targets)\n",
    "                \n",
    "                # Backward pass\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.  \n",
    "                accu_loss += loss.item()\n",
    "                running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                # accuracy metric 계산\n",
    "                # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n",
    "                num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산\n",
    "                num_total += inputs.shape[0]\n",
    "                accu_num_correct += num_correct\n",
    "                accuracy = accu_num_correct / num_total\n",
    "\n",
    "                #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n",
    "                progress_bar.update(1)\n",
    "                if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch횟수마다 또는 맨 마지막 batch에서 update \n",
    "                    progress_bar.set_postfix({\"Loss\": running_avg_loss, \n",
    "                                              \"Accuracy\": accuracy})\n",
    "        \n",
    "        return running_avg_loss, accuracy\n",
    "                \n",
    "    def validate_epoch(self, epoch):\n",
    "        if not self.val_loader:\n",
    "            return None\n",
    "            \n",
    "        self.model.eval()\n",
    "\n",
    "        # running 평균 loss 계산. \n",
    "        accu_loss = 0\n",
    "        running_avg_loss = 0\n",
    "        # 정확도, 정확도 계산을 위한 전체 건수 및 누적 정확건수\n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "        accuracy = 0.0\n",
    "        with tqdm(total=len(self.val_loader), desc=f\"Epoch {epoch+1} [Validating]\", leave=True) as progress_bar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(self.val_loader):\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "                    \n",
    "                    outputs = self.model(inputs)\n",
    "                    \n",
    "                    loss = self.loss_fn(outputs, targets)\n",
    "                    # batch 반복 시 마다 누적  loss를 구하고 이를 batch 횟수로 나눠서 running 평균 loss 구함.  \n",
    "                    accu_loss += loss.item()\n",
    "                    running_avg_loss = accu_loss /(batch_idx + 1)\n",
    "\n",
    "                    # accuracy metric 계산\n",
    "                    # outputs 출력 예측 class값과 targets값 일치 건수 구하고\n",
    "                    num_correct = (outputs.argmax(-1) == targets).sum().item()\n",
    "                    # 배치별 누적 전체 건수와 누적 전체 num_correct 건수로 accuracy 계산  \n",
    "                    num_total += inputs.shape[0]\n",
    "                    accu_num_correct += num_correct\n",
    "                    accuracy = accu_num_correct / num_total\n",
    "\n",
    "                    #tqdm progress_bar에 진행 상황 및 running 평균 loss와 정확도 표시\n",
    "                    progress_bar.update(1)\n",
    "                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:  # 20 batch횟수마다 또는 맨 마지막 batch에서 update \n",
    "                        progress_bar.set_postfix({\"Loss\": running_avg_loss, \n",
    "                                                  \"Accuracy\":accuracy})\n",
    "        return running_avg_loss, accuracy\n",
    "    \n",
    "    def fit(self, epochs):\n",
    "        # epoch 시마다 학습/검증 결과를 기록하는 history dict 생성.\n",
    "        history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = self.train_epoch(epoch)\n",
    "            val_loss, val_acc = self.validate_epoch(epoch)\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f} Train Accuracy: {train_acc:.4f}\",\n",
    "                  f\", Val Loss: {val_loss:.4f} Val Accuracy: {val_acc:.4f}\" if val_loss is not None else \"\")\n",
    "            # epoch 시마다 학습/검증 결과를 기록.\n",
    "            history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "            history['val_loss'].append(val_loss); history['val_acc'].append(val_acc)\n",
    "            \n",
    "        return history \n",
    "    \n",
    "    # 학습이 완료된 모델을 return \n",
    "    def get_trained_model(self):\n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "\n",
    "NUM_INPUT_CHANNELS = 3\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "model = SimpleCNN(num_classes=NUM_CLASSES)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "trainer = Trainer(model=model, loss_fn=loss_fn, optimizer=optimizer,\n",
    "       train_loader=tr_loader, val_loader=val_loader, device=device)\n",
    "# 학습 및 평가 \n",
    "history = trainer.fit(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 시각화 \n",
    "def show_history(history, metric='acc'):\n",
    "    if metric == 'loss':\n",
    "        train_metric_name = 'train_loss'\n",
    "        val_metric_name = 'val_loss'\n",
    "    else:\n",
    "        train_metric_name = 'train_acc'\n",
    "        val_metric_name = 'val_acc'\n",
    "        \n",
    "    plt.plot(history[train_metric_name], label='train')\n",
    "    plt.plot(history[val_metric_name], label='valid')\n",
    "    plt.legend()\n",
    "    \n",
    "show_history(history, metric='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor 클래스로 모델 성능 평가 및 이미지 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        eval_metric = 0.0\n",
    "        \n",
    "        num_total = 0.0\n",
    "        accu_num_correct = 0.0\n",
    "\n",
    "        with tqdm(total=len(loader), desc=f\"[Evaluating]\", leave=True) as progress_bar:\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    targets = targets.to(self.device)\n",
    "                    pred = self.model(inputs)\n",
    "\n",
    "                    # 정확도 계산을 위해 누적 전체 건수와 누적 전체 num_correct 건수 계산  \n",
    "                    num_correct = (pred.argmax(-1) == targets).sum().item()\n",
    "                    num_total += inputs.shape[0]\n",
    "                    accu_num_correct += num_correct\n",
    "                    eval_metric = accu_num_correct / num_total\n",
    "\n",
    "                    progress_bar.update(1)\n",
    "                    if batch_idx % 20 == 0 or (batch_idx + 1) == progress_bar.total:\n",
    "                        progress_bar.set_postfix({\"Accuracy\": eval_metric})\n",
    "        \n",
    "        return eval_metric\n",
    "\n",
    "    def predict_proba(self, inputs):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = inputs.to(self.device)\n",
    "            outputs = self.model(inputs)\n",
    "            #예측값을 반환하므로 targets은 필요 없음.\n",
    "            #targets = targets.to(self.device)\n",
    "            pred_proba = F.softmax(outputs, dim=-1) #또는 dim=1\n",
    "\n",
    "        return pred_proba\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        pred_proba = self.predict_proba(inputs)\n",
    "        pred_class = torch.argmax(pred_proba, dim=-1)\n",
    "\n",
    "        return pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trained_model = trainer.get_trained_model()\n",
    "\n",
    "# 학습데이터와 동일하게 정규화된 데이터를 입력해야 함. \n",
    "# test_dataset = CIFAR10(root='./data', train=False, download=True, transform=ToTensor())\n",
    "# test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "predictor = Predictor(model=trained_model, device=device)\n",
    "eval_metric = predictor.evaluate(test_loader)\n",
    "print(f'test dataset evaluation:{eval_metric:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "plt.figure(figsize=(1, 1))\n",
    "plt.imshow(test_dataset.data[0])\n",
    "plt.title(class_names[test_dataset.targets[0]])\n",
    "\n",
    "print('target value:', test_dataset.targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 반드시 예측할 이미지는 tensor로, shape는 4차원으로 입력. 이를 위해 unsqueeze(0)\n",
    "pred_class = predictor.predict(test_dataset[0][0].unsqueeze(0))\n",
    "print('predicted class:', pred_class.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 출력 Feature Map의 면적 계산하기\n",
    "* 입력 Feature Map의 면적과 Convolution 적용 Kernel size, stride 및 padding에 따른 출력 Feature Map의 면적 계산\n",
    "* I는 입력 Feature Map의 면적(크기), K는 Filter의 Kernel size, P는 Padding(정수), S는 Strides(정수)\n",
    "* O = (I - K + 2P)/S + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stride가 1이고 Padding이 없는 경우 - Kernel size 3 적용\n",
    "* I는 입력 Feature Map의 크기, K는 Filter의 Kernel size, P는 Padding(정수), S는 Strides(정수)\n",
    "* O = (I - K + 2P)/1 + 1 = (5 - 3 + 0 )/1 + 1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "input = torch.randn(1, 5, 5)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=0) #kernel_size=5로 적용\n",
    "output = conv1(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stride가 1이고 Padding이 1인 경우\n",
    "* O = (I - F + 2P)/S + 1 = (5 - 3 + 2 )/1 + 1 = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 5, 5)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1) #padding='same'으로 적용 \n",
    "output = conv1(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Zero Padding 이 적용된 Output 보기\n",
    "input = torch.randn(1, 5, 5)\n",
    "padding_layer = nn.ZeroPad2d(padding=1)\n",
    "padded_input = padding_layer(input)\n",
    "print('padded input shape:', padded_input.shape)\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "output = conv1(padded_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding 시 상하 좌우가 다른 값을 넣기\n",
    "* Conv2d의 padding값으로 Tuple을 입력. 예를 들어 padding=(1, 2)이면 상하가 1, 좌우가 2임.\n",
    "* 상하가 다른값, 또는 좌우가 다른 값으로 Padding 적용하려면 nn.ZeroPad2d나 F.pad()함수를 사용해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 5, 5)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, \n",
    "                  kernel_size=3, padding=(1, 2)) # padding=Tuple에서 맨앞이 상하, 뒤가 좌우\n",
    "output = conv1(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Zero Padding 이 적용된 Output 보기\n",
    "input = torch.randn(1, 5, 5)\n",
    "#padding = (left, right, top, bottom)로 좌,우,상,하 순서임.\n",
    "padding_layer = nn.ZeroPad2d(padding=(0, 1, 0, 2))\n",
    "padded_input = padding_layer(input)\n",
    "print('padded input shape:', padded_input.shape)\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "output = conv1(padded_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "input = torch.randn(1, 5, 5)\n",
    "#pad = (left, right, top, bottom)로 좌,우,상,하 순서임.\n",
    "padded_input = F.pad(input, pad=(0, 1, 0, 2), mode='constant', value=0)\n",
    "print(padded_input.shape)\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3)\n",
    "output = conv1(padded_input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stride가 2이고 Padding이 없는 경우 - Kernel size 3 적용\n",
    "* O = (I - K + 2P)/S + 1 = (5 - 3)/2 + 1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 5, 5)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2)\n",
    "output = conv1(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv의 stride가 2이상일 경우, padding='same'은 적용할 수 없음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 5, 5)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding='same')\n",
    "output = conv1(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 5, 5)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=(1, 1)) \n",
    "output = conv1(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력이 6X6에서 Kernel_size=3, Stride=2 적용\n",
    "* O = (I - K + 2P)/2 + 1 = (6 - 3 + 0)/2 + 1 = 2.5 = 2\n",
    "* stride=2 적용 시에는 맨 가장자리를 Convolution 적용하지 못하는 경우를 피하기 위해 일반적으로 padding 더해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 6, 6)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2) \n",
    "output = conv1(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 6, 6)\n",
    "conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=2, padding=(1, 1)) \n",
    "output = conv1(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxpooling 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 224, 224) # (1, 223, 223)\n",
    "max_pool = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "output = max_pool(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
